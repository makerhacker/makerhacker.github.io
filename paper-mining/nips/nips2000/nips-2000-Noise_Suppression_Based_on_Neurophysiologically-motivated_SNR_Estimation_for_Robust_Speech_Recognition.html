<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-91" href="#">nips2000-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</h1>
<br/><p>Source: <a title="nips-2000-91-pdf" href="http://papers.nips.cc/paper/1902-noise-suppression-based-on-neurophysiologically-motivated-snr-estimation-for-robust-speech-recognition.pdf">pdf</a></p><p>Author: Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier</p><p>Abstract: A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1</p><p>Reference: <a title="nips-2000-91-reference" href="../nips2000_reference/nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Noise suppression based on neurophysiologically-motivated SNR estimation for robust speech recognition  J iirgen Tcharz Medical Physics Group Oldenburg University 26111 Oldenburg Germany tch@medi. [sent-1, score-0.923]
</p><p>2 A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. [sent-6, score-0.487]
</p><p>3 Noise suppression is achieved by attenuating frequency channels according to their SNR. [sent-7, score-0.593]
</p><p>4 The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. [sent-8, score-1.349]
</p><p>5 1  Introduction  One of the major problems in automatic speech recognition (ASR) systems is their lack of robustness in noise, which severely degrades their usefulness in many practical applications. [sent-9, score-0.563]
</p><p>6 Another method to increase robustness of ASR systems is to suppress the background noise before feature extraction. [sent-13, score-0.326]
</p><p>7 Classical approaches for single-channel noise suppression are Spectral Subtraction [3] and related schemes, e. [sent-14, score-0.609]
</p><p>8 [4], where  the noise spectrum is usually measured in detected speech pauses and subtracted from the signal. [sent-16, score-0.954]
</p><p>9 In these approaches, stationarity of the noise has to be assumed while speech is active. [sent-17, score-0.677]
</p><p>10 Furthermore, portions detected as speech pauses must not contain any speech in order to allow for correct noise measurement. [sent-18, score-1.274]
</p><p>11 At the same time, all actual speech pauses should be detected for a fast update of the noise measurement. [sent-19, score-0.856]
</p><p>12 The noise suppression algorithm outlined in this work directly estimates the local SNR in a range of frequency channels even if speech and noise are present at the same time, i. [sent-21, score-1.494]
</p><p>13 , no explicit detection of speech pauses and no assumptions on noise stationarity during speech activity are necessary. [sent-23, score-1.3]
</p><p>14 Thus, both spectral and temporal information is represented in two-dimensional maps. [sent-25, score-0.198]
</p><p>15 These findings were applied to signal processing in a binaural noise suppression system [6] with the introduction of so-called Amplitude Modulation Spectrograms (AMS) , which contain information on both center frequencies and modulation frequencies. [sent-26, score-1.048]
</p><p>16 In the present study, the different representations of speech and noise in AMS patterns are detected by a neural network, which estimates the local SNR in each frequency channel. [sent-27, score-0.925]
</p><p>17 For noise suppression, the frequency bands are attenuated according to the estimated local SNR in the different frequency channels. [sent-28, score-0.59]
</p><p>18 The proposed noise suppression scheme is evaluated in isolated-digit recognition experiments. [sent-29, score-0.69]
</p><p>19 This combination was found to allow for more robust isolated-digit recognition rates, compared to a standard recognizer with mel-cepstral features and HMM modeling [8, 9]. [sent-31, score-0.181]
</p><p>20 Thus, the recognition experiments in this study were conducted with this particular combination to evaluate whether a further increase of robustness can be achieved with additional noise suppression. [sent-32, score-0.374]
</p><p>21 1  The recognition system Noise suppression  Figure 1 shows the processing steps which are performed for noise suppression. [sent-34, score-0.72]
</p><p>22 To generate AMS patterns which are used for SNR estimation, the input signal (16 kHz sampling rate) is short-term level adjusted, i. [sent-35, score-0.142]
</p><p>23 , each 32 ms segment which is later transformed into an AMS pattern is scaled to the same root-mean-square value. [sent-37, score-0.136]
</p><p>24 Each segment is multiplied by a Hanning window and padded with zeros to obtain a frame of 128 samples which is transformed with a FFT into a complex spectrum, with a spectral resolution of 125 Hz. [sent-41, score-0.481]
</p><p>25 This envelope signal is again segmented into overlapping segments of 128 samples (32ms) with an overlap of 64 samples. [sent-46, score-0.155]
</p><p>26 Each segment is multiplied with a Hanning window and padded with zeros to obtain a frame of 256 samples. [sent-47, score-0.205]
</p><p>27 A further FFT is computed and supplies a modulation spectrum in each frequency channel, with a modulation frequency resolution of 15. [sent-48, score-0.801]
</p><p>28 By an appropriate summation of neighbouring FFT bins the frequency axis is transformed to a Bark scale with 15 channels, with center frequencies from 100-7300 Hz. [sent-50, score-0.297]
</p><p>29 The modulation  input signal  scaled input signal  -+- ---c=J level normalization  bandpass time signals  ft~  ~ftl;l ov-add analysis  modulation spectrogram  envelope  LJ ~U  FFI  ftl : ! [sent-51, score-0.686]
</p><p>30 ~ LJ ------'  rescale, logamplitude  output signal  [  -  -+-  [  Figure 1: Processing stages of AMS-based noise suppression. [sent-52, score-0.368]
</p><p>31 frequency spectrum is scaled logarithmically by appropriate summation, which is motivated by psychoacoustical findings about the shape of auditory modulation filters [10). [sent-53, score-0.585]
</p><p>32 The modulation frequency spectrum is restricted to the range between 50-400 Hz and has a resolution of 15 channels. [sent-54, score-0.452]
</p><p>33 Thus, the fundamental frequency of typical voiced speech is represented in the modulation spectrum. [sent-55, score-0.812]
</p><p>34 The AMS pattern on the left side was generated from a voiced speech portion. [sent-60, score-0.463]
</p><p>35 110 Hz) is represented in each center frequency band. [sent-62, score-0.182]
</p><p>36 The AMS pattern on the right side was generated from speech simulating noise. [sent-63, score-0.45]
</p><p>37 The typical spectral tilt can be seen, but there is no structure across modulation frequencies . [sent-64, score-0.468]
</p><p>38 The net consists of 225 input neurons (15*15, the AMS resolution of center frequencies and modulation frequencies, respectively), a hidden layer with 160 neurons, and an output layer with 15 neurons. [sent-66, score-0.397]
</p><p>39 The activity of each output neuron indicates the SNR in one of the 15 center frequency channels. [sent-67, score-0.209]
</p><p>40 For training, the narrow-band SNRs in 15 channels were measured for each AMS analysis frame of the training material prior to adding speech and noise. [sent-68, score-0.59]
</p><p>41 The neural network was trained with AMS patterns generated from 72 min of noisy speech from 400 talkers and 41 natural noise types, using the momentum backpropagation algorithm. [sent-69, score-0.746]
</p><p>42 The 15 output neuron activities that appear for each pattern serve as SNR estimates for the respective frequency channels. [sent-71, score-0.178]
</p><p>43 In a detailed study on AMS-based broad-band SNR estimation [11) it was shown that harmonicity which is well represented in AMS patterns is the most important cue for the neural network to distinguish between speech and noise. [sent-72, score-0.57]
</p><p>44 However, harmonicity is not the only cue, as the algorithm allows for reliable discrimination between unvoiced speech and noise. [sent-73, score-0.453]
</p><p>45 The accuracy of SNR  55  73 100 135 192 246 Modulation Frequency [Hz]  333  55  73 100 135 192 246 Modulation Frequency [Hz]  333  Figure 2: AMS patterns generated from a voiced speech segment (left), and from speech simulating noise (right). [sent-74, score-1.248]
</p><p>46 estimation in terms of mean deviation between the actual and the estimated SNR in each frame, for each frequency channel, was determined with "unknown" test data (36 min of noisy speech). [sent-77, score-0.196]
</p><p>47 The average deviation across all frequency channels was 5. [sent-78, score-0.208]
</p><p>48 4 dB, with a decrease of accuracy towards higher frequency channels. [sent-79, score-0.148]
</p><p>49 Sub-band SNR estimates are utilized for noise suppression by attenuating frequency channels according to their local SNR. [sent-80, score-0.852]
</p><p>50 The gain function which was applied is given by Uk = (SNRk / (SNRk + 1))X , where k denotes the frequency channel, SNR the signalto-noise ratio on a linear scale, and x is an exponent which controls the strength of the attenuation, and which was set to 1. [sent-81, score-0.178]
</p><p>51 Noise suppression based on AMS-derived SNR estimation is performed in the FFTdomain. [sent-83, score-0.398]
</p><p>52 The input signal is segmented into overlapping frames with a window length of 32 ms, and a shift of 16 ms is applied, i. [sent-84, score-0.216]
</p><p>53 The magnitude in each frequency bin is multiplied by the corresponding gain computed from the AMS-based SNR estimation. [sent-88, score-0.185]
</p><p>54 The gain in frequency bins which are not covered by the center frequencies from the SNR estimation is linearly interpolated from neighboring estimation frequencies . [sent-89, score-0.416]
</p><p>55 An inverse FFT is computed, and the enhanced speech is attained by overlapping and adding. [sent-91, score-0.449]
</p><p>56 2  Auditory-based ASR feature extraction  The front end which is used in the recognition system is based on a quantitative model of the "effective" peripheral auditory processing. [sent-93, score-0.289]
</p><p>57 The model simulates both spectral and temporal properties of sound processing in the auditory system which were found in psychoacoustical and physiological experiments. [sent-94, score-0.393]
</p><p>58 The model was originally developed for describing human performance in typical psychoacoustical spectral and temporal masking experiments, e. [sent-95, score-0.25]
</p><p>59 The main processing stages of the auditory model are gammatone filtering, envelope extraction in each frequency channel, adaptive amplitude compression, and low pass filtering of the envelope in each band. [sent-98, score-0.616]
</p><p>60 3  Neural network recognizer  For scoring of the input features, a locally recurrent neural network (LRNN) is employed with three layers of neurons (150 input, 289 hidden, and 10 output neurons). [sent-103, score-0.189]
</p><p>61 The input matrix consists of 5 times the auditory model feature vector with 30 elements, glued together in order to allow the network to memorize a time sequence of input matrices. [sent-105, score-0.2]
</p><p>62 1  Recognition experiments Setup  The speech material for training of the word models and scoring was taken from the ZIFKOM database of Deutsche Telekom AG. [sent-108, score-0.485]
</p><p>63 The background noises were added to the utterances with signal-to-noise ratios ranging from 20 to -10 dB. [sent-113, score-0.149]
</p><p>64 The word models were trained with features from 100 undisturbed and unprocessed utterances of each digit. [sent-114, score-0.158]
</p><p>65 Features for testing were calculated from another 100 utterances of each digit which were distorted by additive noise before preprocessing. [sent-115, score-0.363]
</p><p>66 The recognition rates were measured without noise suppression and with noise suppression as described in Section 2. [sent-116, score-1.345]
</p><p>67 For comparison, the recognition rates were measured with noise suppression based on Spectral Subtraction including residual noise reduction [3] before feature extraction. [sent-118, score-0.995]
</p><p>68 In the first method, speech pauses in the noisy signals were detected using Voice Activity Detection (VAD) [14]. [sent-120, score-0.597]
</p><p>69 The noise measure was updated in speech pauses using a low pass filter with a time constant of 40 ms. [sent-121, score-0.833]
</p><p>70 In the second method, the noise spectrum was measured in speech pauses which were detected from the clean utterances using an energy criterion (thus, perfect speech pause information is provided, which is not available in real applications). [sent-122, score-1.639]
</p><p>71 3 for three types of background noise as a function of the SNR. [sent-125, score-0.292]
</p><p>72 In all tested noises, noise suppression with the proposed algorithm increases the recognition rate in comparison with the unprocessed data and with Spectral Subtraction with VAD-based noise measurement. [sent-126, score-1.053]
</p><p>73 Spectral Subtraction with perfect speech pause detection allows for higher recognition rates than the AMS-based approach in stationary white noise. [sent-127, score-0.789]
</p><p>74 Here, the noise measure for Spectral Subtraction is very accurate during speech activity and allows for effective noise removal. [sent-128, score-0.963]
</p><p>75 AMS-based noise suppression estimates the SNR in every analysis frame, and no a priori information on speech-free segments is provided to the algorithm. [sent-129, score-0.609]
</p><p>76 In  White noise  100 ~ 90 80  ~ <:  :8 '2:  Cl  §  70  . [sent-130, score-0.259]
</p><p>77 :~~:-<~----"  60 50  40 30  a: 20  10  Printing room noise  ~=,. [sent-133, score-0.289]
</p><p>78 L-----'-----''---'-----'---'----'  clean 20 15 10 5  0  -5 -10  SNR [dB) Speech simulating noise 100 r---~~F~~~. [sent-170, score-0.344]
</p><p>79 SNR [dB)  speech simulation noise, which fluctuates in level but not in spectral shape, Spectral Subtraction with perfect speech pause detection works slightly better than AMSbased noise suppression. [sent-201, score-1.589]
</p><p>80 Here, Spectral Subtraction even degrades the recognition rates in some SNRs, compared to the unprocessed data. [sent-203, score-0.261]
</p><p>81 The noise measure from VAD-based or perfect speech pause detection cannot be updated while speech is active. [sent-204, score-1.339]
</p><p>82 Thus, an incorrect spectrum is subtracted and leads to artifacts and degraded recognition performance. [sent-205, score-0.179]
</p><p>83 4  Discussion  The proposed neurophysiologically-motivated noise suppression scheme was shown to significantly improve digit recognition in noise in comparison with unprocessed data and with Spectral Subtraction using VAD-based noise measures. [sent-210, score-1.362]
</p><p>84 A perfect speech pause detection (which is not available yet in real systems) allows for a reliable estimation of the noise floor in stationary noise. [sent-211, score-0.969]
</p><p>85 In non-stationary noise, however, the AMS pattern-based signal classification and noise suppression is advantageous, as it does not depend on speech pause detection and no assumption is necessary about the noise being stationary while speech is active. [sent-212, score-1.952]
</p><p>86 The neurophysiologically-based noise suppression scheme outlined in this paper does not produce such fast fluctuating artifacts. [sent-216, score-0.661]
</p><p>87 In general, a good quality of speech is maintained. [sent-217, score-0.418]
</p><p>88 The choice of the attenuation exponent x has only little impact on the quality of speech in favourable SNRs. [sent-218, score-0.478]
</p><p>89 With decreasing SNR, however, there is a tradeoff between the amount of noise suppression and distortions  of the speech. [sent-219, score-0.609]
</p><p>90 A typical distortion of speech in poor signal-to-noise ratios is an unnatural spectral "coloring", rather than fast fluctuating distortions. [sent-220, score-0.7]
</p><p>91 In informal tests, most listeners did not have the impression that the algorithm improves speech intelligibility, but clearly preferred the processed signal over the unprocessed one, as the background noise was significantly suppressed without annoying artifacts. [sent-221, score-0.884]
</p><p>92 The performance and characteristics of the algorithm of course strongly depends on the training data, as only lttle knowledge on the differences between speech and noise is "hard wired". [sent-223, score-0.677]
</p><p>93 Suppression of acoustic noise in speech using spectral subtraction. [sent-243, score-0.875]
</p><p>94 Speech enhancement using a minimum meansquare error short-time spectral amplitude estimator. [sent-250, score-0.328]
</p><p>95 Speech enhancement based on physiological and psycho acoustical models of modulation perception and binaural interaction. [sent-266, score-0.276]
</p><p>96 A speech recognizer with low complexity based on RNN. [sent-277, score-0.492]
</p><p>97 Exploiting the potential of auditory preprocessing for robust speech recognition by locally recurrent neural networks. [sent-284, score-0.638]
</p><p>98 Combining speech enhancement and auditory feature extraction for robust speech recognition. [sent-293, score-1.068]
</p><p>99 Estimation of the signal-to-noise ratio with amplitude modulation spectrograms. [sent-305, score-0.291]
</p><p>100 A quantitative model of the "effective" signal processing in the auditory system: II. [sent-310, score-0.213]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('speech', 0.418), ('ams', 0.362), ('suppression', 0.35), ('snr', 0.266), ('noise', 0.259), ('modulation', 0.201), ('spectral', 0.198), ('subtraction', 0.178), ('frequency', 0.148), ('pause', 0.121), ('pauses', 0.121), ('auditory', 0.113), ('oldenburg', 0.104), ('unprocessed', 0.104), ('amplitude', 0.09), ('fft', 0.086), ('kollmeier', 0.086), ('recognition', 0.081), ('asr', 0.074), ('recognizer', 0.074), ('spectrum', 0.071), ('signal', 0.07), ('frequencies', 0.069), ('material', 0.067), ('perfect', 0.066), ('channels', 0.06), ('detected', 0.058), ('detection', 0.057), ('ms', 0.056), ('envelope', 0.054), ('utterances', 0.054), ('extraction', 0.053), ('clean', 0.053), ('dau', 0.052), ('fluctuates', 0.052), ('fluctuating', 0.052), ('kasper', 0.052), ('noalgo', 0.052), ('printing', 0.052), ('psychoacoustical', 0.052), ('reininger', 0.052), ('tchorz', 0.052), ('vad', 0.052), ('digit', 0.05), ('estimation', 0.048), ('transformed', 0.046), ('rates', 0.046), ('frame', 0.045), ('voiced', 0.045), ('patterns', 0.042), ('front', 0.042), ('enhancement', 0.04), ('stages', 0.039), ('multiplied', 0.037), ('hz', 0.036), ('pass', 0.035), ('attenuated', 0.035), ('attenuating', 0.035), ('binaural', 0.035), ('hanning', 0.035), ('harmonicity', 0.035), ('kleinschmidt', 0.035), ('kohlrausch', 0.035), ('lrnn', 0.035), ('snrk', 0.035), ('ssj', 0.035), ('robustness', 0.034), ('segment', 0.034), ('center', 0.034), ('background', 0.033), ('db', 0.033), ('channel', 0.033), ('resolution', 0.032), ('germany', 0.032), ('ratios', 0.032), ('simulating', 0.032), ('overlapping', 0.031), ('neurons', 0.031), ('respective', 0.03), ('room', 0.03), ('input', 0.03), ('processing', 0.03), ('zeros', 0.03), ('snrs', 0.03), ('wolf', 0.03), ('exponent', 0.03), ('attenuation', 0.03), ('degrades', 0.03), ('ftl', 0.03), ('modulations', 0.03), ('noises', 0.03), ('padded', 0.03), ('spectrograms', 0.03), ('window', 0.029), ('medical', 0.028), ('activity', 0.027), ('subtracted', 0.027), ('ss', 0.027), ('network', 0.027), ('robust', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="91-tfidf-1" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>Author: Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier</p><p>Abstract: A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1</p><p>2 0.31004018 <a title="91-tfidf-2" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>Author: Hagai Attias, John C. Platt, Alex Acero, Li Deng</p><p>Abstract: This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple microphones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods.</p><p>3 0.22244111 <a title="91-tfidf-3" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>4 0.1691954 <a title="91-tfidf-4" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>Author: Lawrence K. Saul, Jont B. Allen</p><p>Abstract: An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (1l</p><p>5 0.1542206 <a title="91-tfidf-5" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>6 0.15302882 <a title="91-tfidf-6" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>7 0.1392009 <a title="91-tfidf-7" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>8 0.1096919 <a title="91-tfidf-8" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>9 0.10952861 <a title="91-tfidf-9" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>10 0.084325314 <a title="91-tfidf-10" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>11 0.07522478 <a title="91-tfidf-11" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>12 0.073298536 <a title="91-tfidf-12" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>13 0.067016557 <a title="91-tfidf-13" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>14 0.065279871 <a title="91-tfidf-14" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>15 0.062656015 <a title="91-tfidf-15" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>16 0.05372984 <a title="91-tfidf-16" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>17 0.049441416 <a title="91-tfidf-17" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>18 0.048563216 <a title="91-tfidf-18" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>19 0.047761623 <a title="91-tfidf-19" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>20 0.046918932 <a title="91-tfidf-20" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, -0.172), (2, 0.083), (3, 0.25), (4, -0.127), (5, -0.21), (6, -0.364), (7, -0.128), (8, 0.056), (9, 0.065), (10, 0.168), (11, 0.081), (12, -0.026), (13, -0.016), (14, 0.115), (15, -0.003), (16, 0.055), (17, 0.048), (18, -0.013), (19, -0.083), (20, -0.022), (21, 0.019), (22, -0.064), (23, 0.046), (24, -0.044), (25, 0.113), (26, 0.033), (27, 0.036), (28, 0.029), (29, -0.097), (30, 0.033), (31, 0.017), (32, -0.008), (33, -0.041), (34, 0.012), (35, -0.009), (36, 0.02), (37, 0.011), (38, -0.039), (39, 0.09), (40, 0.001), (41, -0.029), (42, 0.041), (43, -0.008), (44, 0.005), (45, -0.039), (46, 0.036), (47, 0.018), (48, -0.039), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98820257 <a title="91-lsi-1" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>Author: Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier</p><p>Abstract: A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1</p><p>2 0.85718 <a title="91-lsi-2" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>Author: Hagai Attias, John C. Platt, Alex Acero, Li Deng</p><p>Abstract: This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple microphones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods.</p><p>3 0.78912497 <a title="91-lsi-3" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>4 0.74742222 <a title="91-lsi-4" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>Author: Lawrence K. Saul, Jont B. Allen</p><p>Abstract: An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (1l</p><p>5 0.62429339 <a title="91-lsi-5" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>Author: Sam T. Roweis</p><p>Abstract: Source separation, or computational auditory scene analysis , attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment. Unmixing algorithms such as lCA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available. I present a technique called refiltering which recovers sources by a nonstationary reweighting (</p><p>6 0.57046318 <a title="91-lsi-6" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>7 0.46395299 <a title="91-lsi-7" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>8 0.43920955 <a title="91-lsi-8" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>9 0.31126085 <a title="91-lsi-9" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>10 0.26368272 <a title="91-lsi-10" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>11 0.25848117 <a title="91-lsi-11" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>12 0.2384996 <a title="91-lsi-12" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>13 0.22747715 <a title="91-lsi-13" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>14 0.213898 <a title="91-lsi-14" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>15 0.20338935 <a title="91-lsi-15" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>16 0.19686632 <a title="91-lsi-16" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>17 0.19336297 <a title="91-lsi-17" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>18 0.18083723 <a title="91-lsi-18" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>19 0.18008301 <a title="91-lsi-19" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>20 0.17842834 <a title="91-lsi-20" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.048), (4, 0.034), (10, 0.017), (17, 0.104), (32, 0.011), (33, 0.07), (42, 0.028), (55, 0.024), (62, 0.018), (65, 0.019), (67, 0.04), (76, 0.016), (81, 0.034), (89, 0.358), (90, 0.023), (91, 0.024), (93, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84077018 <a title="91-lda-1" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>Author: Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier</p><p>Abstract: A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1</p><p>2 0.56209141 <a title="91-lda-2" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.</p><p>3 0.37554261 <a title="91-lda-3" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>4 0.36666459 <a title="91-lda-4" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>Author: Sam T. Roweis</p><p>Abstract: Source separation, or computational auditory scene analysis , attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment. Unmixing algorithms such as lCA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available. I present a technique called refiltering which recovers sources by a nonstationary reweighting (</p><p>5 0.35859472 <a title="91-lda-5" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>Author: Lawrence K. Saul, Jont B. Allen</p><p>Abstract: An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (1l</p><p>6 0.33677742 <a title="91-lda-6" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>7 0.33079159 <a title="91-lda-7" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>8 0.32883626 <a title="91-lda-8" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>9 0.32837275 <a title="91-lda-9" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>10 0.3269839 <a title="91-lda-10" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>11 0.3268815 <a title="91-lda-11" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>12 0.32551301 <a title="91-lda-12" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>13 0.32305333 <a title="91-lda-13" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>14 0.322373 <a title="91-lda-14" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>15 0.32177955 <a title="91-lda-15" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>16 0.31984454 <a title="91-lda-16" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>17 0.31934264 <a title="91-lda-17" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>18 0.31867725 <a title="91-lda-18" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>19 0.31866097 <a title="91-lda-19" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>20 0.31842595 <a title="91-lda-20" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
