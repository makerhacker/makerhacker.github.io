<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>278 hunch net-2007-12-17-New Machine Learning mailing list</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-278" href="#">hunch_net-2007-278</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>278 hunch net-2007-12-17-New Machine Learning mailing list</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-278-html" href="http://hunch.net/?p=306">html</a></p><p>Introduction: IMLS(which is the nonprofit running ICML) has setup a new mailing list
forMachine Learning News. The list address is ML-news@googlegroups.com, and
signup requires a google account (which you can create). Only members can send
messages.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 IMLS(which is the nonprofit running ICML) has setup a new mailing list forMachine Learning News. [sent-1, score-1.444]
</p><p>2 com, and signup requires a google account (which you can create). [sent-3, score-0.633]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('messages', 0.361), ('formachine', 0.361), ('nonprofit', 0.361), ('list', 0.33), ('mailing', 0.289), ('google', 0.249), ('send', 0.234), ('members', 0.234), ('address', 0.214), ('setup', 0.21), ('account', 0.198), ('requires', 0.186), ('running', 0.184), ('icml', 0.149), ('create', 0.143), ('new', 0.07), ('learning', 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="278-tfidf-1" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS(which is the nonprofit running ICML) has setup a new mailing list
forMachine Learning News. The list address is ML-news@googlegroups.com, and
signup requires a google account (which you can create). Only members can send
messages.</p><p>2 0.2504392 <a title="278-tfidf-2" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points outKDNuggetswhich is a well-developed mailing list/news
site with aKDDflavor. This might particularly interest people looking for
industrial jobs in machine learning, as the mailing list has many such.</p><p>3 0.10104863 <a title="278-tfidf-3" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>Introduction: Alex Smolashowed me thisICML 2006webpage. This isNOTthe ICML we know, but
rather some people at "Enformatika". Investigation shows that they registered
with an anonymous yahoo email account fromdotregistrar.comthe "Home of the
$6.79 wholesale domain!" and their nameservers are byTurkticaret, a Turkish
internet company.It appears the website has since been altered to "ICNL" (the
above link uses the google cache).They say that imitation is the sincerest
form of flattery, so the organizers of the realICML 2006must feel quite
flattered.</p><p>4 0.099112153 <a title="278-tfidf-4" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here's a quick reference for summer ML-related conferences sorted by due
date:ConferenceDue dateLocationReviewingKDDFeb 10August 12-16, Beijing,
ChinaSingle BlindCOLTFeb 14June 25-June 27, Edinburgh, ScotlandSingle Blind?
(historically)ICMLFeb 24June 26-July 1, Edinburgh, ScotlandDouble Blind,
author response, zeroSPOFUAIMarch 30August 15-17, Catalina Islands,
CaliforniaDouble Blind, author responseGeographically, this is greatly
dispersed and the UAI/KDD conflict is unfortunate.Machine Learning conferences
are triannual now, betweenNIPS,AIStat, andICML. This has not always been the
case: the academic default is annual summer conferences, then NIPS started
with a December conference, and now AIStat has grown into an April
conference.However, the first claim is not quite correct. NIPS and AIStat have
few competing venues while ICML implicitly competes with many other
conferences accepting machine learning related papers. SinceJoelleand I are
taking a turn as program chairs this year, I</p><p>5 0.095857516 <a title="278-tfidf-5" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>Introduction: This is a rather long post, detailing the ICML 2012 review process. The goal
is to make the process more transparent, help authors understand how we came
to a decision, and discuss the strengths and weaknesses of this process for
future conference organizers.Microsoft’s Conference Management Toolkit (CMT)We
chose to useCMTover other conference management software mainly because of its
rich toolkit. The interface is sub-optimal (to say the least!) but it has
extensive capabilities (to handle bids, author response, resubmissions, etc.),
good import/export mechanisms (to process the data elsewhere), excellent
technical support (to answer late night emails, add new functionalities).
Overall, it was the right choice, although we hope a designer will look at
that interface sometime soon!Toronto Matching System (TMS)TMSis now being used
by many major conferences in our field (including NIPS and UAI). It is an
automated system (developed byLaurent CharlinandRich Zemelat U. Toronto) to
match re</p><p>6 0.095586658 <a title="278-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>7 0.094412714 <a title="278-tfidf-7" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>8 0.086068563 <a title="278-tfidf-8" href="../hunch_net-2009/hunch_net-2009-01-23-An_Active_Learning_Survey.html">338 hunch net-2009-01-23-An Active Learning Survey</a></p>
<p>9 0.085580453 <a title="278-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>10 0.083735667 <a title="278-tfidf-10" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>11 0.080710031 <a title="278-tfidf-11" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>12 0.079114966 <a title="278-tfidf-12" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>13 0.074035168 <a title="278-tfidf-13" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>14 0.072455682 <a title="278-tfidf-14" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>15 0.069504201 <a title="278-tfidf-15" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>16 0.066693902 <a title="278-tfidf-16" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>17 0.064198636 <a title="278-tfidf-17" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>18 0.055398069 <a title="278-tfidf-18" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>19 0.052078456 <a title="278-tfidf-19" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>20 0.051487923 <a title="278-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.062), (1, 0.068), (2, -0.0), (3, 0.035), (4, 0.004), (5, 0.013), (6, 0.006), (7, 0.016), (8, 0.086), (9, -0.01), (10, -0.052), (11, -0.022), (12, 0.06), (13, 0.039), (14, 0.02), (15, -0.062), (16, 0.094), (17, -0.001), (18, -0.018), (19, 0.062), (20, 0.055), (21, -0.098), (22, -0.047), (23, -0.048), (24, 0.042), (25, -0.082), (26, -0.011), (27, -0.03), (28, -0.051), (29, 0.006), (30, -0.057), (31, 0.142), (32, -0.008), (33, 0.015), (34, 0.051), (35, 0.084), (36, -0.016), (37, -0.008), (38, 0.195), (39, 0.08), (40, -0.086), (41, -0.009), (42, -0.034), (43, -0.022), (44, 0.034), (45, 0.084), (46, 0.02), (47, 0.067), (48, 0.159), (49, 0.089)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98099428 <a title="278-lsi-1" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS(which is the nonprofit running ICML) has setup a new mailing list
forMachine Learning News. The list address is ML-news@googlegroups.com, and
signup requires a google account (which you can create). Only members can send
messages.</p><p>2 0.70087957 <a title="278-lsi-2" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points outKDNuggetswhich is a well-developed mailing list/news
site with aKDDflavor. This might particularly interest people looking for
industrial jobs in machine learning, as the mailing list has many such.</p><p>3 0.62540954 <a title="278-lsi-3" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>Introduction: Alex Smolashowed me thisICML 2006webpage. This isNOTthe ICML we know, but
rather some people at "Enformatika". Investigation shows that they registered
with an anonymous yahoo email account fromdotregistrar.comthe "Home of the
$6.79 wholesale domain!" and their nameservers are byTurkticaret, a Turkish
internet company.It appears the website has since been altered to "ICNL" (the
above link uses the google cache).They say that imitation is the sincerest
form of flattery, so the organizers of the realICML 2006must feel quite
flattered.</p><p>4 0.60173506 <a title="278-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list ofmachine learning reading groups.</p><p>5 0.44429961 <a title="278-lsi-5" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>Introduction: Everyone should have received notice forNY ML Symposiumabstracts. Check
carefully, as one was lost by our system.The event itself is October 21, next
week.Leon Bottou,Stephen Boyd, andYoav Freundare giving the invited talks this
year, and there are many spotlights on local work spread throughout the
day.Chris Wigginshas setup 6(!) ML-interested startups to follow the
symposium, which should be of substantial interest to the employment
interested.I also wanted to give an update onICML 2012. Unlike last year, our
deadline is coordinated withAIStat(which is due this Friday). The paper
deadline for ICML has been pushed back to February 24 which should allow
significant time for finishing up papers after the winter break. Other details
may interest people as well:We settled on usingCMTafter checking out the
possibilities. I wasn't looking for this, because I've often found CMT clunky
in terms of easy access to the right information. Nevertheless, the breadth of
features and willingness to s</p><p>6 0.41595614 <a title="278-lsi-6" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>7 0.40825269 <a title="278-lsi-7" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>8 0.39598092 <a title="278-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>9 0.34417757 <a title="278-lsi-9" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>10 0.33704582 <a title="278-lsi-10" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>11 0.33360872 <a title="278-lsi-11" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>12 0.31924853 <a title="278-lsi-12" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>13 0.31645283 <a title="278-lsi-13" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>14 0.31443709 <a title="278-lsi-14" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>15 0.31187913 <a title="278-lsi-15" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>16 0.30582649 <a title="278-lsi-16" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>17 0.29826033 <a title="278-lsi-17" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>18 0.29106501 <a title="278-lsi-18" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>19 0.28618571 <a title="278-lsi-19" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>20 0.27915365 <a title="278-lsi-20" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(74, 0.794)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99215585 <a title="278-lda-1" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS(which is the nonprofit running ICML) has setup a new mailing list
forMachine Learning News. The list address is ML-news@googlegroups.com, and
signup requires a google account (which you can create). Only members can send
messages.</p><p>2 0.89390123 <a title="278-lda-2" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>3 0.88900197 <a title="278-lda-3" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health ofCOLT(Conference on Learning Theory or Computational Learning
Theory depending on who you ask) has been questioned over the last few years.
Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001,
and the attendance at the 2002 Sydney COLT fell to a new low. This occurred in
the general context of machine learning conferences rising in both number and
size over the last decade.Any discussion ofwhyCOLT has had difficulties is
inherently controversial as is any story about well-intentioned people making
the wrong decisions. Nevertheless, this may be worth discussing in the hope of
avoiding problems in the future and general understanding. In any such
discussion there is a strong tendency to identify with a conference/community
in a patriotic manner that is detrimental to thinking. Keep in mind that
conferences exist to further research.My understanding (I wasn't around) is
that COLT started as a subcommunity of the computer science theory community.
This i</p><p>4 0.88214356 <a title="278-lda-4" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>Introduction: We just finished theChicago 2005 Machine Learning Summer School. The school
was 2 weeks long with about 130 (or 140 counting the speakers) participants.
For perspective, this is perhaps the largest graduate level machine learning
class I am aware of anywhere and anytime (previousMLSSs have been close).
Overall, it seemed to go well, although the students are the real authority on
this. For those who missed it, DVDs will be available from our Slovenian
friends. EmailMrs Spela Sitarof the Jozsef Stefan Institute for details.The
following are some notes for future planning and those interested.Good
DecisionsAcquiring the larger-than-necessary "Assembly Hall" atInternational
House. Our attendance came in well above our expectations, so this was a
critical early decision that made a huge difference.The invited speakers were
key. They made a huge difference in the quality of the content.Delegating
early and often was important. One key difficulty here is gauging how much a
volunteer can (or</p><p>5 0.87819171 <a title="278-lda-5" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>Introduction: How should we, as researchers in machine learning, organize ourselves?The most
immediate measurable objective of computer science research is publishing a
paper. The most difficult aspect of publishing a paper is having reviewers
accept and recommend it for publication. The simplest mechanism for doing this
is to show theoretical progress on some standard, well-known easily understood
problem.In doing this, we often fall into a local minima of the research
process. The basic problem in machine learning is that it is very unclear that
the mathematical model is the right one for the (or some) real problem. A good
mathematical model in machine learning should have one fundamental trait: it
should aid the design of effective learning algorithms. To date, our ability
to solve interesting learning problems (speech recognition, machine
translation, object recognition, etcâ&euro;Ś) remains limited (although improving),
so the "rightness" of our models is in doubt.If our mathematical models are
bad, t</p><p>6 0.85727412 <a title="278-lda-6" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>7 0.8509782 <a title="278-lda-7" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>8 0.81285369 <a title="278-lda-8" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>9 0.77811939 <a title="278-lda-9" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>10 0.74604464 <a title="278-lda-10" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>11 0.73463792 <a title="278-lda-11" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>12 0.71994615 <a title="278-lda-12" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>13 0.71532172 <a title="278-lda-13" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>14 0.71114975 <a title="278-lda-14" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>15 0.69566327 <a title="278-lda-15" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>16 0.68673354 <a title="278-lda-16" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>17 0.66137296 <a title="278-lda-17" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>18 0.65282238 <a title="278-lda-18" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>19 0.65201455 <a title="278-lda-19" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>20 0.65016848 <a title="278-lda-20" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
