<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-397" href="#">high_scalability-2008-397</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-397-html" href="http://highscalability.com//blog/2008/9/28/product-happy-hadoop-python.html">html</a></p><p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Happy is a  framework for writing map-reduce programs for Hadoop using Jython. [sent-3, score-0.319]
</p><p>2 It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . [sent-4, score-0.666]
</p><p>3 There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . [sent-5, score-0.23]
</p><p>4 From the website:    Happy is a framework that allows Hadoop jobs to be written and run in Python 2. [sent-7, score-0.377]
</p><p>5 It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well. [sent-9, score-0.271]
</p><p>6 Map-reduce jobs in Happy are defined by sub-classing happy. [sent-13, score-0.255]
</p><p>7 HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. [sent-14, score-0.068]
</p><p>8 Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run(). [sent-15, score-0.593]
</p><p>9 When you call run(), Happy serializes your job instance and copies it and all accompanying  libraries out to the Hadoop cluster. [sent-16, score-0.626]
</p><p>10 Then for each task in the Hadoop job, your job instance is  de-serialized and map or reduce is called. [sent-17, score-0.73]
</p><p>11 The task results are written out using a collector, but aggregate statistics and other roll-up  information can be stored in the happy. [sent-18, score-0.595]
</p><p>12 Jython modules and Java jar files that are being called by your code can be specified using  the environment variable HAPPY_PATH. [sent-20, score-0.513]
</p><p>13 These are added to the Python path at startup, and  are also automatically included when jobs are sent to Hadoop. [sent-21, score-0.53]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hadoop', 0.432), ('happy', 0.39), ('task', 0.263), ('programs', 0.206), ('jobs', 0.18), ('delighted', 0.161), ('usingthe', 0.161), ('job', 0.154), ('instance', 0.144), ('serializes', 0.144), ('sharp', 0.139), ('jar', 0.135), ('path', 0.134), ('dictionary', 0.128), ('edited', 0.128), ('python', 0.125), ('outputs', 0.125), ('writing', 0.113), ('run', 0.111), ('specified', 0.108), ('files', 0.105), ('call', 0.103), ('edges', 0.103), ('collector', 0.101), ('reduce', 0.099), ('inputs', 0.098), ('stored', 0.094), ('parameters', 0.094), ('included', 0.092), ('returned', 0.09), ('runtime', 0.089), ('written', 0.086), ('variable', 0.085), ('copies', 0.081), ('java', 0.081), ('modules', 0.08), ('statistics', 0.076), ('aggregate', 0.076), ('defined', 0.075), ('values', 0.071), ('records', 0.071), ('map', 0.07), ('history', 0.069), ('implementing', 0.068), ('sent', 0.067), ('includes', 0.065), ('release', 0.062), ('startup', 0.06), ('supports', 0.058), ('automatically', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="397-tfidf-1" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>2 0.24616954 <a title="397-tfidf-2" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><p>3 0.21179111 <a title="397-tfidf-3" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>4 0.207223 <a title="397-tfidf-4" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>5 0.19859016 <a title="397-tfidf-5" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>Introduction: Hadoop is a distributed computing platform written in Java. It incorporates features similar to those of the   Google File System and of MapReduce to process vast amounts of data     "Hadoop is a Free Java software framework that supports data intensive distributed applications running on large clusters of commodity computers. It enables applications to easily scale out to thousands of nodes and petabytes of data" (Wikipedia)         * What platform does Hadoop run on?       * Java 1.5.x or higher, preferably from Sun       * Linux       * Windows for development       * Solaris</p><p>6 0.17794096 <a title="397-tfidf-6" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>7 0.17031954 <a title="397-tfidf-7" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>8 0.14065428 <a title="397-tfidf-8" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>9 0.13916145 <a title="397-tfidf-9" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>10 0.13712347 <a title="397-tfidf-10" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>11 0.13194253 <a title="397-tfidf-11" href="../high_scalability-2013/high_scalability-2013-01-14-MongoDB_and_GridFS_for_Inter_and_Intra_Datacenter_Data_Replication_.html">1386 high scalability-2013-01-14-MongoDB and GridFS for Inter and Intra Datacenter Data Replication </a></p>
<p>12 0.12486821 <a title="397-tfidf-12" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>13 0.12338401 <a title="397-tfidf-13" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<p>14 0.12299868 <a title="397-tfidf-14" href="../high_scalability-2010/high_scalability-2010-11-09-Facebook_Uses_Non-Stored_Procedures_to_Update_Social_Graphs.html">936 high scalability-2010-11-09-Facebook Uses Non-Stored Procedures to Update Social Graphs</a></p>
<p>15 0.11685464 <a title="397-tfidf-15" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>16 0.111424 <a title="397-tfidf-16" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>17 0.10664114 <a title="397-tfidf-17" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>18 0.10282169 <a title="397-tfidf-18" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>19 0.10149064 <a title="397-tfidf-19" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>20 0.10069351 <a title="397-tfidf-20" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, 0.043), (2, 0.012), (3, 0.019), (4, 0.031), (5, 0.059), (6, 0.127), (7, 0.029), (8, 0.111), (9, 0.081), (10, 0.08), (11, -0.022), (12, 0.141), (13, -0.177), (14, 0.066), (15, -0.112), (16, -0.066), (17, -0.016), (18, -0.041), (19, 0.068), (20, -0.03), (21, 0.012), (22, 0.096), (23, 0.051), (24, -0.004), (25, 0.057), (26, 0.083), (27, -0.024), (28, 0.036), (29, 0.031), (30, 0.065), (31, 0.126), (32, -0.059), (33, 0.029), (34, 0.021), (35, -0.016), (36, -0.09), (37, 0.006), (38, -0.014), (39, -0.022), (40, 0.014), (41, 0.107), (42, -0.103), (43, -0.031), (44, 0.01), (45, 0.005), (46, 0.045), (47, 0.037), (48, -0.0), (49, -0.007)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99653512 <a title="397-lsi-1" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>2 0.9014259 <a title="397-lsi-2" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>3 0.86486131 <a title="397-lsi-3" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>Introduction: Yahoo has developed a new language called Pig Latin that fit in a sweet spot between high-level declarative querying in the spirit of SQL, and low-level, procedural programming `a la map-reduce and combines best of both worlds.  The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. Pig has just graduated from the Apache Incubator and joined Hadoop as a subproject.  The paper has a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly.  References:  Apache Pig Wiki</p><p>4 0.85356551 <a title="397-lsi-4" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><p>5 0.7832194 <a title="397-lsi-5" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>Introduction: At Monday's  Cloud Computing Meetup ,  Paco Nathan  gave an excellent   Getting Started on Hadoop   talk ( slides ). I found one of Paco's strategies particularly interesting: consider when a service starts charging in cost calculations. Depending on your use case it may be cheaper to go with a more expensive service that charges only for work accomplished rather than charging for both work + startup time.
 
The example is comparing the cost of running Hadoop on AWS yourself versus using Amazon's prepackaged Hadoop service,  Elastic MapReduce  (EMR). The thought may have gone through your mind as it did mine that it doesn't necessarily make sense to use Amazon's Hadoop service. Why pay a premium for EMR when Hadoop will run directly on AWS?
 
One reason is that Amazon has made significant changes to Hadoop to make it run more efficiently and easily on AWS. The other more surprising reason is cost.
 
When starting a 500 node Hadoop cluster, for example, you have to wait for all the node</p><p>6 0.7366727 <a title="397-lsi-6" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>7 0.73425776 <a title="397-lsi-7" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>8 0.73301417 <a title="397-lsi-8" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>9 0.70505017 <a title="397-lsi-9" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>10 0.69396728 <a title="397-lsi-10" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>11 0.67373627 <a title="397-lsi-11" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>12 0.64164722 <a title="397-lsi-12" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>13 0.60511339 <a title="397-lsi-13" href="../high_scalability-2008/high_scalability-2008-09-03-MapReduce_framework_Disco.html">376 high scalability-2008-09-03-MapReduce framework Disco</a></p>
<p>14 0.59609628 <a title="397-lsi-14" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>15 0.59019887 <a title="397-lsi-15" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>16 0.5664252 <a title="397-lsi-16" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>17 0.56602287 <a title="397-lsi-17" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>18 0.5449931 <a title="397-lsi-18" href="../high_scalability-2008/high_scalability-2008-10-15-Need_help_with_your_Hadoop_deployment%3F_This_company_may_help%21.html">415 high scalability-2008-10-15-Need help with your Hadoop deployment? This company may help!</a></p>
<p>19 0.54009145 <a title="397-lsi-19" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<p>20 0.52551115 <a title="397-lsi-20" href="../high_scalability-2013/high_scalability-2013-09-05-Paper%3A_MillWheel%3A_Fault-Tolerant_Stream_Processing_at_Internet_Scale.html">1512 high scalability-2013-09-05-Paper: MillWheel: Fault-Tolerant Stream Processing at Internet Scale</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.165), (2, 0.181), (61, 0.019), (79, 0.11), (85, 0.02), (94, 0.078), (96, 0.309)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.87459427 <a title="397-lda-1" href="../high_scalability-2010/high_scalability-2010-07-30-Basho_Lives_up_to_their_Name_With_Consistent_Smashing.html">868 high scalability-2010-07-30-Basho Lives up to their Name With Consistent Smashing</a></p>
<p>Introduction: For some Friday Fun nerd style, I thought this  demonstration from Basho  on the difference between single master, sharding, and consistent smashing was really clever. I love the use of safety glasses! And it's harder to crash a server with a hammer than you might think...
 
          
 
RecommendedÂ reading:
  
 
  http://labs.google.com/papers/bigtable.html  
  http://research.yahoo.com/project/212</p><p>2 0.81554526 <a title="397-lda-2" href="../high_scalability-2008/high_scalability-2008-07-09-Federation_at_Flickr%3A_Doing_Billions_of_Queries_Per_Day.html">348 high scalability-2008-07-09-Federation at Flickr: Doing Billions of Queries Per Day</a></p>
<p>Introduction: Flickr's lone database guy Dathan Pattishall made his excellent presentation available on how on how Flickr scales its backend to handle tremendous loads. Some of this information is available in  Flickr Architecture , but the paper is so good it's worth another read. If you want to see sharding done right, at scale, take a look.</p><p>3 0.80765063 <a title="397-lda-3" href="../high_scalability-2008/high_scalability-2008-03-18-Database_Design_101.html">281 high scalability-2008-03-18-Database Design 101</a></p>
<p>Introduction: I am working on the design for my database and can't seem to come up with a firm schema.  I am torn between normalizing the data and dealing with the overhead of joins and denormalizing it for easy sharding.  The data is essentially music information per user: UserID, Artist, Album, Song.  This lends itself nicely to be normalized and have separate User, Artist, Album and Song databases with a table full of INTs to tie them together.  This will be in a mostly read based environment and with about 80% being searches of data by artist album or song.  By the time I begin the query for artist, album or song I will already have a list of UserID's to limit the search by.  The problem is that the tables can get unmanageably large pretty quickly and my plan was to shard off users once it got too big. Given this simple data relationship what are the pros and cons of normalizing the data vs denormalizing it?  Should I go with 4 separate, normalized tables or one 4 column table?  Perhaps it might</p><p>4 0.80663538 <a title="397-lda-4" href="../high_scalability-2007/high_scalability-2007-11-20-what_is_j2ee_stack.html">162 high scalability-2007-11-20-what is j2ee stack</a></p>
<p>Introduction: I see everyone talk about lamp stack is less than j2ee stack .i m newbie can anyone plz explain what is j2ee stack</p><p>same-blog 5 0.79057795 <a title="397-lda-5" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>6 0.78598446 <a title="397-lda-6" href="../high_scalability-2008/high_scalability-2008-10-17-Scaling_Spam_Eradication_Using_Purposeful_Games%3A_Die_Spammer_Die%21.html">422 high scalability-2008-10-17-Scaling Spam Eradication Using Purposeful Games: Die Spammer Die!</a></p>
<p>7 0.7816906 <a title="397-lda-7" href="../high_scalability-2013/high_scalability-2013-10-07-Ask_HS%3A_Is_Microsoft_the_Right_Technology_for_a_Scalable_Web-based_System%3F.html">1528 high scalability-2013-10-07-Ask HS: Is Microsoft the Right Technology for a Scalable Web-based System?</a></p>
<p>8 0.76679605 <a title="397-lda-8" href="../high_scalability-2007/high_scalability-2007-10-08-Paper%3A_Understanding_and_Building_High_Availability-Load_Balanced_Clusters.html">117 high scalability-2007-10-08-Paper: Understanding and Building High Availability-Load Balanced Clusters</a></p>
<p>9 0.73785537 <a title="397-lda-9" href="../high_scalability-2012/high_scalability-2012-03-21-The_Conspecific_Hybrid_Cloud.html">1212 high scalability-2012-03-21-The Conspecific Hybrid Cloud</a></p>
<p>10 0.73574823 <a title="397-lda-10" href="../high_scalability-2013/high_scalability-2013-11-15-Stuff_The_Internet_Says_On_Scalability_For_November_15th%2C_2013.html">1549 high scalability-2013-11-15-Stuff The Internet Says On Scalability For November 15th, 2013</a></p>
<p>11 0.73524749 <a title="397-lda-11" href="../high_scalability-2008/high_scalability-2008-10-30-The_case_for_functional_decomposition.html">435 high scalability-2008-10-30-The case for functional decomposition</a></p>
<p>12 0.73439789 <a title="397-lda-12" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>13 0.73330599 <a title="397-lda-13" href="../high_scalability-2011/high_scalability-2011-06-03-Stuff_The_Internet_Says_On_Scalability_For_June_3%2C_2011.html">1052 high scalability-2011-06-03-Stuff The Internet Says On Scalability For June 3, 2011</a></p>
<p>14 0.7068361 <a title="397-lda-14" href="../high_scalability-2010/high_scalability-2010-05-17-7_Lessons_Learned_While_Building_Reddit_to_270_Million_Page_Views_a_Month.html">828 high scalability-2010-05-17-7 Lessons Learned While Building Reddit to 270 Million Page Views a Month</a></p>
<p>15 0.69643623 <a title="397-lda-15" href="../high_scalability-2012/high_scalability-2012-04-16-Instagram_Architecture_Update%3A_What%E2%80%99s_new_with_Instagram%3F.html">1228 high scalability-2012-04-16-Instagram Architecture Update: What’s new with Instagram?</a></p>
<p>16 0.68647742 <a title="397-lda-16" href="../high_scalability-2008/high_scalability-2008-04-18-Scaling_Mania_at_MySQL_Conference_2008.html">303 high scalability-2008-04-18-Scaling Mania at MySQL Conference 2008</a></p>
<p>17 0.67690146 <a title="397-lda-17" href="../high_scalability-2013/high_scalability-2013-06-10-The_10_Deadly_Sins_Against_Scalability.html">1473 high scalability-2013-06-10-The 10 Deadly Sins Against Scalability</a></p>
<p>18 0.67611825 <a title="397-lda-18" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>19 0.66393137 <a title="397-lda-19" href="../high_scalability-2011/high_scalability-2011-05-19-Zynga%27s_Z_Cloud_-_Scale_Fast_or_Fail_Fast_by_Merging_Private_and_Public_Clouds.html">1044 high scalability-2011-05-19-Zynga's Z Cloud - Scale Fast or Fail Fast by Merging Private and Public Clouds</a></p>
<p>20 0.65850258 <a title="397-lda-20" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
