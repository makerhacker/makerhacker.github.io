<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-88" href="#">acl2010-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</h1>
<br/><p>Source: <a title="acl-2010-88-pdf" href="http://aclweb.org/anthology//P/P10/P10-1033.pdf">pdf</a></p><p>Author: Shujie Liu ; Chi-Ho Li ; Ming Zhou</p><p>Abstract: While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1</p><p>Reference: <a title="acl-2010-88-reference" href="../acl2010_reference/acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. [sent-6, score-0.406]
</p><p>2 On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. [sent-8, score-1.002]
</p><p>3 It  does synchronous parsing of two languages with phrasal and word-level alignment as by-product. [sent-10, score-0.275]
</p><p>4 For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al. [sent-11, score-0.261]
</p><p>5 Therefore all attempts to ITG alignment come with some pruning method. [sent-16, score-0.519]
</p><p>6 (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. [sent-18, score-0.656]
</p><p>7 HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. [sent-19, score-0.251]
</p><p>8 As all the principles behind these techniques have certain contribution in making good pruning decision, it is tempting to incorporate all these features in ITG pruning. [sent-20, score-0.302]
</p><p>9 In this paper, we propose a novel discriminative pruning framework for discriminative ITG. [sent-21, score-0.486]
</p><p>10 The pruning model uses no more training data than the discriminative ITG  parser itself, and it uses a log-linear model to integrate all features that help identify the correct span pair (like Model 1 probability and HMM posterior). [sent-22, score-0.8]
</p><p>11 On top of the discriminative pruning method, we also propose a discriminative ITG alignment system using hierarchical phrase pairs. [sent-23, score-0.839]
</p><p>12 In the following, some basic details on the ITG formalism and ITG parsing are first reviewed (Sections 2 and 3), followed by the definition of pruning in ITG (Section 4). [sent-24, score-0.312]
</p><p>13 From the viewpoint of word alignment, the terminal unary rules provide the links of word pairs, whereas the binary rules represent the reordering factor. [sent-39, score-0.326]
</p><p>14 Both ITG alignment 316  Proce dinUgsp osfa tlhae, 4S8wthed Aen n,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-46, score-0.241]
</p><p>15 Secondly, the simple ITG leads to redundancy if word alignment is the sole purpose of applying  ITG. [sent-50, score-0.28]
</p><p>16 3  Basics of ITG Parsing  Based on the rules in normal form, ITG word alignment is done in a similar way to chart parsing (Wu, 1997). [sent-68, score-0.372]
</p><p>17 The base step applies all relevant terminal unary rules to establish the links of word pairs. [sent-69, score-0.246]
</p><p>18 The word pairs are then combined into span pairs in all possible ways. [sent-70, score-0.375]
</p><p>19 Larger and larger span pairs are recursively built until the sentence pair is built. [sent-71, score-0.415]
</p><p>20 Each node (rectangle) represents a pair, marked with certain phrase category, of foreign span (F-span) and English span (E-span) (the upper half of the rectangle) and the associated alignment hypothesis (the lower half). [sent-73, score-1.071]
</p><p>21 Each graph like Figure 1(a) shows only one deri-  vation and also only one alignment hypothesis. [sent-74, score-0.241]
</p><p>22 Each hypernode (rectangle) comprises both a span pair (upper half) and the list of possible alignment hypotheses (lower half) for that span pair. [sent-76, score-0.983]
</p><p>23 The hyperedges show how larger span pairs are derived from smaller span pairs. [sent-77, score-0.584]
</p><p>24 Note that a hypernode may have more than one alignment hypothesis, since a hypernode may be derived through more than one hyperedge (e. [sent-78, score-0.439]
</p><p>25 Due to the use of normal form, the hypotheses of a span pair are different from each other. [sent-81, score-0.425]
</p><p>26 4  Pruning in ITG Parsing  The ITG parsing framework has three levels of pruning: 1) To discard some unpromising span pairs; 2) To discard some unpromising F-spans and/or E-spans; 3) To discard some unpromising alignment hypotheses for a particular span pair. [sent-82, score-1.045]
</p><p>27 (2008)) is very radical as it implies discarding  too many span pairs. [sent-85, score-0.255]
</p><p>28 It is empirically found to be highly harmful to alignment performance and therefore not adopted in this paper. [sent-86, score-0.241]
</p><p>29 The third type of pruning is equivalent to minimizing the beam size of alignment hypotheses in each hypernode. [sent-87, score-0.634]
</p><p>30 That is, during the bottom-up construction of the span pair repertoire, each span pair keeps only the best alignment hypothesis. [sent-89, score-0.929]
</p><p>31 Once the complete parse tree is built, the k-best list of the topmost span is obtained by minimally expanding the list of alignment hypotheses of minimal number of span pairs. [sent-90, score-0.821]
</p><p>32 The first type of pruning is equivalent to minimizing the number of hypernodes in a hypergraph. [sent-91, score-0.332]
</p><p>33 The task of ITG pruning is defined in this paper as the first type of pruning; i. [sent-92, score-0.278]
</p><p>34 1 The pruning method should maintain a balance between efficiency (run as quickly as possible) and performance (keep as many correct span pairs as possible). [sent-95, score-0.612]
</p><p>35 317  A naïve approach is that the required pruning method outputs a score given a span pair. [sent-98, score-0.559]
</p><p>36 5  The DPDI Framework  DPDI, the discriminative pruning model proposed in this paper, assigns score to a span pair ? [sent-100, score-0.752]
</p><p>37 1 Training Samples Discriminative approaches to word alignment use manually annotated alignment for sentence pairs. [sent-141, score-0.523]
</p><p>38 Discriminative pruning, however, handles not only a sentence pair but every possible span pair. [sent-142, score-0.365]
</p><p>39 Rather than recruiting annotators for marking span pairs, we modify the parsing algorithm in Section 3 so as to produce span pair annotation out of sentence-level annotation. [sent-144, score-0.633]
</p><p>40 If the sentence-level annotation satisfies the alignment constraints of ITG, then each F-span will have only one E-span in the parse tree. [sent-146, score-0.241]
</p><p>41 Consider the example in Figure 2, where the golden links in the alignment annotation are ? [sent-149, score-0.363]
</p><p>42 When such situation happens, we calculate the product of the inside and outside probability of each alignment hypothesis of the span pair, based on the probabilities of the links from some simpler alignment model2. [sent-168, score-0.968]
</p><p>43 The E-span with the most probable hypothesis is selected as the alignment of the F-span. [sent-169, score-0.27]
</p><p>44 It should be noted that this automatic span pair annotation may violate some of the links in the original sentence-level alignment annotation. [sent-172, score-0.668]
</p><p>45 f1 f2 f3 f4  e1  e2  e3  e4  Figure 3: An example of inside-out alignment The training samples thus obtained are positive training samples. [sent-178, score-0.322]
</p><p>46 Given an SMT system which produces, with 2 The formulae of the inside and outside probability of a span pair will be elaborated in Section 5. [sent-184, score-0.451]
</p><p>47 , 2000) probabilities of the word pairs inside and outside a span pair ( ? [sent-348, score-0.506]
</p><p>48 probability word pairs within the span pair):  of  ? [sent-359, score-0.325]
</p><p>49 probability the word pairs outside the span pair): ? [sent-385, score-0.361]
</p><p>50 The features are explained with the 319  example of Figure 5, in which the span pair in interest is ? [sent-414, score-0.368]
</p><p>51 The four links are produced by some simpler alignment model like HMM. [sent-419, score-0.351]
</p><p>52 The feature value of the example span pair is (2* 1)/(2+2)=0. [sent-455, score-0.344]
</p><p>53 is the number of links which are inconsistent with the phrase pair according to some simpler alignment model (e. [sent-485, score-0.569]
</p><p>54 is defined as the average ratio of foreign sentence length to Eng-  lish sentence length, and it is estimated to be around 1. [sent-514, score-0.225]
</p><p>55 The rationale underlying this feature is that the ratio of span length should not be too deviated from the average ratio of sentence length. [sent-516, score-0.402]
</p><p>56 a phrase of the foreign sentence usually occupies roughly the same position of the equivalent English phrase. [sent-561, score-0.247]
</p><p>57 The feature value for 3 An inconsistent link connects a word within the phrase pair to some word outside the phrase pair. [sent-562, score-0.42]
</p><p>58 (2009) show that posterior probabilities from the HMM alignment model is useful for pruning. [sent-570, score-0.268]
</p><p>59 Therefore, we design two new features by replacing the link count in link ratio and inconsistent link ratio with the sum of the link‟s posterior probability. [sent-571, score-0.349]
</p><p>60 6  The DITG Models  The discriminative ITG alignment can be conceived as a two-staged process. [sent-572, score-0.345]
</p><p>61 In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. [sent-574, score-0.59]
</p><p>62 Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). [sent-577, score-0.359]
</p><p>63 Each model selects the best alignment hypotheses of each span pair, given a set of features. [sent-578, score-0.54]
</p><p>64 The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. [sent-582, score-0.351]
</p><p>65 1  Word-to-word DITG  The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et. [sent-586, score-0.457]
</p><p>66 Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by including phrase pairs. [sent-600, score-0.219]
</p><p>67 HP-DITG extends Cherry and Lin‟s approach by not only employing simple phrase pairs but also hierarchical  phrase pairs (Chiang, 2007). [sent-604, score-0.281]
</p><p>68 refer to the English and foreign side of the i-th (simple/hierarchical) phrase pair respectively. [sent-614, score-0.294]
</p><p>69 During parsing, each span pair does not only examine all possible combinations of sub-span pairs using binary rules, but also checks if the yield of that span pair is exactly the same as that phrase pair. [sent-628, score-0.807]
</p><p>70 If so, then the alignment links within the phrase pair (which are obtained in standard phrase pair extraction procedure) are taken as an alternative alignment hypothesis of that span pair. [sent-629, score-1.165]
</p><p>71 1" during parsing, each span pair checks if it contains the lexical anchors "of" and " 的", and if the remaining words in its yield can form two sub-span pairs which fit the reordering constraint among  的  的  ? [sent-644, score-0.472]
</p><p>72 (Note that span pairs of any category in the ITG normal form grammar can substitute for ? [sent-647, score-0.372]
</p><p>73 ) If both conditions hold, then the span pair is assigned an alignment hypothesis which combines the alignment links among the lexical anchors (? [sent-650, score-0.961]
</p><p>74 The rule probabilities and lexical weights in both Englishto-foreign and foreign-to-English directions are estimated and taken as features, in addition to those features in W-DITG, in the discriminative model of alignment hypothesis selection. [sent-658, score-0.425]
</p><p>75 7  Evaluation  DPDI is evaluated against the baselines of Tictac-toe (TTT) pruning (Zhang and Gildea, 2005) and Dynamic Program (DP) pruning (Haghighi et al. [sent-659, score-0.556]
</p><p>76 Based on DPDI, HP-DITG is evaluated against the alignment systems GIZA++ and BITG. [sent-662, score-0.241]
</p><p>77 We will first evaluate pruning regarding the pruning decisions themselves. [sent-665, score-0.556]
</p><p>78 That is, the first evaluation metric, pruning error rate (henceforth PER), measures how many correct E-spans are discarded. [sent-666, score-0.343]
</p><p>79 The major drawback of PER is that not all decisions in pruning would impact on alignment quality, since certain F-spans are of little use to the entire ITG parse tree. [sent-667, score-0.519]
</p><p>80 An alternative criterion is the upper bound on alignment F-score, which essentially measures how many links in annotated alignment can be kept in ITG parse. [sent-668, score-0.677]
</p><p>81 The upper bound of recall is the hit score divided by the total number of golden links. [sent-740, score-0.236]
</p><p>82 The upper bound of alignment F-score  can thus be calculated as well. [sent-750, score-0.353]
</p><p>83 hit=m[e1ax,e{13A+]/:1[f,1 ,+f21]}=2 A→[C,C]  A→[C,C]  Finally, we also do end-to-end evaluation using both F-score in alignment and Bleu score in translation. [sent-751, score-0.267]
</p><p>84 2 Experiment Data Both discriminative pruning and alignment need  training data and test data. [sent-754, score-0.644]
</p><p>85 3  Small-scale Evaluation  The first set of experiments evaluates the performance of the three pruning methods using the small 241-sentence set. [sent-763, score-0.278]
</p><p>86 Each pruning method is plugged in both W-DITG and HP-DITG. [sent-764, score-0.278]
</p><p>87 IBM Model 1 and HMM alignment model are reimplemented as they are required by the three ITG pruning methods. [sent-765, score-0.519]
</p><p>88 number of E-spans per F-span), although DPDI spends a bit more time (due to the more complicated model), DPDI makes far less incorrect pruning decisions than the TTT. [sent-769, score-0.278]
</p><p>89 (2009) performs much poorer than the other two pruning methods. [sent-777, score-0.278]
</p><p>90 A possible explanation is that better pruning not only speeds up the parsing/alignment process but also guides the search process to focus on the most promising region of the search space. [sent-787, score-0.3]
</p><p>91 e531u728DP for HP-DITG 8 Conclusion and Future Work  This paper reviews word alignment through ITG  parsing, and clarifies the problem of ITG pruning. [sent-791, score-0.261]
</p><p>92 A discriminative pruning model and two discriminative ITG alignments systems are proposed. [sent-792, score-0.486]
</p><p>93 The pruning model is shown to be superior to all existing ITG pruning methods, and the HP-DITG alignment system is shown to improve state-ofthe-art alignment and translation quality. [sent-793, score-1.057]
</p><p>94 As the success of HP-DITG illustrates the merit of hierarchical phrase pair, in future we should investigate more features on the relationship between span pair and hierarchical phrase pair. [sent-796, score-0.592]
</p><p>95 The Normal Form Grammar  alignment F-score and Bleu score. [sent-798, score-0.241]
</p><p>96 On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair. [sent-801, score-0.275]
</p><p>97 On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning like DPDI guides the subsequent ITG alignment process so that less links inconsistent to good phrase pairs are produced. [sent-802, score-1.239]
</p><p>98 This also explains (in Tables 2 and 3) why DPDI with beam size 10 leads to higher Bleu than TTT with beam size 20, even though both pruning methods lead to roughly the same alignment F-score. [sent-803, score-0.649]
</p><p>99 , and rule schemas (7) are unary rules for alignment to null. [sent-811, score-0.335]
</p><p>100 ) If there are both English and foreign words linked to null, rule (5) ensures that those English 323  words linked to null precede those foreign words linked to null. [sent-817, score-0.41]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('itg', 0.642), ('dpdi', 0.388), ('pruning', 0.278), ('span', 0.255), ('alignment', 0.241), ('ditg', 0.145), ('foreign', 0.136), ('discriminative', 0.104), ('hypernode', 0.099), ('mert', 0.09), ('pair', 0.089), ('links', 0.083), ('phrase', 0.069), ('haghighi', 0.065), ('upper', 0.065), ('inconsistent', 0.06), ('hit', 0.059), ('link', 0.057), ('ttt', 0.057), ('inversion', 0.055), ('smt', 0.055), ('unary', 0.054), ('pairs', 0.05), ('terminal', 0.049), ('unpromising', 0.048), ('ratio', 0.047), ('bound', 0.047), ('beam', 0.045), ('transduction', 0.045), ('hypotheses', 0.044), ('hierarchical', 0.043), ('cherry', 0.043), ('interval', 0.043), ('hmm', 0.042), ('elaborated', 0.042), ('hypergraph', 0.041), ('rules', 0.04), ('samples', 0.039), ('golden', 0.039), ('normal', 0.037), ('rectangle', 0.036), ('error', 0.036), ('moore', 0.036), ('outside', 0.036), ('constraint', 0.035), ('linked', 0.035), ('dp', 0.034), ('parsing', 0.034), ('null', 0.033), ('chiang', 0.033), ('caonsdtdp', 0.032), ('appendix', 0.032), ('rationale', 0.032), ('zhang', 0.032), ('bilingual', 0.031), ('grammar', 0.03), ('denero', 0.029), ('inside', 0.029), ('hypothesis', 0.029), ('correct', 0.029), ('bleu', 0.029), ('aligns', 0.028), ('envelope', 0.028), ('hypernodes', 0.028), ('gildea', 0.028), ('probabilities', 0.027), ('simpler', 0.027), ('minimizing', 0.026), ('topmost', 0.026), ('score', 0.026), ('och', 0.025), ('discard', 0.024), ('harbin', 0.024), ('rge', 0.024), ('hyperedges', 0.024), ('basics', 0.024), ('inverted', 0.024), ('features', 0.024), ('rank', 0.024), ('henceforth', 0.023), ('anchors', 0.023), ('nist', 0.023), ('guides', 0.022), ('merits', 0.022), ('roughly', 0.021), ('half', 0.021), ('training', 0.021), ('sentence', 0.021), ('word', 0.02), ('reality', 0.02), ('hao', 0.02), ('reordering', 0.02), ('obstacle', 0.02), ('intervals', 0.02), ('translation', 0.019), ('robert', 0.019), ('wu', 0.019), ('liu', 0.019), ('leads', 0.019), ('vogel', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="88-tfidf-1" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Ming Zhou</p><p>Abstract: While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1</p><p>2 0.48013783 <a title="88-tfidf-2" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>3 0.27734733 <a title="88-tfidf-3" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>4 0.19125463 <a title="88-tfidf-4" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>5 0.17399105 <a title="88-tfidf-5" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>Author: Joern Wuebker ; Arne Mauser ; Hermann Ney</p><p>Abstract: Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering mod- els in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.</p><p>6 0.14374593 <a title="88-tfidf-6" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>7 0.1424554 <a title="88-tfidf-7" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>8 0.14119922 <a title="88-tfidf-8" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>9 0.12935045 <a title="88-tfidf-9" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>10 0.12674998 <a title="88-tfidf-10" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>11 0.12228099 <a title="88-tfidf-11" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>12 0.12152546 <a title="88-tfidf-12" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>13 0.10923846 <a title="88-tfidf-13" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>14 0.090879396 <a title="88-tfidf-14" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>15 0.087816633 <a title="88-tfidf-15" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>16 0.086434402 <a title="88-tfidf-16" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>17 0.082617342 <a title="88-tfidf-17" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>18 0.077470608 <a title="88-tfidf-18" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>19 0.074942082 <a title="88-tfidf-19" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>20 0.074205413 <a title="88-tfidf-20" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.211), (1, -0.292), (2, -0.024), (3, 0.01), (4, 0.043), (5, 0.074), (6, -0.145), (7, 0.095), (8, 0.102), (9, -0.109), (10, -0.121), (11, -0.131), (12, -0.151), (13, 0.099), (14, -0.059), (15, 0.027), (16, 0.003), (17, -0.003), (18, -0.099), (19, -0.046), (20, 0.065), (21, 0.087), (22, 0.062), (23, -0.037), (24, -0.074), (25, 0.07), (26, -0.028), (27, 0.103), (28, 0.015), (29, -0.012), (30, -0.009), (31, -0.015), (32, 0.026), (33, 0.085), (34, 0.084), (35, 0.072), (36, -0.046), (37, 0.084), (38, -0.018), (39, 0.02), (40, -0.022), (41, 0.139), (42, 0.028), (43, -0.066), (44, 0.015), (45, -0.01), (46, -0.034), (47, -0.045), (48, -0.06), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95068026 <a title="88-lsi-1" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Ming Zhou</p><p>Abstract: While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1</p><p>2 0.88781059 <a title="88-lsi-2" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>3 0.79162878 <a title="88-lsi-3" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>4 0.74056822 <a title="88-lsi-4" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>5 0.72848541 <a title="88-lsi-5" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>6 0.70894039 <a title="88-lsi-6" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>7 0.61909056 <a title="88-lsi-7" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>8 0.5795399 <a title="88-lsi-8" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>9 0.53042024 <a title="88-lsi-9" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>10 0.51006365 <a title="88-lsi-10" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>11 0.49183315 <a title="88-lsi-11" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>12 0.47714812 <a title="88-lsi-12" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>13 0.38208732 <a title="88-lsi-13" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>14 0.35748428 <a title="88-lsi-14" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>15 0.33292842 <a title="88-lsi-15" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>16 0.33009553 <a title="88-lsi-16" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>17 0.32825473 <a title="88-lsi-17" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>18 0.3168793 <a title="88-lsi-18" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>19 0.30967149 <a title="88-lsi-19" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>20 0.3089999 <a title="88-lsi-20" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.015), (18, 0.011), (25, 0.062), (39, 0.014), (42, 0.016), (44, 0.011), (56, 0.165), (59, 0.185), (73, 0.062), (78, 0.022), (83, 0.098), (84, 0.021), (98, 0.197)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97632289 <a title="88-lda-1" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>Author: Richard Power</p><p>Abstract: We describe the strategy currently pursued for verbalising OWL ontologies by sentences in Controlled Natural Language (i.e., combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals, classes, and properties) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology. We then show, through analysis of a corpus of ontologies, that although these assumptions could in principle be violated, they are overwhelmingly respected in practice by ontology developers.</p><p>2 0.92526549 <a title="88-lda-2" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>Author: Hiroshi Echizen-ya ; Kenji Araki</p><p>Abstract: As described in this paper, we propose a new automatic evaluation method for machine translation using noun-phrase chunking. Our method correctly determines the matching words between two sentences using corresponding noun phrases. Moreover, our method determines the similarity between two sentences in terms of the noun-phrase order of appearance. Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced us- ing automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR7. Experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency.</p><p>same-paper 3 0.91857278 <a title="88-lda-3" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Ming Zhou</p><p>Abstract: While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1</p><p>4 0.86691535 <a title="88-lda-4" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>5 0.8611753 <a title="88-lda-5" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>Author: Decong Li ; Sujian Li ; Wenjie Li ; Wei Wang ; Weiguang Qu</p><p>Abstract: It is a fundamental and important task to extract key phrases from documents. Generally, phrases in a document are not independent in delivering the content of the document. In order to capture and make better use of their relationships in key phrase extraction, we suggest exploring the Wikipedia knowledge to model a document as a semantic network, where both n-ary and binary relationships among phrases are formulated. Based on a commonly accepted assumption that the title of a document is always elaborated to reflect the content of a document and consequently key phrases tend to have close semantics to the title, we propose a novel semi-supervised key phrase extraction approach in this paper by computing the phrase importance in the semantic network, through which the influence of title phrases is propagated to the other phrases iteratively. Experimental results demonstrate the remarkable performance of this approach. 1</p><p>6 0.85971534 <a title="88-lda-6" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>7 0.85852665 <a title="88-lda-7" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>8 0.85777932 <a title="88-lda-8" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>9 0.85744536 <a title="88-lda-9" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>10 0.85656905 <a title="88-lda-10" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>11 0.85612631 <a title="88-lda-11" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>12 0.85608619 <a title="88-lda-12" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>13 0.8548007 <a title="88-lda-13" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>14 0.85469604 <a title="88-lda-14" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>15 0.85235089 <a title="88-lda-15" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>16 0.85186338 <a title="88-lda-16" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>17 0.85132754 <a title="88-lda-17" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>18 0.85098279 <a title="88-lda-18" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>19 0.85008621 <a title="88-lda-19" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>20 0.84922016 <a title="88-lda-20" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
