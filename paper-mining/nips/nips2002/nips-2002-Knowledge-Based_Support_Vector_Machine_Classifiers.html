<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-121" href="#">nips2002-121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</h1>
<br/><p>Source: <a title="nips-2002-121-pdf" href="http://papers.nips.cc/paper/2222-knowledge-based-support-vector-machine-classifiers.pdf">pdf</a></p><p>Author: Glenn M. Fung, Olvi L. Mangasarian, Jude W. Shavlik</p><p>Abstract: Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. The resulting formulation leads to a linear program that can be solved efficiently. Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. Keywords: use and refinement of prior knowledge, support vector machines, linear programming 1</p><p>Reference: <a title="nips-2002-121-reference" href="../nips2002_reference/nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. [sent-6, score-0.854]
</p><p>2 The resulting formulation leads to a linear program that can be solved efficiently. [sent-7, score-0.337]
</p><p>3 Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. [sent-8, score-0.187]
</p><p>4 Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. [sent-9, score-0.776]
</p><p>5 One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. [sent-10, score-0.763]
</p><p>6 Keywords: use and refinement of prior knowledge, support vector machines, linear programming  1  Introduction  Support vector machines (SVMs) have played a major role in classification problems [18,3, 11]. [sent-11, score-0.675]
</p><p>7 However unlike other classification tools such as knowledge-based neural networks [16, 17, 7], little work [15] has gone into incorporating prior knowledge into support vector machines. [sent-12, score-0.728]
</p><p>8 In this work we present a novel approach to incorporating prior knowledge in the form of polyhedral knowledge sets in the input space of the given data. [sent-13, score-1.097]
</p><p>9 These knowledge sets, which can be as simple as cubes, are supposed to belong to one of two categories into which all the data is divided. [sent-14, score-0.331]
</p><p>10 Thus, a single knowledge set can be interpreted as a generalization of a training example, which typically consists of a single point in input space. [sent-15, score-0.332]
</p><p>11 In contrast, each of our knowledge sets consists of a region in the same space. [sent-16, score-0.403]
</p><p>12 By using a powerful tool from mathematical programming, theorems of the alternative [9, Chapter 2], we are able to embed such prior data into a linear program that can be efficiently solved by any of the publicly available solvers. [sent-17, score-0.329]
</p><p>13 In Section 2 we describe the linear support vector machine classifier and give a linear program for it. [sent-19, score-0.747]
</p><p>14 We then describe how prior knowledge, in the form of polyhedral knowledge sets belonging to one of two classes can be characterized. [sent-20, score-0.809]
</p><p>15 In Section 3 we incorporate these polyhedral sets into our linear programming formulation which results in our knowledge-based support vector machine (KSVM) formulation (19). [sent-21, score-0.992]
</p><p>16 This formulation is capable of generating a linear classifier based on real data and/or prior knowledge. [sent-22, score-0.613]
</p><p>17 Section 4 gives a brief summary of numerical results that compare various linear and nonlinear classifiers with and without the incorporation of prior knowledge. [sent-23, score-0.43]
</p><p>18 A separating plane, with respect to two given point sets A and B in R n , is a plane that attempts to separate R n into two halfspaces such that each open halfspace contains points mostly of A or B. [sent-38, score-0.619]
</p><p>19 A bounding plane to the set A is a plane that places A in one of the two closed halfspaces that the plane generates. [sent-39, score-0.865]
</p><p>20 III denotes the I-norm as defined in the Introduction, y is a vector of slack variables measuring empirical error and (w, 'Y) characterize a separating plane depicted in Figure 1. [sent-46, score-0.492]
</p><p>21 That this problem is indeed a linear program, can be easily seen from the equivalent formulation: min  (W ,"Y ,y ,t)ERn +l +=+n  {ve'y+e't I D(Aw - q) +y ~ e,t ~ w ~ -t,y ~ a},  (2)  where e is a vector of ones of appropriate dimension. [sent-47, score-0.198]
</p><p>22 For economy of notation we shall use the first formulation (1) with the understanding that computational implementation is via (2). [sent-48, score-0.137]
</p><p>23 As depicted in Figure l(a), w is the normal to the bounding planes: x'w  = 'Y + 1, x'w = 'Y - 1,  (3)  that bound the points belonging to the sets A + and A-respectively. [sent-49, score-0.417]
</p><p>24 (4)  Consequently, the plane: x'w = 'Y,  (5)  midway between the bounding planes (3), is a separating plane that separates points belonging to A + from those belonging to A-completely if y = 0, else only approximately. [sent-52, score-0.811]
</p><p>25 The I-norm term Ilwlll in (1), which is half the reciprocal of the distance 11,,7111 measured using the oo-norm distance [10] between the two bounding planes of  (3) (see Figure l(a)), maximizes this distance, often called the "margin". [sent-53, score-0.25]
</p><p>26 Maximizing the margin enhances the generalization capability of a support vector machine [18, 3]. [sent-54, score-0.247]
</p><p>27 If the classes are linearly inseparable, then the two planes bound the two classes with a "soft margin" (i. [sent-55, score-0.137]
</p><p>28 bound approximately with some error) determined by the nonnegative error variable y, that is: AiW + Yi 2: ry + 1, for Dii = 1, AiW - Yi ::; ry - 1, for Dii = -1. [sent-57, score-0.42]
</p><p>29 (6) The I-norm of the error variable Y is minimized parametrically with weight /J in (1), resulting in an approximate separating plane (5) which classifies as follows: x E A+ if sign(x'w - ry) = 1, x E A- if sign(x'w - ry) = -1. [sent-58, score-0.344]
</p><p>30 (7) Suppose now that we have prior information of the following type. [sent-59, score-0.129]
</p><p>31 All points x lying in the polyhedral set determined by the linear inequalities: Bx ::; b, (8) belong to class A +. [sent-60, score-0.327]
</p><p>32 Looking at Figure 1 (a) or at the inequalities (4) we conclude that the following implication must hold: Bx::; b ===? [sent-62, score-0.184]
</p><p>33 (9) That is, the knowledge set {x I Bx ::; b} lies on the A + side of the bounding plane x'w = ry+ 1. [sent-64, score-0.644]
</p><p>34 Later, in (19), we will accommodate the case when the implication (9) cannot be satisfied exactly by the introduction of slack error variables. [sent-65, score-0.211]
</p><p>35 For now, assuming that the implication (9) holds for a given (w, ry), it follows that (9) is equivalent to: Bx ::; b, x'w < ry + 1, has no solution x. [sent-66, score-0.352]
</p><p>36 In fact, under the natural assumption that the prior knowledge set {x I Bx ::; b} is nonempty, the forward implication: (10)===? [sent-69, score-0.427]
</p><p>37 Then for a given (w, ry), the implication (9) is equivalent to the statement (11). [sent-77, score-0.176]
</p><p>38 In other words, the set {x I Bx ::; b} lies in the halfspace {x I w' x 2: ry + I} if and only if there exists u such that B'u + w = 0, b'u + ry + 1 ::; 0 and u 2: O. [sent-78, score-0.474]
</p><p>39 8] we have that (10) is equivalent to either: B'u + w = 0, b'u + ry + 1::; 0, u 2: 0, having solution (u, w), (13) or B'u = 0, b'u < 0, u 2: 0, having solution u. [sent-82, score-0.195]
</p><p>40 D  This proposition will play a key role in incorporating knowledge sets, such as {x I Bx ::; b}, into one of two categories in a support vector classifier formulation as demonstrated in the next section. [sent-85, score-1.031]
</p><p>41 - 15  -15  -20  X'W= Y +1  -30 ~---:---j x'w= y  -40  -~·~0------ '5 ---- '0 ~~ -~~---- 5 ------~----~ -~  -45 '--------~----~------~----~----~  -20  - 15  - 10  (a) Figure 1:  -5  (b)  (a): A linear SVM separation for 200 points in R2 using the linear programming  formulation (1). [sent-86, score-0.521]
</p><p>42 (b): A linear SVM separation for the salTIe 200 points in R2 as those in  Figure l(a) but using the linear programming forlTIulation (19) which incorporates three knowledge sets: { x  I B ' x :'0  b'} into the halfspace of A + , and { x  into the halfspace of A - , as depicted above. [sent-87, score-0.894]
</p><p>43 I C'x :'0  c'}, { x  I C 2 x :'0  c2 }  Note the substantial difference between the  linear classifiers x' w = , of both figures. [sent-88, score-0.176]
</p><p>44 3  Knowledge-Based SVM Classification  We describe now how to incorporate prior knowledge in the form of polyhedral sets into our linear programming SVM classifier formulation (1). [sent-89, score-1.35]
</p><p>45 We assume that we are given the following knowledge sets:  k sets belonging to A+ : {x I B ix ::; bi } , i = 1, . [sent-90, score-0.548]
</p><p>46 ,k IZ sets belonging to A- : {x I eix::; ci }, i = 1, . [sent-93, score-0.197]
</p><p>47 1 that, relative to the bounding planes (3):  There exist u i , i = 1, . [sent-97, score-0.25]
</p><p>48 ,1:fi _ _ -  e  (17)  We now incorporate the knowledge sets (16) into the SVM linear programming formulation (1) classifier, by adding the conditions (17) as constraints to it as follows: min w" ,(y ,u i ,v j )2':O  s. [sent-110, score-0.806]
</p><p>49 ,IZ  (18)  This linear programming formulation will ensure that each of the knowledge sets { x I BiX::; bi } , i = 1, . [sent-124, score-0.787]
</p><p>50 ,IZ lie on the appropriate side of the bounding planes (3). [sent-130, score-0.25]
</p><p>51 However, there is no guarantee that such bounding planes exist that will precisely separate these two classes of knowledge sets, just as there is no a priori guarantee that the original points b elonging to the sets A + and A-are linearly separable. [sent-131, score-0.716]
</p><p>52 ,£, just like the slack error variable y of the SVM formulation (1), and attempt to drive these error variables to zero by modifying our last formulation above as follows: e  k  . [sent-138, score-0.389]
</p><p>53 ,£  (19)  This is our final knowledge-based linear programming formulation which incorporates the knowledge sets (16) into the linear classifier with weight j. [sent-152, score-1.081]
</p><p>54 L = a then the linear program (19) degenerates to (1), the linear program associated with an ordinary linear SVM. [sent-157, score-0.567]
</p><p>55 However, if set v = 0, then the linear program (19) generates a linear SVM that is strictly based on knowledge sets, but not on any specific training data. [sent-158, score-0.643]
</p><p>56 This will be demonstrated in the breast cancer dataset of Section 4. [sent-160, score-0.279]
</p><p>57 Note that the I-norm term Ilwlll can be replaced by one half the 2-norm squared, ~llwll~, which is the usual margin maximization term for ordinary support vector machine classifiers [18, 3]. [sent-161, score-0.432]
</p><p>58 However, this changes the linear program (19) to a quadratic program which typically takes longer time to solve. [sent-162, score-0.321]
</p><p>59 For standard SVMs, support vectors consist of all data points which are the complement of the data points that can be dropped from the problem without changing the separating plane (5) [18, 11]. [sent-163, score-0.551]
</p><p>60 Thus for our knowledge-based linear programming formulation (19), support vectors correspond to data points (rows of the matrix A) for which the Lagrange multipliers are nonzero, because solving (19) with these data points only will give the same answer as solving (19) with the entire matrix A. [sent-164, score-0.63]
</p><p>61 The concept of support vectors has to be modified as follows for our knowledge sets. [sent-165, score-0.44]
</p><p>62 Since each knowledge set in (16) is represented by a matrix Bi or j , each row of these matrices can be thought of as characterizing a boundary plane of the knowledge set. [sent-166, score-0.829]
</p><p>63 In our formulation (19) above, such rows are wiped out if the corresponding components of the variables u i or v j are zero at an optimal solution. [sent-167, score-0.168]
</p><p>64 We call the complement of these components of the the knowledge sets (16), support constraints. [sent-168, score-0.514]
</p><p>65 Deleting constraints (rows of Bi or j ), for which the corresponding components of u i or v j are zero, will not alter the solution of the knowledge-based linear program (19). [sent-169, score-0.2]
</p><p>66 Deletion of non-support constraints can be considered a refinement of prior knowledge [17]. [sent-171, score-0.511]
</p><p>67 Another type of of refinement of prior knowledge may occur when the separating plane x' w = I' intersects one of the knowledge sets. [sent-172, score-1.176]
</p><p>68 In such a case the plane x'w = I' can be added as an inequality to the knowledge set it intersects. [sent-173, score-0.531]
</p><p>69 e  e  We demonstrate the geometry of incorporating knowledge sets by considering a synthetic example in R2 with m = 200 points, 100 of which are in A + and the other 100 in A -. [sent-175, score-0.485]
</p><p>70 Figure 1 (a) depicts ordinary linear separation using the linear SVM formulation (1). [sent-176, score-0.431]
</p><p>71 We now incorporate three knowledge sets into the the problem:  {x I Blx ::; bl } belonging to A+ and {x I Clx ::; c l } and {x I C 2 x ::; c2 } belonging to A -, and solve our linear program (19) with f-l = 100 and v = 1. [sent-177, score-0.874]
</p><p>72 We depict the new linear separation in Figure 1 (b) and note the substantial change generated in the linear separation by the incorporation of these three knowledge sets. [sent-178, score-0.622]
</p><p>73 Also note that since the plane x'w = "( intersects the knowledge set {x I BlX ::; bl }, this knowledge set can be refined to the following {x I B 1 X ::; bl, w' x 2: "(}. [sent-179, score-0.935]
</p><p>74 4  Numerical Testing  Numerical tests, which are described in detail in [6], were carried out on the DNA promoter recognition dataset [17] and the Wisconsin prognostic breast cancer dataset WPBC (ftp:j /ftp. [sent-180, score-0.463]
</p><p>75 The prior knowledge for this dataset, which consists of a set of 14 prior rules, matches none of the examples of the training set. [sent-188, score-0.59]
</p><p>76 However, they do capture significant information about promoters and it is known that incorporating them into a classifier results in a more accurate classifier [17]. [sent-190, score-0.697]
</p><p>77 These 14 prior rules were converted in a straightforward manner [6] into 64 knowledge sets. [sent-191, score-0.525]
</p><p>78 After expressing the prior knowledge in the form of polyhedral sets and applying KSVM, we obtained 5 errors out of 106 (5/106). [sent-200, score-0.717]
</p><p>79 KSVM was also compared with [16] where a hybrid learning system maps problem specific prior knowledge, represented in propositional logic into neural networks and then, refines this reformulated knowledge using back propagation. [sent-203, score-0.459]
</p><p>80 However, it is important to note that our classifier is a much simpler linear classifier, sign(x'w - "(), while the neural network classifier of KBANN is a considerably more complex nonlinear classifier. [sent-206, score-0.615]
</p><p>81 Furthermore, we note that KSVM is simpler to implement than KBANN and requires merely a commonly available linear programming solver. [sent-207, score-0.194]
</p><p>82 In addition, KSVM which is a linear support vector machine classifier, improves by 44. [sent-208, score-0.279]
</p><p>83 4% the error of an ordinary linear I-norm SVM classifier that does not utilize prior knowledge sets. [sent-209, score-0.892]
</p><p>84 The second dataset used in our numerical tests was the Wisconsin breast cancer prognosis dataset WPBC using a 60-month cutoff for predicting recurrence or nonrecurrence of the disease [2]. [sent-210, score-0.563]
</p><p>85 9)  ===}  NON RECUR  It is important to note that the rules described above can be applied directly to classify only 32 of the given 110 given points of the training dataset and correctly classify 22 of these 32 points. [sent-213, score-0.347]
</p><p>86 Hence, if the rules are applied as a classifier by themselves the classification accuracy would be 20%. [sent-215, score-0.425]
</p><p>87 As such, these rules are not very useful by themselves and doctors use them in conjunction with other rules [8]. [sent-216, score-0.265]
</p><p>88 However, using our approach the rules were converted to linear inequalities and used in our KSVM algorithm without any use of the data, i. [sent-217, score-0.235]
</p><p>89 The resulting linear classifier in the 2-dimensional space of L(ymph) and T(umor) achieved 66. [sent-220, score-0.347]
</p><p>90 This result is remarkable because our knowledge-based formulation can be applied to problems where training data may not be available whereas expert knowledge may be readily available in the form of knowledge sets. [sent-224, score-0.849]
</p><p>91 This fact makes this method considerably different from previous hybrid methods like KBANN where training examples are needed in order to refine prior knowledge. [sent-225, score-0.163]
</p><p>92 5  Conclusion & Future Directions  We have proposed an efficient procedure for incorporating prior knowledge in the form of knowledge sets into a linear support vector machine classifier either in combination with a given dataset or based solely on the knowledge sets. [sent-227, score-1.849]
</p><p>93 This novel and promising approach of handling prior knowledge is worthy of further study, especially ways to handle and simplify the combinatorial nature of incorporating prior knowledge into linear inequalities. [sent-228, score-1.015]
</p><p>94 A class of possible future applications might be to problems where training data may not be easily available whereas expert knowledge may be readily available in the form of knowledge sets. [sent-229, score-0.712]
</p><p>95 This would correspond to solving our knowledge based linear program (19) with l/ = O. [sent-230, score-0.529]
</p><p>96 A typical example of this type was breast cancer prognosis [8] where knowledge sets by themselves generated a linear classifier as good as any classifier based on data points. [sent-231, score-1.311]
</p><p>97 This is a new way of incorporating prior knowledge into powerful support vector machine classifiers. [sent-232, score-0.709]
</p><p>98 Also, the concept of support constraints as discussed at the end of Section 3, warrants further study that may lead to a systematic simplification of prior knowledge sets. [sent-233, score-0.538]
</p><p>99 Feature selection via concave minimization and support vector machines. [sent-245, score-0.16]
</p><p>100 Prior knowledge and the creation of "virtual" examples for RBF networks. [sent-287, score-0.298]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('knowledge', 0.298), ('classifier', 0.268), ('ksvm', 0.238), ('plane', 0.233), ('bx', 0.203), ('ry', 0.195), ('polyhedral', 0.185), ('ftp', 0.138), ('formulation', 0.137), ('planes', 0.137), ('ilwlll', 0.132), ('kbann', 0.132), ('prior', 0.129), ('implication', 0.126), ('wisconsin', 0.123), ('program', 0.121), ('programming', 0.115), ('bounding', 0.113), ('support', 0.111), ('aiw', 0.106), ('prognosis', 0.106), ('sets', 0.105), ('svm', 0.104), ('rules', 0.098), ('classifiers', 0.097), ('breast', 0.097), ('belonging', 0.092), ('promoter', 0.092), ('dii', 0.092), ('dataset', 0.092), ('cancer', 0.09), ('ordinary', 0.088), ('halfspace', 0.084), ('refinement', 0.084), ('artificial', 0.082), ('incorporating', 0.082), ('separating', 0.081), ('promoters', 0.079), ('linear', 0.079), ('shavlik', 0.078), ('sign', 0.071), ('aw', 0.07), ('incorporation', 0.07), ('ern', 0.069), ('doctors', 0.069), ('points', 0.063), ('mangasarian', 0.063), ('dna', 0.062), ('classification', 0.059), ('inequalities', 0.058), ('slack', 0.055), ('numerical', 0.055), ('proposition', 0.053), ('bi', 0.053), ('blx', 0.053), ('eix', 0.053), ('farkas', 0.053), ('fung', 0.053), ('halfspaces', 0.053), ('intersects', 0.053), ('lymph', 0.053), ('nonhomogeneous', 0.053), ('olvi', 0.053), ('recur', 0.053), ('rumelhart', 0.053), ('towell', 0.053), ('wpbc', 0.053), ('madison', 0.053), ('bl', 0.053), ('statement', 0.05), ('vector', 0.049), ('separation', 0.048), ('margin', 0.047), ('expert', 0.047), ('contradiction', 0.046), ('tumor', 0.046), ('depicted', 0.044), ('sj', 0.042), ('mining', 0.041), ('equivalence', 0.04), ('machine', 0.04), ('min', 0.038), ('ri', 0.038), ('readily', 0.035), ('november', 0.035), ('training', 0.034), ('briefly', 0.034), ('incorporate', 0.034), ('categories', 0.033), ('specific', 0.032), ('ones', 0.032), ('tests', 0.031), ('denoted', 0.031), ('cj', 0.031), ('follows', 0.031), ('rows', 0.031), ('solving', 0.031), ('classify', 0.03), ('sciences', 0.03), ('error', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="121-tfidf-1" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>Author: Glenn M. Fung, Olvi L. Mangasarian, Jude W. Shavlik</p><p>Abstract: Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. The resulting formulation leads to a linear program that can be solved efficiently. Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. Keywords: use and refinement of prior knowledge, support vector machines, linear programming 1</p><p>2 0.17322996 <a title="121-tfidf-2" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>3 0.10973924 <a title="121-tfidf-3" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>Author: Yves Grandvalet, Stéphane Canu</p><p>Abstract: This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors deﬁning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters deﬁning the classiﬁer. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem.</p><p>4 0.10669032 <a title="121-tfidf-4" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>5 0.10145215 <a title="121-tfidf-5" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>Author: Shantanu Chakrabartty, Gert Cauwenberghs</p><p>Abstract: Forward decoding kernel machines (FDKM) combine large-margin classifiers with hidden Markov models (HMM) for maximum a posteriori (MAP) adaptive sequence estimation. State transitions in the sequence are conditioned on observed data using a kernel-based probability model trained with a recursive scheme that deals effectively with noisy and partially labeled data. Training over very large data sets is accomplished using a sparse probabilistic support vector machine (SVM) model based on quadratic entropy, and an on-line stochastic steepest descent algorithm. For speaker-independent continuous phone recognition, FDKM trained over 177 ,080 samples of the TlMIT database achieves 80.6% recognition accuracy over the full test set, without use of a prior phonetic language model.</p><p>6 0.10139573 <a title="121-tfidf-6" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>7 0.087671749 <a title="121-tfidf-7" href="./nips-2002-Learning_with_Multiple_Labels.html">135 nips-2002-Learning with Multiple Labels</a></p>
<p>8 0.07766211 <a title="121-tfidf-8" href="./nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>9 0.076954536 <a title="121-tfidf-9" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>10 0.07609117 <a title="121-tfidf-10" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>11 0.072545648 <a title="121-tfidf-11" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>12 0.068828933 <a title="121-tfidf-12" href="./nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">19 nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>13 0.068457223 <a title="121-tfidf-13" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>14 0.066518061 <a title="121-tfidf-14" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>15 0.065395758 <a title="121-tfidf-15" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>16 0.064999878 <a title="121-tfidf-16" href="./nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>17 0.062403917 <a title="121-tfidf-17" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>18 0.055780429 <a title="121-tfidf-18" href="./nips-2002-Informed_Projections.html">115 nips-2002-Informed Projections</a></p>
<p>19 0.05531038 <a title="121-tfidf-19" href="./nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">56 nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>20 0.054741073 <a title="121-tfidf-20" href="./nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">99 nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2002_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.192), (1, -0.089), (2, 0.02), (3, -0.049), (4, 0.033), (5, 0.011), (6, 0.013), (7, 0.006), (8, -0.002), (9, -0.014), (10, 0.092), (11, -0.112), (12, -0.019), (13, -0.123), (14, -0.028), (15, -0.078), (16, 0.066), (17, 0.032), (18, 0.091), (19, 0.052), (20, -0.007), (21, 0.005), (22, 0.166), (23, -0.113), (24, -0.026), (25, -0.145), (26, 0.072), (27, -0.027), (28, 0.044), (29, -0.108), (30, 0.037), (31, -0.067), (32, -0.091), (33, -0.022), (34, 0.085), (35, 0.076), (36, 0.021), (37, 0.057), (38, -0.04), (39, -0.021), (40, -0.116), (41, 0.063), (42, 0.013), (43, 0.009), (44, -0.0), (45, -0.057), (46, 0.14), (47, 0.161), (48, 0.154), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9606216 <a title="121-lsi-1" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>Author: Glenn M. Fung, Olvi L. Mangasarian, Jude W. Shavlik</p><p>Abstract: Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. The resulting formulation leads to a linear program that can be solved efficiently. Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. Keywords: use and refinement of prior knowledge, support vector machines, linear programming 1</p><p>2 0.63650721 <a title="121-lsi-2" href="./nips-2002-PAC-Bayes_%26_Margins.html">161 nips-2002-PAC-Bayes & Margins</a></p>
<p>Author: John Langford, John Shawe-Taylor</p><p>Abstract: unkown-abstract</p><p>3 0.56472462 <a title="121-lsi-3" href="./nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><p>4 0.51839906 <a title="121-lsi-4" href="./nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><p>5 0.47807458 <a title="121-lsi-5" href="./nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">165 nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>Author: empty-author</p><p>Abstract: We discuss the problem of ranking k instances with the use of a</p><p>6 0.46492901 <a title="121-lsi-6" href="./nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">178 nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>7 0.45884904 <a title="121-lsi-7" href="./nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">93 nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>8 0.44080275 <a title="121-lsi-8" href="./nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">192 nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>9 0.39414105 <a title="121-lsi-9" href="./nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">24 nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>10 0.36124435 <a title="121-lsi-10" href="./nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">86 nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>11 0.35395238 <a title="121-lsi-11" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>12 0.34316221 <a title="121-lsi-12" href="./nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">16 nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>13 0.34245363 <a title="121-lsi-13" href="./nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">140 nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>14 0.34018338 <a title="121-lsi-14" href="./nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">6 nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>15 0.32793728 <a title="121-lsi-15" href="./nips-2002-The_Decision_List_Machine.html">194 nips-2002-The Decision List Machine</a></p>
<p>16 0.32714245 <a title="121-lsi-16" href="./nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">52 nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>17 0.32085675 <a title="121-lsi-17" href="./nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">157 nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>18 0.3200326 <a title="121-lsi-18" href="./nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">114 nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>19 0.3155756 <a title="121-lsi-19" href="./nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">35 nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>20 0.3128581 <a title="121-lsi-20" href="./nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">37 nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2002_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(42, 0.048), (54, 0.672), (55, 0.023), (68, 0.014), (74, 0.045), (92, 0.041), (98, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99606806 <a title="121-lda-1" href="./nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>Author: Glenn M. Fung, Olvi L. Mangasarian, Jude W. Shavlik</p><p>Abstract: Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. The resulting formulation leads to a linear program that can be solved efficiently. Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. Keywords: use and refinement of prior knowledge, support vector machines, linear programming 1</p><p>2 0.99561977 <a title="121-lda-2" href="./nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">77 nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert function Spaces. We introduce a concept of scalesensitive effective data dimension, and show that it characterizes the convergence rate of the underlying learning problem. Using this concept, we can naturally extend results for parametric estimation problems in ﬁnite dimensional spaces to non-parametric kernel learning methods. We derive upper bounds on the generalization performance and show that the resulting convergent rates are optimal under various circumstances.</p><p>3 0.99400938 <a title="121-lda-3" href="./nips-2002-Automatic_Alignment_of_Local_Representations.html">36 nips-2002-Automatic Alignment of Local Representations</a></p>
<p>Author: Yee W. Teh, Sam T. Roweis</p><p>Abstract: We present an automatic alignment procedure which maps the disparate internal representations learned by several local dimensionality reduction experts into a single, coherent global coordinate system for the original data space. Our algorithm can be applied to any set of experts, each of which produces a low-dimensional local representation of a highdimensional input. Unlike recent efforts to coordinate such models by modifying their objective functions [1, 2], our algorithm is invoked after training and applies an efﬁcient eigensolver to post-process the trained models. The post-processing has no local optima and the size of the system it must solve scales with the number of local models rather than the number of original data points, making it more efﬁcient than model-free algorithms such as Isomap [3] or LLE [4]. 1 Introduction: Local vs. Global Dimensionality Reduction Beyond density modelling, an important goal of unsupervised learning is to discover compact, informative representations of high-dimensional data. If the data lie on a smooth low dimensional manifold, then an excellent encoding is the coordinates internal to that manifold. The process of determining such coordinates is dimensionality reduction. Linear dimensionality reduction methods such as principal component analysis and factor analysis are easy to train but cannot capture the structure of curved manifolds. Mixtures of these simple unsupervised models [5, 6, 7, 8] have been used to perform local dimensionality reduction, and can provide good density models for curved manifolds, but unfortunately such mixtures cannot do dimensionality reduction. They do not describe a single, coherent low-dimensional coordinate system for the data since there is no pressure for the local coordinates of each component to agree. Roweis et al [1] recently proposed a model which performs global coordination of local coordinate systems in a mixture of factor analyzers (MFA). Their model is trained by maximizing the likelihood of the data, with an additional variational penalty term to encourage the internal coordinates of the factor analyzers to agree. While their model can trade off modelling the data and having consistent local coordinate systems, it requires a user given trade-off parameter, training is quite inefﬁcient (although [2] describes an improved training algorithm for a more constrained model), and it has quite serious local minima problems (methods like LLE [4] or Isomap [3] have to be used for initialization). In this paper we describe a novel, automatic way to align the hidden representations used by each component of a mixture of dimensionality reducers into a single global representation of the data throughout space. Given an already trained mixture, the alignment is achieved by applying an eigensolver to a matrix constructed from the internal representations of the mixture components. Our method is efﬁcient, simple to implement, and has no local optima in its optimization nor any learning rates or annealing schedules. 2 The Locally Linear Coordination Algorithm H 9¥ EI¡ CD66B9 ©9B 766 % G F 5 #</p><p>4 0.99035597 <a title="121-lda-4" href="./nips-2002-Global_Versus_Local_Methods_in_Nonlinear_Dimensionality_Reduction.html">97 nips-2002-Global Versus Local Methods in Nonlinear Dimensionality Reduction</a></p>
<p>Author: Vin D. Silva, Joshua B. Tenenbaum</p><p>Abstract: Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps.</p><p>5 0.9627921 <a title="121-lda-5" href="./nips-2002-Discriminative_Binaural_Sound_Localization.html">67 nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>Author: Ehud Ben-reuven, Yoram Singer</p><p>Abstract: Time difference of arrival (TDOA) is commonly used to estimate the azimuth of a source in a microphone array. The most common methods to estimate TDOA are based on ﬁnding extrema in generalized crosscorrelation waveforms. In this paper we apply microphone array techniques to a manikin head. By considering the entire cross-correlation waveform we achieve azimuth prediction accuracy that exceeds extrema locating methods. We do so by quantizing the azimuthal angle and treating the prediction problem as a multiclass categorization task. We demonstrate the merits of our approach by evaluating the various approaches on Sony’s AIBO robot.</p><p>6 0.96239483 <a title="121-lda-6" href="./nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">104 nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>7 0.91428465 <a title="121-lda-7" href="./nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">156 nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>8 0.88157749 <a title="121-lda-8" href="./nips-2002-Information_Diffusion_Kernels.html">113 nips-2002-Information Diffusion Kernels</a></p>
<p>9 0.87235171 <a title="121-lda-9" href="./nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">34 nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>10 0.86975837 <a title="121-lda-10" href="./nips-2002-Kernel_Dependency_Estimation.html">119 nips-2002-Kernel Dependency Estimation</a></p>
<p>11 0.85915357 <a title="121-lda-11" href="./nips-2002-Stochastic_Neighbor_Embedding.html">190 nips-2002-Stochastic Neighbor Embedding</a></p>
<p>12 0.85333389 <a title="121-lda-12" href="./nips-2002-Hyperkernels.html">106 nips-2002-Hyperkernels</a></p>
<p>13 0.8499006 <a title="121-lda-13" href="./nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">60 nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>14 0.8481524 <a title="121-lda-14" href="./nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>15 0.84149653 <a title="121-lda-15" href="./nips-2002-Intrinsic_Dimension_Estimation_Using_Packing_Numbers.html">117 nips-2002-Intrinsic Dimension Estimation Using Packing Numbers</a></p>
<p>16 0.83890384 <a title="121-lda-16" href="./nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">64 nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>17 0.83856046 <a title="121-lda-17" href="./nips-2002-Kernel_Design_Using_Boosting.html">120 nips-2002-Kernel Design Using Boosting</a></p>
<p>18 0.83820027 <a title="121-lda-18" href="./nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">80 nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>19 0.82139957 <a title="121-lda-19" href="./nips-2002-Critical_Lines_in_Symmetry_of_Mixture_Models_and_its_Application_to_Component_Splitting.html">63 nips-2002-Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting</a></p>
<p>20 0.81948483 <a title="121-lda-20" href="./nips-2002-Charting_a_Manifold.html">49 nips-2002-Charting a Manifold</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
