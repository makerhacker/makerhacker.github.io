<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-31" href="#">nips2007-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</h1>
<br/><p>Source: <a title="nips-2007-31-pdf" href="http://papers.nips.cc/paper/3266-bayesian-agglomerative-clustering-with-coalescents.pdf">pdf</a></p><p>Author: Yee W. Teh, Daniel J. Hsu, Hal Daume</p><p>Abstract: We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman’s coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over the state-of-the-art, and demonstrate our approach in document clustering and phylolinguistics. 1</p><p>Reference: <a title="nips-2007-31-reference" href="../nips2007_reference/nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coalesc', 0.714), ('genealog', 0.31), ('ri', 0.233), ('kingm', 0.202), ('li', 0.194), ('agglom', 0.176), ('smc', 0.148), ('tli', 0.124), ('ti', 0.112), ('hierarch', 0.097), ('slav', 0.095), ('bhc', 0.093), ('clust', 0.091), ('germ', 0.08), ('celt', 0.078), ('rom', 0.076), ('tre', 0.075), ('langu', 0.066), ('greedy', 0.065), ('dur', 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="31-tfidf-1" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>Author: Yee W. Teh, Daniel J. Hsu, Hal Daume</p><p>Abstract: We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman’s coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over the state-of-the-art, and demonstrate our approach in document clustering and phylolinguistics. 1</p><p>2 0.11212197 <a title="31-tfidf-2" href="./nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">68 nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>Author: Charlie Frogner, Avi Pfeffer</p><p>Abstract: Dynamic Bayesian networks are structured representations of stochastic processes. Despite their structure, exact inference in DBNs is generally intractable. One approach to approximate inference involves grouping the variables in the process into smaller factors and keeping independent beliefs over these factors. In this paper we present several techniques for decomposing a dynamic Bayesian network automatically to enable factored inference. We examine a number of features of a DBN that capture different types of dependencies that will cause error in factored inference. An empirical comparison shows that the most useful of these is a heuristic that estimates the mutual information introduced between factors by one step of belief propagation. In addition to features computed over entire factors, for efﬁciency we explored scores computed over pairs of variables. We present search methods that use these features, pairwise and not, to ﬁnd a factorization, and we compare their results on several datasets. Automatic factorization extends the applicability of factored inference to large, complex models that are undesirable to factor by hand. Moreover, tests on real DBNs show that automatic factorization can achieve signiﬁcantly lower error in some cases. 1</p><p>3 0.090974927 <a title="31-tfidf-3" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>Author: Ozgur Sumer, Umut Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Motivated by stochastic systems in which observed evidence and conditional dependencies between states of the network change over time, and certain quantities of interest (marginal distributions, likelihood estimates etc.) must be updated, we study the problem of adaptive inference in tree-structured Bayesian networks. We describe an algorithm for adaptive inference that handles a broad range of changes to the network and is able to maintain marginal distributions, MAP estimates, and data likelihoods in all expected logarithmic time. We give an implementation of our algorithm and provide experiments that show that the algorithm can yield up to two orders of magnitude speedups on answering queries and responding to dynamic changes over the sum-product algorithm. 1</p><p>4 0.066245079 <a title="31-tfidf-4" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>Author: Daichi Mochihashi, Eiichiro Sumita</p><p>Abstract: We present a nonparametric Bayesian method of estimating variable order Markov processes up to a theoretically inﬁnite order. By extending a stick-breaking prior, which is usually deﬁned on a unit interval, “vertically” to the trees of inﬁnite depth associated with a hierarchical Chinese restaurant process, our model directly infers the hidden orders of Markov dependencies from which each symbol originated. Experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model, while computationally much efﬁcient in both time and space. We expect that this basic model will also extend to the variable order hierarchical clustering of general data. 1</p><p>5 0.064796649 <a title="31-tfidf-5" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>Author: José M. Hernández-lobato, Tjeerd Dijkstra, Tom Heskes</p><p>Abstract: We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efﬁcient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with signiﬁcant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).</p><p>6 0.06267447 <a title="31-tfidf-6" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>7 0.060802493 <a title="31-tfidf-7" href="./nips-2007-Ensemble_Clustering_using_Semidefinite_Programming.html">80 nips-2007-Ensemble Clustering using Semidefinite Programming</a></p>
<p>8 0.060781974 <a title="31-tfidf-8" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>9 0.060772546 <a title="31-tfidf-9" href="./nips-2007-Convex_Clustering_with_Exemplar-Based_Models.html">61 nips-2007-Convex Clustering with Exemplar-Based Models</a></p>
<p>10 0.059313666 <a title="31-tfidf-10" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>11 0.058166735 <a title="31-tfidf-11" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>12 0.056211345 <a title="31-tfidf-12" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>13 0.056143168 <a title="31-tfidf-13" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<p>14 0.054006964 <a title="31-tfidf-14" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>15 0.051792763 <a title="31-tfidf-15" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>16 0.051293179 <a title="31-tfidf-16" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>17 0.048603185 <a title="31-tfidf-17" href="./nips-2007-Hierarchical_Penalization.html">99 nips-2007-Hierarchical Penalization</a></p>
<p>18 0.046832334 <a title="31-tfidf-18" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>19 0.04612134 <a title="31-tfidf-19" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>20 0.045232099 <a title="31-tfidf-20" href="./nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, -0.019), (2, 0.023), (3, -0.059), (4, 0.003), (5, -0.082), (6, -0.045), (7, 0.006), (8, -0.075), (9, 0.048), (10, -0.055), (11, 0.041), (12, 0.044), (13, 0.02), (14, -0.012), (15, 0.026), (16, -0.075), (17, -0.048), (18, 0.022), (19, -0.027), (20, -0.042), (21, 0.069), (22, -0.055), (23, -0.012), (24, 0.032), (25, -0.11), (26, 0.03), (27, 0.036), (28, 0.011), (29, -0.032), (30, -0.009), (31, -0.007), (32, 0.0), (33, 0.012), (34, -0.014), (35, -0.007), (36, 0.054), (37, -0.095), (38, -0.065), (39, 0.073), (40, 0.011), (41, 0.187), (42, -0.003), (43, -0.086), (44, 0.005), (45, -0.037), (46, 0.15), (47, 0.033), (48, -0.049), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87795544 <a title="31-lsi-1" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>Author: Yee W. Teh, Daniel J. Hsu, Hal Daume</p><p>Abstract: We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman’s coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over the state-of-the-art, and demonstrate our approach in document clustering and phylolinguistics. 1</p><p>2 0.60484874 <a title="31-lsi-2" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>Author: José M. Hernández-lobato, Tjeerd Dijkstra, Tom Heskes</p><p>Abstract: We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efﬁcient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with signiﬁcant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).</p><p>3 0.56260151 <a title="31-lsi-3" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>Author: Daichi Mochihashi, Eiichiro Sumita</p><p>Abstract: We present a nonparametric Bayesian method of estimating variable order Markov processes up to a theoretically inﬁnite order. By extending a stick-breaking prior, which is usually deﬁned on a unit interval, “vertically” to the trees of inﬁnite depth associated with a hierarchical Chinese restaurant process, our model directly infers the hidden orders of Markov dependencies from which each symbol originated. Experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model, while computationally much efﬁcient in both time and space. We expect that this basic model will also extend to the variable order hierarchical clustering of general data. 1</p><p>4 0.56143683 <a title="31-lsi-4" href="./nips-2007-Efficient_Principled_Learning_of_Thin_Junction_Trees.html">78 nips-2007-Efficient Principled Learning of Thin Junction Trees</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present the ﬁrst truly polynomial algorithm for PAC-learning the structure of bounded-treewidth junction trees – an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efﬁcient exact inference. For a constant treewidth, our algorithm has polynomial time and sample complexity. If a junction tree with sufﬁciently strong intraclique dependencies exists, we provide strong theoretical guarantees in terms of KL divergence of the result from the true distribution. We also present a lazy extension of our approach that leads to very signiﬁcant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets. One of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of variables with only polynomially many mutual information computations on ﬁxed-size subsets of variables, if the underlying distribution can be approximated by a bounded-treewidth junction tree. 1</p><p>5 0.54952097 <a title="31-lsi-5" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>6 0.54676974 <a title="31-lsi-6" href="./nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">68 nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>7 0.53499997 <a title="31-lsi-7" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>8 0.47586897 <a title="31-lsi-8" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>9 0.45203409 <a title="31-lsi-9" href="./nips-2007-Convex_Clustering_with_Exemplar-Based_Models.html">61 nips-2007-Convex Clustering with Exemplar-Based Models</a></p>
<p>10 0.44331726 <a title="31-lsi-10" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>11 0.44155356 <a title="31-lsi-11" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<p>12 0.43919465 <a title="31-lsi-12" href="./nips-2007-Ensemble_Clustering_using_Semidefinite_Programming.html">80 nips-2007-Ensemble Clustering using Semidefinite Programming</a></p>
<p>13 0.43723688 <a title="31-lsi-13" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>14 0.43702656 <a title="31-lsi-14" href="./nips-2007-Modeling_homophily_and_stochastic_equivalence_in_symmetric_relational_data.html">131 nips-2007-Modeling homophily and stochastic equivalence in symmetric relational data</a></p>
<p>15 0.42174706 <a title="31-lsi-15" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>16 0.3987487 <a title="31-lsi-16" href="./nips-2007-The_Infinite_Gamma-Poisson_Feature_Model.html">196 nips-2007-The Infinite Gamma-Poisson Feature Model</a></p>
<p>17 0.39758486 <a title="31-lsi-17" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>18 0.39684495 <a title="31-lsi-18" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>19 0.36238277 <a title="31-lsi-19" href="./nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</a></p>
<p>20 0.35473314 <a title="31-lsi-20" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.01), (12, 0.315), (25, 0.062), (27, 0.022), (30, 0.081), (45, 0.057), (46, 0.078), (53, 0.051), (56, 0.019), (60, 0.138), (62, 0.032), (90, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.66489685 <a title="31-lda-1" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>Author: Yee W. Teh, Daniel J. Hsu, Hal Daume</p><p>Abstract: We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman’s coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over the state-of-the-art, and demonstrate our approach in document clustering and phylolinguistics. 1</p><p>2 0.54518318 <a title="31-lda-2" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>Author: Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, Pierre F. Baldi</p><p>Abstract: Large repositories of source code create new challenges and opportunities for statistical machine learning. Here we ﬁrst develop Sourcerer, an infrastructure for the automated crawling, parsing, and database storage of open source software. Sourcerer allows us to gather Internet-scale source code. For instance, in one experiment, we gather 4,632 java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data ﬁrst reveal robust power-law behavior for package, SLOC, and lexical containment distributions. We then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to signiﬁcantly improve software retrieval performance, increasing the AUC metric to 0.84– roughly 10-30% better than previous approaches based on text alone. Supplementary material may be found at: http://sourcerer.ics.uci.edu/nips2007/nips07.html. 1</p><p>3 0.52406341 <a title="31-lda-3" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>Author: Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl</p><p>Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a ﬁxed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topologyextraction approaches and show how having the two-dimensional topology can be exploited.</p><p>4 0.52340508 <a title="31-lda-4" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>Author: Qiuhua Liu, Xuejun Liao, Lawrence Carin</p><p>Abstract: A semi-supervised multitask learning (MTL) framework is presented, in which M parameterized semi-supervised classiﬁers, each associated with one of M partially labeled data manifolds, are learned jointly under the constraint of a softsharing prior imposed over the parameters of the classiﬁers. The unlabeled data are utilized by basing classiﬁer learning on neighborhoods, induced by a Markov random walk over a graph representation of each manifold. Experimental results on real data sets demonstrate that semi-supervised MTL yields signiﬁcant improvements in generalization performance over either semi-supervised single-task learning (STL) or supervised MTL. 1</p><p>5 0.52140689 <a title="31-lda-5" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We ﬁrst learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classiﬁcation can then be further improved by using backpropagation through the DBN to discriminatively ﬁne-tune the covariance kernel.</p><p>6 0.51778388 <a title="31-lda-6" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>7 0.51751858 <a title="31-lda-7" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>8 0.51736599 <a title="31-lda-8" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>9 0.51724225 <a title="31-lda-9" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>10 0.51718628 <a title="31-lda-10" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>11 0.51684785 <a title="31-lda-11" href="./nips-2007-Bayesian_Co-Training.html">32 nips-2007-Bayesian Co-Training</a></p>
<p>12 0.51665276 <a title="31-lda-12" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>13 0.51605421 <a title="31-lda-13" href="./nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>14 0.51583135 <a title="31-lda-14" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>15 0.51521337 <a title="31-lda-15" href="./nips-2007-Ensemble_Clustering_using_Semidefinite_Programming.html">80 nips-2007-Ensemble Clustering using Semidefinite Programming</a></p>
<p>16 0.51488596 <a title="31-lda-16" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>17 0.51428628 <a title="31-lda-17" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>18 0.51413918 <a title="31-lda-18" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>19 0.513291 <a title="31-lda-19" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>20 0.51323819 <a title="31-lda-20" href="./nips-2007-Discriminative_K-means_for_Clustering.html">70 nips-2007-Discriminative K-means for Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
