<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-45" href="#">acl2012-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</h1>
<br/><p>Source: <a title="acl-2012-45-pdf" href="http://aclweb.org/anthology//P/P12/P12-1026.pdf">pdf</a></p><p>Author: Weiwei Sun ; Hans Uszkoreit</p><p>Abstract: From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.</p><p>Reference: <a title="acl-2012-45-reference" href="../acl2012_reference/acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn,  Abstract From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. [sent-3, score-0.668]
</p><p>2 Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. [sent-4, score-0.629]
</p><p>3 Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. [sent-5, score-0.151]
</p><p>4 Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. [sent-6, score-0.563]
</p><p>5 Many successful tagging algorithms developed for English have been applied to many other languages as well. [sent-10, score-0.274]
</p><p>6 While state-of-theart tagging systems have achieved accuracies above 97% on English, Chinese POS tagging has proven to be more challenging and obtained accuracies about 93-94% (Tseng et al. [sent-16, score-0.865]
</p><p>7 It is generally accepted that Chinese POS tagging often requires more sophisticated language processing techniques that are capable of drawing inferences from more subtle linguistic knowledge. [sent-20, score-0.274]
</p><p>8 From a linguistic point of view, meaning arises from the differences between linguistic units, including words, phrases and so on, and these differences are of two kinds: paradigmatic (concerning substitution) and syntagmatic (concerning positioning). [sent-21, score-0.563]
</p><p>9 Both paradigmatic and syntagmatic lexical relations have a great impact on POS tagging, because the value of a word is determined by the two relations. [sent-23, score-0.745]
</p><p>10 Our error analysis ofa state-of-the-art Chinese POS tagger shows that the lack of both paradigmatic and syntagmatic lexical knowledge accounts for a large part of tagging errors. [sent-24, score-1.054]
</p><p>11 This paper is concerned with capturing paradig-  matic and syntagmatic lexical relations to advance the state-of-the-art of Chinese POS tagging. [sent-25, score-0.477]
</p><p>12 First, we employ unsupervised word clustering to explore paradigmatic relations that are encoded in largescale unlabeled data. [sent-26, score-0.74]
</p><p>13 Second, we study the possible impact of syntagmatic relations on POS tagging by comparatively analyzing a (syntax-free) sequential tagging model Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-28, score-1.056]
</p><p>14 Inspired by the analysis, we employ a full parser to implicitly capture syntagmatic relations and propose a Bootstrap Aggregating (Bagging) model to combine the complementary strengths of a sequential tagger and a parser. [sent-31, score-0.78]
</p><p>15 We implement a discriminative sequential classification model for POS tagging which achieves the state-of-the-art accuracy. [sent-33, score-0.447]
</p><p>16 This con-  firms the importance of the paradigmatic relations. [sent-35, score-0.273]
</p><p>17 We then present a comparative study of our tagger and the Berkeley parser, and show that the combination of the two models can significantly improve tagging accuracy. [sent-36, score-0.461]
</p><p>18 Cluster-based features and the Bagging model result in a relative error reduction of 18% in terms of the word classification accuracy. [sent-38, score-0.146]
</p><p>19 1 Previous Work Many algorithms have been applied to computationally assigning POS labels to English words, including hand-written rules, generative HMM tagging and discriminative sequence labeling. [sent-40, score-0.366]
</p><p>20 While state-of-theart tagging systems have achieved accuracies above 97% on English, Chinese POS tagging has proven  to be more challenging and obtains accuracies about 93-94% (Tseng et al. [sent-44, score-0.865]
</p><p>21 Both discriminative and generative models have been explored for Chinese POS tagging (Tseng et al. [sent-48, score-0.366]
</p><p>22 Their evaluations on the Chinese Treebank show that Chinese POS tagging obtains an accuracy of about 93-94%. [sent-59, score-0.274]
</p><p>23 To deeply analyze the POS tagging problem for Chinese, we implement a discriminative sequential model. [sent-67, score-0.447]
</p><p>24 To compare our tagger with the state-of-the-art, we conduct  an experiment using the data setting of (Huang et al. [sent-88, score-0.187]
</p><p>25 ) of our tagger and results reported in (Huang et al. [sent-96, score-0.187]
</p><p>26 Despite of simplicity, our discriminative POS tagging model achieves a state-of-the-art performance, even better. [sent-103, score-0.366]
</p><p>27 69%  Table 1: Tagging accuracies on the test data (setting 1). [sent-110, score-0.145]
</p><p>28 To avoid overestimating the tagging accuracy, these statistics exclude all 244 punctuations. [sent-116, score-0.274]
</p><p>29 ” This analysis suggests that a main topic to enhance Chinese POS tagging is to bridge the gap between the infrequent words and frequent words. [sent-120, score-0.417]
</p><p>30 Table 3 shows the tagging accuracies relative to the length of the spans. [sent-134, score-0.459]
</p><p>31 This analysis suggests that syntagmatic lexical relations plays a significant role in POS tagging, and sometimes words located far from the current token affect its tagging much. [sent-136, score-0.71]
</p><p>32 3  Capturing Paradigmatic Relations via Word Clustering  To bridge the gap between high and low frequency words, we employ word clustering to acquire the knowledge about paradigmatic lexical relations from large-scale texts. [sent-144, score-0.765]
</p><p>33 Our work is also inspired by the successful application of word clustering to named entity recognition (Miller et al. [sent-145, score-0.295]
</p><p>34 1 Word Clustering Word clustering is a technique for partitioning sets of words into subsets of syntactically or semantically similar words. [sent-149, score-0.292]
</p><p>35 It is a very useful technique to capture paradigmatic or substitutional similarity among words. [sent-150, score-0.3]
</p><p>36 1 Clustering Algorithms Various clustering techniques have been proposed, some of which, for example, perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms. [sent-153, score-0.797]
</p><p>37 In this paper, we focus on distributional word clustering that is based on the assumption that words that appear in similar contexts (especially surrounding words) tend to have similar meanings. [sent-154, score-0.336]
</p><p>38 Brown Clustering Our first choice is the bottomup agglomerative word clustering algorithm of (Brown et al. [sent-156, score-0.295]
</p><p>39 , 1992) which derives a hierarchical clustering of words from unlabeled data. [sent-157, score-0.381]
</p><p>40 This algorithm generates a hard clustering each word belongs to exactly one cluster. [sent-158, score-0.295]
</p><p>41 MKCLS Clustering We also do experiments by using another popular clustering method based on 2http : / / c s . [sent-172, score-0.251]
</p><p>42 However, we expect that our approach can  function with other clustering algorithms. [sent-185, score-0.251]
</p><p>43 2 Improving Tagging with Cluster Features Our discriminative sequential tagger is easy to be extended with arbitrary features and therefore suitable to explore additional features derived from other 3http : / / code . [sent-204, score-0.42]
</p><p>44 We are relying on the ability of the discriminative learning method to explore informative features, which play a central role in boosting the tagging performance. [sent-212, score-0.366]
</p><p>45 Table 4 summarizes the tagging results on the development data with different feature configurations. [sent-238, score-0.314]
</p><p>46 From this table, we can clearly see the impact of word clustering features on POS tagging. [sent-241, score-0.358]
</p><p>47 Moreover, these increases are consistent regardless of the clustering algorithms. [sent-243, score-0.251]
</p><p>48 Both clustering algorithms contributes to the overall performance equivalently. [sent-244, score-0.251]
</p><p>49 A natural strategy for extending current experiments is to include both clustering results together, or to include more than one cluster granularity. [sent-245, score-0.291]
</p><p>50 For 246 each clustering algorithm, there are not much differences among different sizes of the total clustering numbers. [sent-247, score-0.502]
</p><p>51 When a comparable amount of unlabeled data (five years’ data) is used, the further increase of the unlabeled data for clustering does not lead to much changes of the tagging performance. [sent-248, score-0.651]
</p><p>52 77%  Table 5: Tagging accuracies relative to sizes of training data. [sent-260, score-0.185]
</p><p>53 Table 5 summarizes the accuracies of the systems when trained on smaller portions of the labeled data. [sent-264, score-0.213]
</p><p>54 5 Analysis Word clustering derives paradigmatic relational information from unlabeled data by grouping words  into different sets. [sent-270, score-0.654]
</p><p>55 As a result, the contribution of word clustering to POS tagging is two-fold. [sent-271, score-0.569]
</p><p>56 On the one hand, word clustering captures and abstracts context information. [sent-272, score-0.295]
</p><p>57 On the other hand, the clustering of the OOV words to some extent fights the sparse data problem by correlating an OOV word with in-vocabulary (IV) words through their classes. [sent-274, score-0.424]
</p><p>58 To evaluate the two contributions of the word clustering, we limit entries of the clustering lexicon to only contain IV words, i. [sent-275, score-0.295]
</p><p>59 The gap between the baseline and +IV clustering models can be viewed as the contribution of the first effect, while the gap between the +IV clustering and +All clustering models can be viewed as the second contribution. [sent-279, score-0.817]
</p><p>60 This table is also helpful to understand the impact of the clustering information on the prediction of OOV words. [sent-292, score-0.311]
</p><p>61 4  Capturing Syntagmatic Relations via Constituency Parsing Syntactic analysis, especially the full and deep one, reflects syntagmatic relations of words and phrases of sentences. [sent-293, score-0.406]
</p><p>62 We present a series of empirical studies of the tagging results of our syntax-free sequential tagger and a syntax-based chart parser5, aiming at illuminating more precisely the impact of information about phrase-structures on POS tagging. [sent-294, score-0.617]
</p><p>63 The analysis is helpful to understand the role of syntag-  matic lexical relations in POS prediction. [sent-295, score-0.144]
</p><p>64 Compared to lexicalized parsers, the PCFGLA parsers leverages on an automatic procedure to 5Both the tagger and the parser are trained on the same portion from CTB. [sent-300, score-0.306]
</p><p>65 The majority of the words that are better labeled by the tagger are content words, including nouns(NN, NR, NT), numbers (CD, OD), predicates (VA, VC, VE), adverbs (AD), nominal modifiers (JJ), and so on. [sent-327, score-0.256]
</p><p>66 For example, nouns and verbs are open classes because new nouns and verbs are continually coined or borrowed from other languages, while DEC/DEG are two closed classes because only the function word “的” is assigned to 6http : / / code . [sent-335, score-0.152]
</p><p>67 69%  Overall  Table 8: Tagging accuracies of relative to word classes. [sent-397, score-0.229]
</p><p>68 The discriminative model can conveniently include many features, especially features related to the word formation, which are important to predict words of open classes. [sent-399, score-0.236]
</p><p>69 Table 9 summarizes the tagging accuracies relative to IV and OOV words. [sent-400, score-0.499]
</p><p>70 On the whole, the Berkeley parser processes IV words slightly better than our tagger, but processes OOV words significantly worse. [sent-401, score-0.168]
</p><p>71 77% Table 9: Tagging accuracies of the IV and OOV words. [sent-407, score-0.145]
</p><p>72 The disambiguation of these 248 words normally require more syntactic clues, which is very hard and inappropriate for a sequential tagger to capture. [sent-415, score-0.336]
</p><p>73 We conclude that discriminative sequential tagging model can better capture local syntactic and morphological information, while the full parser can better capture global syntactic structural information. [sent-417, score-0.684]
</p><p>74 The discriminative tagging model are limited  ×  by the Markov assumption and inadequate to correctly label structure related words. [sent-418, score-0.366]
</p><p>75 2 Enhancing POS Tagging via Bagging The diversity analysis suggests that we may improve parsing by simply combining the tagger and the parser. [sent-420, score-0.233]
</p><p>76 We introduce a Bagging model to integrate different POS tagging models. [sent-423, score-0.274]
</p><p>77 Each Di is separately used to train a tagger and a parser. [sent-427, score-0.187]
</p><p>78 In the tagging phase, the 2m models outputs 2m tagging results, each word is assigned one POS label. [sent-429, score-0.592]
</p><p>79 The final tagging is the voting result of these 2m labels. [sent-430, score-0.274]
</p><p>80 Because each new data set Di in bagging algorithm is generated by a random procedure, the performance of all Bagging experiments are not the same. [sent-436, score-0.358]
</p><p>81 We can see that the Bagging model taking both sequential tagging and chart parsing models as basic systems outperform the baseline systems and the Bagging model taking either model in isolation as basic systems. [sent-438, score-0.443]
</p><p>82 An  Figure 1: Tagging accuracies of Bagging models. [sent-439, score-0.145]
</p><p>83 Tagger-Bagging and Tagger(WC)-Bagging means that the Bagging system built on the tagger with and without word clusters. [sent-440, score-0.231]
</p><p>84 Tagger+Paser-Bagging and Tagger(WC)+Paser-Bagging means that the Bagging systems are built on both tagger and parser. [sent-442, score-0.187]
</p><p>85 We still use a Bagging model to integrate the discriminative tagger and the Berkeley parser. [sent-446, score-0.279]
</p><p>86 The only difference between current experiment and previous experiment is that the sub-tagging models are trained with help of word clustering features. [sent-447, score-0.295]
</p><p>87 We can see that the improvements that come from two ways, namely capturing syntagmatic and paradigmatic relations, are not much overlapping and the combination of them gives more. [sent-449, score-0.606]
</p><p>88 The word clustering features and the Bagging model result in a relative error reduction of 18% in terms of the classification accuracy. [sent-452, score-0.397]
</p><p>89 The significant improvement of the POS tagging also help successive language processing. [sent-453, score-0.274]
</p><p>90 34% Table 10: Tagging accuracies on the test data (CoNLL). [sent-459, score-0.145]
</p><p>91 11 indicate that the parsing accuracy of the Berkeley parser can be simply improved by inputting the Berkeley parser with the POS Bagging results. [sent-460, score-0.218]
</p><p>92 Although the combination with a syntax-based tagger is very effective, there are two weaknesses: (1) a syntax-based model relies on linguistically rich syntactic annotations that are not easy to acquire; (2) a syntax-based model is computationally expensive which causes efficiency difficulties. [sent-461, score-0.214]
</p><p>93 (CoNLL) 6  Conclusion  We hold a view of structuralist linguistics and study the impact of paradigmatic and syntagmatic lexical relations on Chinese POS tagging. [sent-469, score-0.775]
</p><p>94 First, we harvest word partition information from large-scale raw texts to capture paradigmatic relations and use such knowledge to enhance a supervised tagger via feature engineering. [sent-470, score-0.65]
</p><p>95 Second, we comparatively analyze syntax-free and syntax-based models and employ a Bagging model to integrate a sequential tagger and a chart parser to capture syntagmatic relations that have a great impact on non-local disambiguation. [sent-471, score-0.884]
</p><p>96 Svmtool: A general pos tagger generator based on support vector machines. [sent-514, score-0.348]
</p><p>97 Improving a simple bigram hmm part-of-speech tagger by latent annotation and self-training. [sent-526, score-0.284]
</p><p>98 Joint models for Chinese pos tagging and dependency parsing. [sent-561, score-0.435]
</p><p>99 Morphological features help pos tagging of unknown words across language varieties. [sent-647, score-0.506]
</p><p>100 Transitionbased parsing of the Chinese treebank using a global discriminative model. [sent-671, score-0.171]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bagging', 0.358), ('syntagmatic', 0.29), ('tagging', 0.274), ('paradigmatic', 0.273), ('clustering', 0.251), ('antho', 0.215), ('ogy', 0.205), ('tagger', 0.187), ('url', 0.185), ('oov', 0.162), ('pos', 0.161), ('chinese', 0.148), ('accuracies', 0.145), ('ss', 0.124), ('aclweb', 0.118), ('iv', 0.112), ('huang', 0.101), ('lweb', 0.098), ('discriminative', 0.092), ('tseng', 0.092), ('parser', 0.086), ('wi', 0.084), ('sequential', 0.081), ('relations', 0.075), ('wc', 0.068), ('unlabeled', 0.063), ('mkcls', 0.059), ('berkeley', 0.05), ('hmm', 0.049), ('http', 0.048), ('bigram', 0.048), ('correlating', 0.047), ('dx', 0.047), ('dfki', 0.047), ('parsing', 0.046), ('association', 0.045), ('word', 0.044), ('enhance', 0.044), ('saarland', 0.044), ('morphological', 0.043), ('capturing', 0.043), ('closed', 0.042), ('chart', 0.042), ('clusters', 0.041), ('words', 0.041), ('summarizes', 0.04), ('cluster', 0.04), ('relative', 0.04), ('augmentations', 0.039), ('matic', 0.039), ('msp', 0.039), ('structuralist', 0.039), ('upervised', 0.039), ('sun', 0.038), ('va', 0.036), ('linguistics', 0.035), ('weiwei', 0.035), ('employ', 0.034), ('segmentation', 0.034), ('ctb', 0.034), ('jj', 0.034), ('nr', 0.034), ('impact', 0.033), ('parsers', 0.033), ('classes', 0.033), ('treebank', 0.033), ('reduction', 0.032), ('gap', 0.032), ('vv', 0.032), ('enhancements', 0.031), ('finished', 0.031), ('clues', 0.031), ('pages', 0.03), ('ac', 0.03), ('features', 0.03), ('lexical', 0.03), ('conll', 0.029), ('conveniently', 0.029), ('huihsin', 0.029), ('kneser', 0.029), ('comparatively', 0.029), ('lavergne', 0.029), ('zhongqiang', 0.029), ('gigaword', 0.028), ('labeled', 0.028), ('proven', 0.027), ('aggregating', 0.027), ('mandarin', 0.027), ('prediction', 0.027), ('syntactic', 0.027), ('petrov', 0.027), ('span', 0.027), ('capture', 0.027), ('author', 0.026), ('computational', 0.026), ('sighan', 0.026), ('bridge', 0.026), ('derives', 0.026), ('enhancing', 0.026), ('dealing', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="45-tfidf-1" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun ; Hans Uszkoreit</p><p>Abstract: From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.</p><p>2 0.21494159 <a title="45-tfidf-2" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>Author: Jun Hatori ; Takuya Matsuzaki ; Yusuke Miyao ; Jun'ichi Tsujii</p><p>Abstract: We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</p><p>3 0.19741 <a title="45-tfidf-3" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>Author: Jinho D. Choi ; Martha Palmer</p><p>Abstract: This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selection approach, coupled with a one-pass, leftto-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. this model selection approach to more sophisticated tagging improve their robustness even We believe that can be applied algorithms and further.</p><p>4 0.18802494 <a title="45-tfidf-4" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>Author: Hyun-Je Song ; Jeong-Woo Son ; Tae-Gil Noh ; Seong-Bae Park ; Sang-Jo Lee</p><p>Abstract: All types of part-of-speech (POS) tagging errors have been equally treated by existing taggers. However, the errors are not equally important, since some errors affect the performance of subsequent natural language processing (NLP) tasks seriously while others do not. This paper aims to minimize these serious errors while retaining the overall performance of POS tagging. Two gradient loss functions are proposed to reflect the different types of errors. They are designed to assign a larger cost to serious errors and a smaller one to minor errors. Through a set of POS tagging experiments, it is shown that the classifier trained with the proposed loss functions reduces serious errors compared to state-of-the-art POS taggers. In addition, the experimental result on text chunking shows that fewer serious errors help to improve the performance of sub- sequent NLP tasks.</p><p>5 0.18552411 <a title="45-tfidf-5" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>Author: Weiwei Sun ; Xiaojun Wan</p><p>Abstract: We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.</p><p>6 0.15615121 <a title="45-tfidf-6" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>7 0.1297731 <a title="45-tfidf-7" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>8 0.12580104 <a title="45-tfidf-8" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>9 0.12419735 <a title="45-tfidf-9" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>10 0.12147058 <a title="45-tfidf-10" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>11 0.12071465 <a title="45-tfidf-11" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>12 0.11974508 <a title="45-tfidf-12" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>13 0.11094315 <a title="45-tfidf-13" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>14 0.10555319 <a title="45-tfidf-14" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<p>15 0.10108325 <a title="45-tfidf-15" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<p>16 0.094807655 <a title="45-tfidf-16" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>17 0.094489433 <a title="45-tfidf-17" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>18 0.094441049 <a title="45-tfidf-18" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>19 0.092669576 <a title="45-tfidf-19" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>20 0.09078037 <a title="45-tfidf-20" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.266), (1, 0.024), (2, -0.175), (3, -0.156), (4, -0.009), (5, 0.129), (6, 0.051), (7, -0.209), (8, 0.01), (9, -0.016), (10, 0.047), (11, 0.012), (12, 0.096), (13, -0.035), (14, 0.04), (15, 0.178), (16, 0.087), (17, 0.033), (18, 0.07), (19, 0.04), (20, -0.009), (21, 0.13), (22, -0.115), (23, 0.085), (24, 0.164), (25, -0.001), (26, -0.028), (27, -0.005), (28, 0.13), (29, 0.023), (30, 0.056), (31, 0.023), (32, -0.049), (33, 0.057), (34, 0.021), (35, -0.067), (36, -0.022), (37, -0.007), (38, 0.038), (39, 0.069), (40, 0.013), (41, -0.039), (42, 0.03), (43, -0.013), (44, 0.042), (45, 0.069), (46, -0.065), (47, 0.038), (48, 0.056), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96816581 <a title="45-lsi-1" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun ; Hans Uszkoreit</p><p>Abstract: From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.</p><p>2 0.87207097 <a title="45-lsi-2" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>Author: Weiwei Sun ; Xiaojun Wan</p><p>Abstract: We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.</p><p>3 0.8168661 <a title="45-lsi-3" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>Author: Jun Hatori ; Takuya Matsuzaki ; Yusuke Miyao ; Jun'ichi Tsujii</p><p>Abstract: We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</p><p>4 0.81587672 <a title="45-lsi-4" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>Author: Jinho D. Choi ; Martha Palmer</p><p>Abstract: This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selection approach, coupled with a one-pass, leftto-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. this model selection approach to more sophisticated tagging improve their robustness even We believe that can be applied algorithms and further.</p><p>5 0.74259984 <a title="45-lsi-5" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>Author: Hyun-Je Song ; Jeong-Woo Son ; Tae-Gil Noh ; Seong-Bae Park ; Sang-Jo Lee</p><p>Abstract: All types of part-of-speech (POS) tagging errors have been equally treated by existing taggers. However, the errors are not equally important, since some errors affect the performance of subsequent natural language processing (NLP) tasks seriously while others do not. This paper aims to minimize these serious errors while retaining the overall performance of POS tagging. Two gradient loss functions are proposed to reflect the different types of errors. They are designed to assign a larger cost to serious errors and a smaller one to minor errors. Through a set of POS tagging experiments, it is shown that the classifier trained with the proposed loss functions reduces serious errors compared to state-of-the-art POS taggers. In addition, the experimental result on text chunking shows that fewer serious errors help to improve the performance of sub- sequent NLP tasks.</p><p>6 0.67334938 <a title="45-lsi-6" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>7 0.65019751 <a title="45-lsi-7" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>8 0.64174044 <a title="45-lsi-8" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>9 0.62641758 <a title="45-lsi-9" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>10 0.5812068 <a title="45-lsi-10" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>11 0.55057335 <a title="45-lsi-11" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<p>12 0.50950599 <a title="45-lsi-12" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>13 0.49827704 <a title="45-lsi-13" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>14 0.48003942 <a title="45-lsi-14" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>15 0.47211868 <a title="45-lsi-15" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>16 0.46670473 <a title="45-lsi-16" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<p>17 0.45780417 <a title="45-lsi-17" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>18 0.45676538 <a title="45-lsi-18" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>19 0.44947368 <a title="45-lsi-19" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>20 0.43823498 <a title="45-lsi-20" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.018), (26, 0.059), (28, 0.057), (30, 0.021), (37, 0.042), (39, 0.046), (59, 0.016), (68, 0.159), (71, 0.024), (74, 0.039), (82, 0.026), (84, 0.031), (85, 0.053), (90, 0.163), (92, 0.064), (94, 0.03), (99, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8512519 <a title="45-lda-1" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun ; Hans Uszkoreit</p><p>Abstract: From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.</p><p>2 0.7799688 <a title="45-lda-2" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>3 0.77540815 <a title="45-lda-3" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili ; Martha Palmer</p><p>Abstract: In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean- ingful syntactic/semantic structures, which allows for improving the state-of-the-art.</p><p>4 0.77490634 <a title="45-lda-4" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>Author: Wan-Yu Lin ; Nanyun Peng ; Chun-Chao Yen ; Shou-de Lin</p><p>Abstract: In this paper, we introduce a framework that identifies online plagiarism by exploiting lexical, syntactic and semantic features that includes duplication-gram, reordering and alignment of words, POS and phrase tags, and semantic similarity of sentences. We establish an ensemble framework to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real-world online plagiarism cases but also outperforms several state-of-the-art algorithms and commercial software. Keywords Plagiarism Detection, Lexical, Syntactic, Semantic 1.</p><p>5 0.77357417 <a title="45-lda-5" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>Author: Lea Frermann ; Francis Bond</p><p>Abstract: We present a system for cross-lingual parse disambiguation, exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different languages have different ambiguities. We simultaneously reduce ambiguity in multiple languages in a fully automatic way. Evaluation shows that the system reliably discards dispreferred parses from the raw parser output, which results in a pre-selection that can speed up manual treebanking.</p><p>6 0.7697826 <a title="45-lda-6" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.76963007 <a title="45-lda-7" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>8 0.76907492 <a title="45-lda-8" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>9 0.7690711 <a title="45-lda-9" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>10 0.76877272 <a title="45-lda-10" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>11 0.76875913 <a title="45-lda-11" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>12 0.76812822 <a title="45-lda-12" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>13 0.76697111 <a title="45-lda-13" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>14 0.76696372 <a title="45-lda-14" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>15 0.7644375 <a title="45-lda-15" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>16 0.76382816 <a title="45-lda-16" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>17 0.76368302 <a title="45-lda-17" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>18 0.7631858 <a title="45-lda-18" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>19 0.76238066 <a title="45-lda-19" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>20 0.76203793 <a title="45-lda-20" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
