<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-106" href="#">emnlp2010-106</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</h1>
<br/><p>Source: <a title="emnlp-2010-106-pdf" href="http://aclweb.org/anthology//D/D10/D10-1066.pdf">pdf</a></p><p>Author: Eugene Charniak</p><p>Abstract: We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only find a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for topdown parsers of 89.4%,a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001) . The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming.</p><p>Reference: <a title="emnlp-2010-106-reference" href="../emnlp2010_reference/emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('roark', 0.431), ('np', 0.302), ('queu', 0.254), ('nns', 0.251), ('pars', 0.226), ('nod', 0.225), ('forest', 0.176), ('qt', 0.139), ('lap', 0.129), ('vp', 0.122), ('par', 0.114), ('pretermin', 0.111), ('earley', 0.111), ('pick', 0.102), ('tre', 0.096), ('go', 0.088), ('johnson', 0.083), ('wi', 0.075), ('decid', 0.071), ('termin', 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="106-tfidf-1" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>Author: Eugene Charniak</p><p>Abstract: We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only find a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for topdown parsers of 89.4%,a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001) . The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming.</p><p>2 0.24990498 <a title="106-tfidf-2" href="./emnlp-2010-Non-Isomorphic_Forest_Pair_Translation.html">86 emnlp-2010-Non-Isomorphic Forest Pair Translation</a></p>
<p>Author: Hui Zhang ; Min Zhang ; Haizhou Li ; Eng Siong Chng</p><p>Abstract: This paper studies two issues, non-isomorphic structure translation and target syntactic structure usage, for statistical machine translation in the context of forest-based tree to tree sequence translation. For the first issue, we propose a novel non-isomorphic translation framework to capture more non-isomorphic structure mappings than traditional tree-based and tree-sequence-based translation methods. For the second issue, we propose a parallel space searching method to generate hypothesis using tree-to-string model and evaluate its syntactic goodness using tree-to-tree/tree sequence model. This not only reduces the search complexity by merging spurious-ambiguity translation paths and solves the data sparseness issue in training, but also serves as a syntax-based target language model for better grammatical generation. Experiment results on the benchmark data show our proposed two solutions are very effective, achieving significant performance improvement over baselines when applying to different translation models.</p><p>3 0.23574485 <a title="106-tfidf-3" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>4 0.22205135 <a title="106-tfidf-4" href="./emnlp-2010-Utilizing_Extra-Sentential_Context_for_Parsing.html">118 emnlp-2010-Utilizing Extra-Sentential Context for Parsing</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: Syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse. We present an analysis of the WSJ portion of the Penn Treebank, and show that syntactic consistency is pervasive across productions with various lefthand side nonterminals. Then, we implement a reranking constituent parser that makes use of extra-sentential context in its feature set. Using a linear-chain conditional random field, we improve parsing accuracy over the generative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies.</p><p>5 0.19231668 <a title="106-tfidf-5" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>Author: Liang Huang ; Haitao Mi</p><p>Abstract: Syntax-based translation models should in principle be efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration. In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in averagecase polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice). Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++).</p><p>6 0.1743017 <a title="106-tfidf-6" href="./emnlp-2010-Unsupervised_Parse_Selection_for_HPSG.html">114 emnlp-2010-Unsupervised Parse Selection for HPSG</a></p>
<p>7 0.1716924 <a title="106-tfidf-7" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>8 0.17049891 <a title="106-tfidf-8" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>9 0.14976722 <a title="106-tfidf-9" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>10 0.13232203 <a title="106-tfidf-10" href="./emnlp-2010-Self-Training_with_Products_of_Latent_Variable_Grammars.html">96 emnlp-2010-Self-Training with Products of Latent Variable Grammars</a></p>
<p>11 0.12603183 <a title="106-tfidf-11" href="./emnlp-2010-Effects_of_Empty_Categories_on_Machine_Translation.html">40 emnlp-2010-Effects of Empty Categories on Machine Translation</a></p>
<p>12 0.10363608 <a title="106-tfidf-12" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>13 0.10125708 <a title="106-tfidf-13" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>14 0.093220614 <a title="106-tfidf-14" href="./emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</a></p>
<p>15 0.086409159 <a title="106-tfidf-15" href="./emnlp-2010-Automatic_Discovery_of_Manner_Relations_and_its_Applications.html">21 emnlp-2010-Automatic Discovery of Manner Relations and its Applications</a></p>
<p>16 0.082665958 <a title="106-tfidf-16" href="./emnlp-2010-Joint_Training_and_Decoding_Using_Virtual_Nodes_for_Cascaded_Segmentation_and_Tagging_Tasks.html">69 emnlp-2010-Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks</a></p>
<p>17 0.082447067 <a title="106-tfidf-17" href="./emnlp-2010-A_Fast_Decoder_for_Joint_Word_Segmentation_and_POS-Tagging_Using_a_Single_Discriminative_Model.html">2 emnlp-2010-A Fast Decoder for Joint Word Segmentation and POS-Tagging Using a Single Discriminative Model</a></p>
<p>18 0.080468029 <a title="106-tfidf-18" href="./emnlp-2010-Evaluating_the_Impact_of_Alternative_Dependency_Graph_Encodings_on_Solving_Event_Extraction_Tasks.html">46 emnlp-2010-Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks</a></p>
<p>19 0.079710931 <a title="106-tfidf-19" href="./emnlp-2010-On_Dual_Decomposition_and_Linear_Programming_Relaxations_for_Natural_Language_Processing.html">88 emnlp-2010-On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing</a></p>
<p>20 0.07948456 <a title="106-tfidf-20" href="./emnlp-2010-A_Game-Theoretic_Approach_to_Generating_Spatial_Descriptions.html">4 emnlp-2010-A Game-Theoretic Approach to Generating Spatial Descriptions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.288), (1, 0.157), (2, 0.288), (3, 0.321), (4, -0.061), (5, -0.018), (6, -0.099), (7, -0.035), (8, 0.009), (9, -0.102), (10, -0.109), (11, -0.068), (12, -0.059), (13, -0.015), (14, -0.036), (15, 0.044), (16, 0.151), (17, 0.101), (18, -0.001), (19, 0.185), (20, 0.043), (21, 0.161), (22, -0.083), (23, 0.031), (24, 0.034), (25, -0.06), (26, 0.012), (27, 0.022), (28, 0.038), (29, 0.058), (30, 0.033), (31, -0.065), (32, -0.021), (33, -0.027), (34, -0.085), (35, -0.067), (36, -0.082), (37, -0.017), (38, 0.063), (39, -0.021), (40, -0.028), (41, 0.059), (42, 0.036), (43, 0.055), (44, -0.005), (45, -0.006), (46, -0.043), (47, 0.043), (48, 0.048), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94743925 <a title="106-lsi-1" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>Author: Eugene Charniak</p><p>Abstract: We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only find a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for topdown parsers of 89.4%,a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001) . The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming.</p><p>2 0.70351046 <a title="106-lsi-2" href="./emnlp-2010-Utilizing_Extra-Sentential_Context_for_Parsing.html">118 emnlp-2010-Utilizing Extra-Sentential Context for Parsing</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: Syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse. We present an analysis of the WSJ portion of the Penn Treebank, and show that syntactic consistency is pervasive across productions with various lefthand side nonterminals. Then, we implement a reranking constituent parser that makes use of extra-sentential context in its feature set. Using a linear-chain conditional random field, we improve parsing accuracy over the generative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies.</p><p>3 0.62921411 <a title="106-lsi-3" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>Author: Liang Huang ; Haitao Mi</p><p>Abstract: Syntax-based translation models should in principle be efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration. In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in averagecase polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice). Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++).</p><p>4 0.61941856 <a title="106-lsi-4" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>5 0.6092447 <a title="106-lsi-5" href="./emnlp-2010-Non-Isomorphic_Forest_Pair_Translation.html">86 emnlp-2010-Non-Isomorphic Forest Pair Translation</a></p>
<p>Author: Hui Zhang ; Min Zhang ; Haizhou Li ; Eng Siong Chng</p><p>Abstract: This paper studies two issues, non-isomorphic structure translation and target syntactic structure usage, for statistical machine translation in the context of forest-based tree to tree sequence translation. For the first issue, we propose a novel non-isomorphic translation framework to capture more non-isomorphic structure mappings than traditional tree-based and tree-sequence-based translation methods. For the second issue, we propose a parallel space searching method to generate hypothesis using tree-to-string model and evaluate its syntactic goodness using tree-to-tree/tree sequence model. This not only reduces the search complexity by merging spurious-ambiguity translation paths and solves the data sparseness issue in training, but also serves as a syntax-based target language model for better grammatical generation. Experiment results on the benchmark data show our proposed two solutions are very effective, achieving significant performance improvement over baselines when applying to different translation models.</p><p>6 0.55109233 <a title="106-lsi-6" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>7 0.54871517 <a title="106-lsi-7" href="./emnlp-2010-Effects_of_Empty_Categories_on_Machine_Translation.html">40 emnlp-2010-Effects of Empty Categories on Machine Translation</a></p>
<p>8 0.54714835 <a title="106-lsi-8" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>9 0.53131181 <a title="106-lsi-9" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>10 0.52952963 <a title="106-lsi-10" href="./emnlp-2010-Unsupervised_Parse_Selection_for_HPSG.html">114 emnlp-2010-Unsupervised Parse Selection for HPSG</a></p>
<p>11 0.49143329 <a title="106-lsi-11" href="./emnlp-2010-Evaluating_the_Impact_of_Alternative_Dependency_Graph_Encodings_on_Solving_Event_Extraction_Tasks.html">46 emnlp-2010-Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks</a></p>
<p>12 0.47677958 <a title="106-lsi-12" href="./emnlp-2010-Improved_Fully_Unsupervised_Parsing_with_Zoomed_Learning.html">60 emnlp-2010-Improved Fully Unsupervised Parsing with Zoomed Learning</a></p>
<p>13 0.42190978 <a title="106-lsi-13" href="./emnlp-2010-Automatic_Discovery_of_Manner_Relations_and_its_Applications.html">21 emnlp-2010-Automatic Discovery of Manner Relations and its Applications</a></p>
<p>14 0.40554127 <a title="106-lsi-14" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>15 0.38613093 <a title="106-lsi-15" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>16 0.3841753 <a title="106-lsi-16" href="./emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</a></p>
<p>17 0.3753452 <a title="106-lsi-17" href="./emnlp-2010-Using_Unknown_Word_Techniques_to_Learn_Known_Words.html">117 emnlp-2010-Using Unknown Word Techniques to Learn Known Words</a></p>
<p>18 0.35048494 <a title="106-lsi-18" href="./emnlp-2010-Dual_Decomposition_for_Parsing_with_Non-Projective_Head_Automata.html">38 emnlp-2010-Dual Decomposition for Parsing with Non-Projective Head Automata</a></p>
<p>19 0.33402818 <a title="106-lsi-19" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>20 0.32579377 <a title="106-lsi-20" href="./emnlp-2010-A_Game-Theoretic_Approach_to_Generating_Spatial_Descriptions.html">4 emnlp-2010-A Game-Theoretic Approach to Generating Spatial Descriptions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.027), (4, 0.034), (7, 0.074), (25, 0.045), (31, 0.025), (35, 0.06), (38, 0.046), (39, 0.089), (44, 0.011), (47, 0.107), (49, 0.035), (58, 0.037), (61, 0.022), (62, 0.021), (64, 0.01), (71, 0.282)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72681969 <a title="106-lda-1" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>Author: Eugene Charniak</p><p>Abstract: We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only find a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for topdown parsers of 89.4%,a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001) . The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming.</p><p>2 0.52244514 <a title="106-lda-2" href="./emnlp-2010-Learning_First-Order_Horn_Clauses_from_Web_Text.html">72 emnlp-2010-Learning First-Order Horn Clauses from Web Text</a></p>
<p>Author: Stefan Schoenmackers ; Jesse Davis ; Oren Etzioni ; Daniel Weld</p><p>Abstract: input. Even the entire Web corpus does not explicitly answer all questions, yet inference can uncover many implicit answers. But where do inference rules come from? This paper investigates the problem of learning inference rules from Web text in an unsupervised, domain-independent manner. The SHERLOCK system, described herein, is a first-order learner that acquires over 30,000 Horn clauses from Web text. SHERLOCK embodies several innovations, including a novel rule scoring function based on Statistical Relevance (Salmon et al., 1971) which is effective on ambiguous, noisy and incomplete Web extractions. Our experiments show that inference over the learned rules discovers three times as many facts (at precision 0.8) as the TEXTRUNNER system which merely extracts facts explicitly stated in Web text.</p><p>3 0.52067131 <a title="106-lda-3" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>Author: Zhongqiang Huang ; Martin Cmejrek ; Bowen Zhou</p><p>Abstract: In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previous studies, we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus, based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side. In our model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decod- ing time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.</p><p>4 0.51948416 <a title="106-lda-4" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>5 0.51447862 <a title="106-lda-5" href="./emnlp-2010-Function-Based_Question_Classification_for_General_QA.html">51 emnlp-2010-Function-Based Question Classification for General QA</a></p>
<p>Author: Fan Bu ; Xingwei Zhu ; Yu Hao ; Xiaoyan Zhu</p><p>Abstract: In contrast with the booming increase of internet data, state-of-art QA (question answering) systems, otherwise, concerned data from specific domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way. Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines. In this framework, question classification is the primary task. However, the current paradigms of question classification were focused on some specified type of questions, i.e. factoid questions, which are inappropriate for the general QA. In this paper, we propose a new question classification paradigm, which includes a question taxonomy suitable to the general QA and a question classifier based on MLN (Markov logic network), where rule-based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach. Experiments show that our method outperforms traditional question classification approaches.</p><p>6 0.51347309 <a title="106-lda-6" href="./emnlp-2010-Effects_of_Empty_Categories_on_Machine_Translation.html">40 emnlp-2010-Effects of Empty Categories on Machine Translation</a></p>
<p>7 0.51065648 <a title="106-lda-7" href="./emnlp-2010-SRL-Based_Verb_Selection_for_ESL.html">95 emnlp-2010-SRL-Based Verb Selection for ESL</a></p>
<p>8 0.50897813 <a title="106-lda-8" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>9 0.50860327 <a title="106-lda-9" href="./emnlp-2010-What%27s_with_the_Attitude%3F_Identifying_Sentences_with_Attitude_in_Online_Discussions.html">120 emnlp-2010-What's with the Attitude? Identifying Sentences with Attitude in Online Discussions</a></p>
<p>10 0.50513762 <a title="106-lda-10" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>11 0.50477487 <a title="106-lda-11" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>12 0.50434321 <a title="106-lda-12" href="./emnlp-2010-An_Efficient_Algorithm_for_Unsupervised_Word_Segmentation_with_Branching_Entropy_and_MDL.html">17 emnlp-2010-An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL</a></p>
<p>13 0.50320858 <a title="106-lda-13" href="./emnlp-2010-Non-Isomorphic_Forest_Pair_Translation.html">86 emnlp-2010-Non-Isomorphic Forest Pair Translation</a></p>
<p>14 0.50181603 <a title="106-lda-14" href="./emnlp-2010-Unsupervised_Parse_Selection_for_HPSG.html">114 emnlp-2010-Unsupervised Parse Selection for HPSG</a></p>
<p>15 0.50169045 <a title="106-lda-15" href="./emnlp-2010-Joint_Inference_for_Bilingual_Semantic_Role_Labeling.html">68 emnlp-2010-Joint Inference for Bilingual Semantic Role Labeling</a></p>
<p>16 0.50129843 <a title="106-lda-16" href="./emnlp-2010-A_Multi-Pass_Sieve_for_Coreference_Resolution.html">8 emnlp-2010-A Multi-Pass Sieve for Coreference Resolution</a></p>
<p>17 0.50113565 <a title="106-lda-17" href="./emnlp-2010-%22Poetic%22_Statistical_Machine_Translation%3A_Rhyme_and_Meter.html">1 emnlp-2010-"Poetic" Statistical Machine Translation: Rhyme and Meter</a></p>
<p>18 0.50082654 <a title="106-lda-18" href="./emnlp-2010-Improved_Fully_Unsupervised_Parsing_with_Zoomed_Learning.html">60 emnlp-2010-Improved Fully Unsupervised Parsing with Zoomed Learning</a></p>
<p>19 0.49977908 <a title="106-lda-19" href="./emnlp-2010-Automatic_Detection_and_Classification_of_Social_Events.html">20 emnlp-2010-Automatic Detection and Classification of Social Events</a></p>
<p>20 0.49852875 <a title="106-lda-20" href="./emnlp-2010-Using_Unknown_Word_Techniques_to_Learn_Known_Words.html">117 emnlp-2010-Using Unknown Word Techniques to Learn Known Words</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
