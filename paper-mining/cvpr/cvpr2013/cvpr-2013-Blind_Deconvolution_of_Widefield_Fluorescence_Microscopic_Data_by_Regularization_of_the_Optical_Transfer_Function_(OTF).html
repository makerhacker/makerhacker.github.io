<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-65" href="#">cvpr2013-65</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</h1>
<br/><p>Source: <a title="cvpr-2013-65-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Keuper_Blind_Deconvolution_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>Reference: <a title="cvpr-2013-65-reference" href="../cvpr2013_reference/cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('psf', 0.424), ('widefield', 0.393), ('deconvolv', 0.307), ('otf', 0.294), ('rmse', 0.289), ('kftv', 0.284), ('specim', 0.276), ('microscop', 0.272), ('record', 0.131), ('kip', 0.108), ('mlem', 0.108), ('blind', 0.106), ('fluoresc', 0.102), ('drosophil', 0.081), ('nucle', 0.08), ('confoc', 0.073), ('tv', 0.059), ('deno', 0.059), ('foury', 0.057), ('sk', 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="65-tfidf-1" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>2 0.33858359 <a title="65-tfidf-2" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>Author: James Gregson, Felix Heide, Matthias B. Hullin, Mushfiqur Rouf, Wolfgang Heidrich</p><p>Abstract: We present a novel stochastic framework for non-blind deconvolution based on point samples obtained from random walks. Unlike previous methods that must be tailored to specific regularization strategies, the new Stochastic Deconvolution method allows arbitrary priors, including nonconvex and data-dependent regularizers, to be introduced and tested with little effort. Stochastic Deconvolution is straightforward to implement, produces state-of-the-art results and directly leads to a natural boundary condition for image boundaries and saturated pixels.</p><p>3 0.17998174 <a title="65-tfidf-3" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>4 0.16354871 <a title="65-tfidf-4" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>5 0.14796607 <a title="65-tfidf-5" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>6 0.13406904 <a title="65-tfidf-6" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>7 0.11692348 <a title="65-tfidf-7" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>8 0.11107671 <a title="65-tfidf-8" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>9 0.11047119 <a title="65-tfidf-9" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>10 0.10324775 <a title="65-tfidf-10" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>11 0.094434351 <a title="65-tfidf-11" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>12 0.083267689 <a title="65-tfidf-12" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>13 0.077802256 <a title="65-tfidf-13" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>14 0.065610282 <a title="65-tfidf-14" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>15 0.057191007 <a title="65-tfidf-15" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>16 0.049992003 <a title="65-tfidf-16" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>17 0.049949508 <a title="65-tfidf-17" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>18 0.048993919 <a title="65-tfidf-18" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>19 0.046739403 <a title="65-tfidf-19" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>20 0.044744268 <a title="65-tfidf-20" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.08), (1, -0.036), (2, -0.059), (3, -0.111), (4, 0.028), (5, 0.15), (6, 0.065), (7, 0.008), (8, 0.003), (9, 0.022), (10, -0.015), (11, 0.036), (12, -0.02), (13, -0.033), (14, 0.064), (15, -0.011), (16, 0.0), (17, -0.044), (18, 0.016), (19, -0.003), (20, 0.011), (21, 0.034), (22, 0.023), (23, -0.013), (24, 0.005), (25, 0.012), (26, -0.009), (27, -0.037), (28, -0.043), (29, 0.0), (30, 0.048), (31, 0.02), (32, 0.017), (33, -0.005), (34, 0.086), (35, 0.018), (36, 0.019), (37, -0.034), (38, -0.1), (39, 0.036), (40, -0.028), (41, -0.002), (42, 0.015), (43, 0.065), (44, 0.024), (45, 0.079), (46, 0.003), (47, 0.137), (48, -0.026), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92260116 <a title="65-lsi-1" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>2 0.78219259 <a title="65-lsi-2" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>Author: James Gregson, Felix Heide, Matthias B. Hullin, Mushfiqur Rouf, Wolfgang Heidrich</p><p>Abstract: We present a novel stochastic framework for non-blind deconvolution based on point samples obtained from random walks. Unlike previous methods that must be tailored to specific regularization strategies, the new Stochastic Deconvolution method allows arbitrary priors, including nonconvex and data-dependent regularizers, to be introduced and tested with little effort. Stochastic Deconvolution is straightforward to implement, produces state-of-the-art results and directly leads to a natural boundary condition for image boundaries and saturated pixels.</p><p>3 0.74605292 <a title="65-lsi-3" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>4 0.65702885 <a title="65-lsi-4" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>5 0.63821959 <a title="65-lsi-5" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>6 0.61024392 <a title="65-lsi-6" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>7 0.57506114 <a title="65-lsi-7" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>8 0.56601638 <a title="65-lsi-8" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>9 0.55447423 <a title="65-lsi-9" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>10 0.5489493 <a title="65-lsi-10" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>11 0.51170939 <a title="65-lsi-11" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>12 0.47019809 <a title="65-lsi-12" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>13 0.46374995 <a title="65-lsi-13" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>14 0.44553274 <a title="65-lsi-14" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>15 0.4343209 <a title="65-lsi-15" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<p>16 0.41608056 <a title="65-lsi-16" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>17 0.41435117 <a title="65-lsi-17" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>18 0.38215354 <a title="65-lsi-18" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>19 0.37557438 <a title="65-lsi-19" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>20 0.36122376 <a title="65-lsi-20" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.055), (4, 0.076), (5, 0.106), (37, 0.043), (50, 0.022), (51, 0.017), (69, 0.358), (81, 0.043), (86, 0.075), (95, 0.015), (97, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.65077597 <a title="65-lda-1" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>2 0.57816452 <a title="65-lda-2" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>Author: Thoma Papadhimitri, Paolo Favaro</p><p>Abstract: We investigate the problem of reconstructing normals, albedo and lights of Lambertian surfaces in uncalibrated photometric stereo under the perspective projection model. Our analysis is based on establishing the integrability constraint. In the orthographicprojection case, it is well-known that when such constraint is imposed, a solution can be identified only up to 3 parameters, the so-called generalized bas-relief (GBR) ambiguity. We show that in the perspective projection case the solution is unique. We also propose a closed-form solution which is simple, efficient and robust. We test our algorithm on synthetic data and publicly available real data. Our quantitative tests show that our method outperforms all prior work of uncalibrated photometric stereo under orthographic projection.</p><p>3 0.55135554 <a title="65-lda-3" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>Author: Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian</p><p>Abstract: The Inverse Document Frequency (IDF) is prevalently utilized in the Bag-of-Words based image search. The basic idea is to assign less weight to terms with high frequency, and vice versa. However, the estimation of visual word frequency is coarse and heuristic. Therefore, the effectiveness of the conventional IDF routine is marginal, and far from optimal. To tackle thisproblem, thispaper introduces a novel IDF expression by the use of Lp-norm pooling technique. . edu . cn qit i @ c s an . ut s a . edu ? ? ? ? ? ? ? ? Carefully designed, the proposed IDF takes into account the term frequency, document frequency, the complexity of images, as well as the codebook information. Optimizing the IDF function towards optimal balancing between TF and pIDF weights yields the so-called Lp-norm IDF (pIDF). WpIDe sFho wwe ithghatts sth yeie clodsnv tehnetio son-acla IlDleFd i Ls a special case of our generalized version, and two novel IDFs, i.e. the average IDF and the max IDF, can also be derived from our formula. Further, by counting for the term-frequency in each image, the proposed Lp-norm IDF helps to alleviate the viismuaalg we,o trhde b purrosptionseesds phenomenon. Our method is evaluated through extensive experiments on three benchmark datasets (Oxford 5K, Paris 6K and Flickr 1M). We report a performance improvement of as large as 27.1% over the baseline approach. Moreover, since the Lp-norm IDF is computed offline, no extra computation or memory cost is introduced to the system at all.</p><p>4 0.53502101 <a title="65-lda-4" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>Author: Roni Mittelman, Honglak Lee, Benjamin Kuipers, Silvio Savarese</p><p>Abstract: The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. Attributes provide an intermediate feature representation in between low-level features and the class categories, leading to improved learning on novel categories from few examples. However, a major caveat is that learning semantic attributes is a laborious task, requiring a significant amount of time and human intervention to provide labels. In order to address this issue, we propose a weakly supervised approach to learn mid-level features, where only class-level supervision is provided during training. We develop a novel extension of the restricted Boltzmann machine (RBM) by incorporating a Beta-Bernoulli process factor potential for hidden units. Unlike the standard RBM, our model uses the class labels to promote category-dependent sharing of learned features, which tends to improve the generalization performance. By using semantic attributes for which annotations are available, we show that we can find correspondences between the learned mid-level features and the labeled attributes. Therefore, the mid-level features have distinct semantic characterization which is similar to that given by the semantic attributes, even though their labeling was not provided during training. Our experimental results on object recognition tasks show significant performance gains, outperforming existing methods which rely on manually labeled semantic attributes.</p><p>5 0.52549958 <a title="65-lda-5" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>Author: Qiang Chen, Zheng Song, Rogerio Feris, Ankur Datta, Liangliang Cao, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In recent years, efficiency of large-scale object detection has arisen as an important topic due to the exponential growth in the size of benchmark object detection datasets. Most current object detection methods focus on improving accuracy of large-scale object detection with efficiency being an afterthought. In this paper, we present the Efficient Maximum Appearance Search (EMAS) model which is an order of magnitude faster than the existing state-of-the-art large-scale object detection approaches, while maintaining comparable accuracy. Our EMAS model consists of representing an image as an ensemble of densely sampled feature points with the proposed Pointwise Fisher Vector encoding method, so that the learnt discriminative scoring function can be applied locally. Consequently, the object detection problem is transformed into searching an image sub-area for maximum local appearance probability, thereby making EMAS an order of magnitude faster than the traditional detection methods. In addition, the proposed model is also suitable for incorporating global context at a negligible extra computational cost. EMAS can also incorporate fusion of multiple features, which greatly improves its performance in detecting multiple object categories. Our experiments show that the proposed algorithm can perform detection of 1000 object classes in less than one minute per image on the Image Net ILSVRC2012 dataset and for 107 object classes in less than 5 seconds per image for the SUN09 dataset using a single CPU.</p><p>6 0.52476633 <a title="65-lda-6" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>7 0.52280581 <a title="65-lda-7" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>8 0.48868382 <a title="65-lda-8" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>9 0.48259553 <a title="65-lda-9" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>10 0.44104779 <a title="65-lda-10" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>11 0.43721229 <a title="65-lda-11" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>12 0.43708614 <a title="65-lda-12" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>13 0.43429387 <a title="65-lda-13" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>14 0.4334676 <a title="65-lda-14" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>15 0.43275756 <a title="65-lda-15" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>16 0.43207425 <a title="65-lda-16" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>17 0.43088719 <a title="65-lda-17" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>18 0.43077764 <a title="65-lda-18" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>19 0.43031287 <a title="65-lda-19" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>20 0.42956179 <a title="65-lda-20" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
