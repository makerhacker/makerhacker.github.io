<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>462 high scalability-2008-12-06-Paper: Real-world Concurrency</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-462" href="#">high_scalability-2008-462</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>462 high scalability-2008-12-06-Paper: Real-world Concurrency</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-462-html" href="http://highscalability.com//blog/2008/12/6/paper-real-world-concurrency.html">html</a></p><p>Introduction: An excellent article by Bryan Cantrill and Jeff Bonwick on how to write multi-
threaded code. With more processors and no magic bullet solution for how to
use them, knowing how to write multiprocessor code that doesn't screw up your
system is still a valuable skill. Some topics:Know your cold paths from your
hot paths.Intuition is frequently wrong--be data intensive.Know when--and when
not--to break up a lock.Be wary of readers/writer locks.Consider per-CPU
locking.Know when to broadcast--and when to signal.Learn to debug
postmortem.Design your systems to be composable.Don't use a semaphore where a
mutex would suffice.Consider memory retiring to implement per-chain hash-table
locks.Be aware of false sharing.Consider using nonblocking synchronization
routines to monitor contention.When reacquiring locks, consider using
generation counts to detect state change.Use wait- and lock-free structures
only if you absolutely must.Prepare for the thrill of victory--and the agony
of defeat.While I</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('retiring', 0.221), ('thrill', 0.221), ('wary', 0.221), ('locks', 0.215), ('bonwick', 0.207), ('cantrill', 0.207), ('multiprocessor', 0.207), ('nonblocking', 0.207), ('semaphore', 0.207), ('routines', 0.191), ('composable', 0.175), ('rating', 0.171), ('bryan', 0.171), ('screw', 0.165), ('mutex', 0.155), ('bullet', 0.147), ('threaded', 0.139), ('false', 0.136), ('paths', 0.127), ('debug', 0.125)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="462-tfidf-1" href="../high_scalability-2008/high_scalability-2008-12-06-Paper%3A_Real-world_Concurrency.html">462 high scalability-2008-12-06-Paper: Real-world Concurrency</a></p>
<p>Introduction: An excellent article by Bryan Cantrill and Jeff Bonwick on how to write multi-
threaded code. With more processors and no magic bullet solution for how to
use them, knowing how to write multiprocessor code that doesn't screw up your
system is still a valuable skill. Some topics:Know your cold paths from your
hot paths.Intuition is frequently wrong--be data intensive.Know when--and when
not--to break up a lock.Be wary of readers/writer locks.Consider per-CPU
locking.Know when to broadcast--and when to signal.Learn to debug
postmortem.Design your systems to be composable.Don't use a semaphore where a
mutex would suffice.Consider memory retiring to implement per-chain hash-table
locks.Be aware of false sharing.Consider using nonblocking synchronization
routines to monitor contention.When reacquiring locks, consider using
generation counts to detect state change.Use wait- and lock-free structures
only if you absolutely must.Prepare for the thrill of victory--and the agony
of defeat.While I</p><p>2 0.090886623 <a title="462-tfidf-2" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core
architectures: Everything You Always Wanted to Know About Synchronization but
Were Afraid to Ask.The goal is to pick a locking approach that doesn't degrade
as the number of cores increase. Like everything else in life, that doesn't
appear to be generically possible:None of the nine locking schemes we consider
consistently outperforms any other one, on all target architectures or
workloads. Strictly speaking, to seek optimality, a lock algorithm should thus
be selected based on the hardware platform and the expected workload.
Abstract:This paper presents the most exhaustive study of synchronization to
date. We span multiple layers, from hardware cache-coherence protocols up to
high-level concurrent software. We do so on different types architectures,
from single-socket - uniform and nonuniform - to multi-socket - directory and
broadcastbased - many-cores. We draw a set of observations that, roughly
speaking,</p><p>3 0.080288433 <a title="462-tfidf-3" href="../high_scalability-2008/high_scalability-2008-12-03-Java_World_Interview_on_Scalability_and_Other_Java_Scalability_Secrets.html">459 high scalability-2008-12-03-Java World Interview on Scalability and Other Java Scalability Secrets</a></p>
<p>Introduction: OK, thisinterview is with meon Java scalability issues. I sound like a bigger
idiot than I would like, but I suppose it could have been worse. The Java
World folks were very nice and did a good job, so there's no blame on them
:-)The interview went an interesting direction, but there's more I'd like add
and I will do so here. Two major rules regarding Java and scalability have
popped out at me:Java - It's the platform stupid. Java the language isn't the
big win. What is the big win is the ecosystem building up around the JVM,
libraries, and toolsets.Java - It's the community stupid. A lot of creativity
is being expended on leveraging the Java platform to meet scalability
challenges. The amazing community that has built up around Java is pushing
Java to the next level in almost every direction imaginable.The fecundity of
the Java ecosystem can most readily be seen with the efforts to tame our
multi-core future. There's a multi-core crisis going in case you haven't
heard. It's all you'll</p><p>4 0.07517492 <a title="462-tfidf-4" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as: The Future of the Parallelism and its Challenges I
mentioned that synchronization the access to the shared resource is the major
challenge to write parallel code.The synchronization and coordination take
long time from the overall execution time, which reduce the benefits of the
parallelism; the synchronization and coordination also reduce the
scalability.There are many forms of synchronization and coordination, such
as:Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and
Parallel Runtime Library. Create and enqueue task objects require
synchronization that it's takes long time especially if we create it into
recursive work such as: Quick Sort algorithm.Synchronization the access to
shared data.But there are a few techniques to avoid these issues, such as:
Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object).
Simply if we reduce the shared data by re-architect our code this will gives
us a huge benefits in performance and s</p><p>5 0.074266151 <a title="462-tfidf-5" href="../high_scalability-2012/high_scalability-2012-10-24-Saving_Cash_Using_Less_Cache_-__90%25_Savings_in_the_Caching_Tier.html">1346 high scalability-2012-10-24-Saving Cash Using Less Cache -  90% Savings in the Caching Tier</a></p>
<p>Introduction: In a paper delivered at HotCloud '12 by a group from CMU and Intel Labs,
Saving Cash by Using Less Cache (slides,  pdf), they show it may be possible
to use less DRAM under low load conditions to save on operational costs. There
are some issues with this idea, but in a give me more cache era, it could be
an interesting source of cost savings for your product. Caching is used
to:Reduce load on the database.Reduce latency.Problem:RAM in the cloud is
quite expensive. A third of costs can come from the caching tier.Solution:
Shrink your cache when the load is lower. Their work shows when the load drops
below a certain point you can throw away 50% of your cache while still
maintaining performance.A few popular items often account for most of your
hits, implying can remove the cache for the long tail. Use two tiers of
servers, the Retiring Group, which is the group of servers you want to get rid
of. The Primary Group is the group of servers you are keeping. A cache request
goes to both serve</p><p>6 0.071417369 <a title="462-tfidf-6" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>7 0.069159918 <a title="462-tfidf-7" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>8 0.067145422 <a title="462-tfidf-8" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>9 0.061953921 <a title="462-tfidf-9" href="../high_scalability-2009/high_scalability-2009-10-30-Hot_Scalabilty_Links_for_October_30_2009.html">734 high scalability-2009-10-30-Hot Scalabilty Links for October 30 2009</a></p>
<p>10 0.06143713 <a title="462-tfidf-10" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>11 0.058210831 <a title="462-tfidf-11" href="../high_scalability-2012/high_scalability-2012-09-07-Stuff_The_Internet_Says_On_Scalability_For_September_7%2C_2012.html">1318 high scalability-2012-09-07-Stuff The Internet Says On Scalability For September 7, 2012</a></p>
<p>12 0.058152385 <a title="462-tfidf-12" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>13 0.055150338 <a title="462-tfidf-13" href="../high_scalability-2012/high_scalability-2012-10-12-Stuff_The_Internet_Says_On_Scalability_For_October_12%2C_2012.html">1339 high scalability-2012-10-12-Stuff The Internet Says On Scalability For October 12, 2012</a></p>
<p>14 0.053881653 <a title="462-tfidf-14" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<p>15 0.053021591 <a title="462-tfidf-15" href="../high_scalability-2013/high_scalability-2013-01-15-More_Numbers_Every_Awesome_Programmer_Must_Know.html">1387 high scalability-2013-01-15-More Numbers Every Awesome Programmer Must Know</a></p>
<p>16 0.052531183 <a title="462-tfidf-16" href="../high_scalability-2013/high_scalability-2013-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_4th%2C_2013.html">1527 high scalability-2013-10-04-Stuff The Internet Says On Scalability For October 4th, 2013</a></p>
<p>17 0.051738311 <a title="462-tfidf-17" href="../high_scalability-2009/high_scalability-2009-08-18-Real_World_Web%3A_Performance_%26_Scalability.html">684 high scalability-2009-08-18-Real World Web: Performance & Scalability</a></p>
<p>18 0.050734509 <a title="462-tfidf-18" href="../high_scalability-2011/high_scalability-2011-05-20-Stuff_The_Internet_Says_On_Scalability_For_May_20%2C_2011.html">1045 high scalability-2011-05-20-Stuff The Internet Says On Scalability For May 20, 2011</a></p>
<p>19 0.050669029 <a title="462-tfidf-19" href="../high_scalability-2008/high_scalability-2008-09-03-Some_Facebook_Secrets_to_Better_Operations.html">378 high scalability-2008-09-03-Some Facebook Secrets to Better Operations</a></p>
<p>20 0.050166901 <a title="462-tfidf-20" href="../high_scalability-2011/high_scalability-2011-04-01-Stuff_The_Internet_Says_On_Scalability_For_April_1%2C_2011.html">1015 high scalability-2011-04-01-Stuff The Internet Says On Scalability For April 1, 2011</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.073), (1, 0.049), (2, -0.0), (3, 0.008), (4, 0.02), (5, 0.034), (6, 0.013), (7, 0.028), (8, -0.034), (9, -0.004), (10, -0.019), (11, -0.017), (12, 0.014), (13, 0.003), (14, -0.012), (15, -0.032), (16, 0.014), (17, -0.002), (18, -0.001), (19, -0.002), (20, 0.014), (21, -0.011), (22, -0.022), (23, 0.016), (24, -0.018), (25, -0.016), (26, 0.031), (27, 0.018), (28, 0.012), (29, 0.038), (30, -0.02), (31, 0.001), (32, 0.005), (33, 0.007), (34, -0.017), (35, 0.006), (36, 0.039), (37, 0.022), (38, 0.037), (39, 0.025), (40, 0.003), (41, 0.022), (42, -0.024), (43, 0.025), (44, -0.005), (45, 0.003), (46, 0.027), (47, 0.038), (48, -0.016), (49, 0.002)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91995984 <a title="462-lsi-1" href="../high_scalability-2008/high_scalability-2008-12-06-Paper%3A_Real-world_Concurrency.html">462 high scalability-2008-12-06-Paper: Real-world Concurrency</a></p>
<p>Introduction: An excellent article by Bryan Cantrill and Jeff Bonwick on how to write multi-
threaded code. With more processors and no magic bullet solution for how to
use them, knowing how to write multiprocessor code that doesn't screw up your
system is still a valuable skill. Some topics:Know your cold paths from your
hot paths.Intuition is frequently wrong--be data intensive.Know when--and when
not--to break up a lock.Be wary of readers/writer locks.Consider per-CPU
locking.Know when to broadcast--and when to signal.Learn to debug
postmortem.Design your systems to be composable.Don't use a semaphore where a
mutex would suffice.Consider memory retiring to implement per-chain hash-table
locks.Be aware of false sharing.Consider using nonblocking synchronization
routines to monitor contention.When reacquiring locks, consider using
generation counts to detect state change.Use wait- and lock-free structures
only if you absolutely must.Prepare for the thrill of victory--and the agony
of defeat.While I</p><p>2 0.72749674 <a title="462-lsi-2" href="../high_scalability-2013/high_scalability-2013-05-22-Strategy%3A_Stop_Using_Linked-Lists.html">1462 high scalability-2013-05-22-Strategy: Stop Using Linked-Lists</a></p>
<p>Introduction: What data structure is more sacred than the link list? If we get rid of it
what silly interview questions would we use instead? But not using linked-
lists is exactly what Aater Suleman recommends inShould you ever use Linked-
Lists?InThe Secret To 10 Million Concurrent Connectionsone of the important
strategies is not scribbling data all over memory via pointers because
following pointers increases cache misses whichreduces performance. And
there's nothing more iconic of pointers than the link list.Here are Aeter's
reasons to be anti-linked-list:They reduce the benefit of out-of-order
execution.They throw off hardware prefetching.They reduce DRAM and TLB
locality.They cannot leverage SIMD.They are harder to send to GPUs.He also
demolishes the pros of linked-lists, finding arrays a better option in almost
every case. Good discussion in the comment section as not everyone
agrees.Patrick Wyatt detailshow a linked-list threading bugrepeatedly crashed
Starcraft. Also a good comment discuss</p><p>3 0.68755502 <a title="462-lsi-3" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as: The Future of the Parallelism and its Challenges I
mentioned that synchronization the access to the shared resource is the major
challenge to write parallel code.The synchronization and coordination take
long time from the overall execution time, which reduce the benefits of the
parallelism; the synchronization and coordination also reduce the
scalability.There are many forms of synchronization and coordination, such
as:Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and
Parallel Runtime Library. Create and enqueue task objects require
synchronization that it's takes long time especially if we create it into
recursive work such as: Quick Sort algorithm.Synchronization the access to
shared data.But there are a few techniques to avoid these issues, such as:
Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object).
Simply if we reduce the shared data by re-architect our code this will gives
us a huge benefits in performance and s</p><p>4 0.66752887 <a title="462-lsi-4" href="../high_scalability-2013/high_scalability-2013-05-10-Stuff_The_Internet_Says_On_Scalability_For_May_10%2C_2013.html">1455 high scalability-2013-05-10-Stuff The Internet Says On Scalability For May 10, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:(In Thailand, they figured out how to solve the
age-old queuing problem!) Nanoscale: Plants IM Using Nanoscale Sound Waves;100
petabytes: CERN data storageQuotable Quotes:Geoff Arnold: Arguably all
interesting advances in computer science and software engineering occur when a
resource that was previously scarce or expensive becomes cheap and
plentiful.@jamesurquhart: "Complexity is a characteristic of the system, not
of the parts in it." -Dekker@louisnorthmore: Scaling down - now that's
scalability!@peakscale: Where distributed systems people retire to forget the
madness: http://en.wikipedia.org/wiki/Antipaxos @dozba: "The Linux Game
Database" ... Well, at least they will never have scaling problems.Michael
Widenius: There is no reason at all to use MySQL@steveloughran: Whenever
someone says "unlimited scalability", ask if that exceeds theberkenstein
bound@nationofminds: "I have infinite MIPS. Unlimited scalability. And zero
effing patience." Endowing cel</p><p>5 0.64558017 <a title="462-lsi-5" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus
protocols. Henry starts with a very useful discussion of what all this talk
about consensus really means:The consensus problem is the problem of getting a
set of nodes in a distributed system to agree on something - it might be a
value, a course of action or a decision. Achieving consensus allows a
distributed system to act as a single entity, with every individual node aware
of and in agreement with the actions of the whole of the network.In this
article Henry tackles Two-Phase Commit, the protocol most databases use to
arrive at a consensus for database writes. The article is very well written
with lots of pretty and informative pictures. He did a really good job.In
conclusion we learn 2PC is very efficient, a minimal number of messages are
exchanged and latency is low. The problem is when a co-ordinator fails
availability is dramatically reduced. This is why 2PC isn't generally used on
highly distributed systems</p><p>6 0.64226043 <a title="462-lsi-6" href="../high_scalability-2014/high_scalability-2014-03-28-Stuff_The_Internet_Says_On_Scalability_For_March_28th%2C_2014.html">1621 high scalability-2014-03-28-Stuff The Internet Says On Scalability For March 28th, 2014</a></p>
<p>7 0.63980865 <a title="462-lsi-7" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>8 0.63529849 <a title="462-lsi-8" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>9 0.63524997 <a title="462-lsi-9" href="../high_scalability-2012/high_scalability-2012-05-04-Stuff_The_Internet_Says_On_Scalability_For_May_4%2C_2012.html">1239 high scalability-2012-05-04-Stuff The Internet Says On Scalability For May 4, 2012</a></p>
<p>10 0.63519472 <a title="462-lsi-10" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>11 0.63156283 <a title="462-lsi-11" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>12 0.63054979 <a title="462-lsi-12" href="../high_scalability-2012/high_scalability-2012-05-02-12_Ways_to_Increase_Throughput_by_32X_and_Reduce_Latency_by__20X.html">1237 high scalability-2012-05-02-12 Ways to Increase Throughput by 32X and Reduce Latency by  20X</a></p>
<p>13 0.62747914 <a title="462-lsi-13" href="../high_scalability-2013/high_scalability-2013-04-26-Stuff_The_Internet_Says_On_Scalability_For_April_26%2C_2013.html">1447 high scalability-2013-04-26-Stuff The Internet Says On Scalability For April 26, 2013</a></p>
<p>14 0.62701327 <a title="462-lsi-14" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>15 0.61989945 <a title="462-lsi-15" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>16 0.61664748 <a title="462-lsi-16" href="../high_scalability-2013/high_scalability-2013-04-05-Stuff_The_Internet_Says_On_Scalability_For_April_5%2C_2013.html">1436 high scalability-2013-04-05-Stuff The Internet Says On Scalability For April 5, 2013</a></p>
<p>17 0.61090326 <a title="462-lsi-17" href="../high_scalability-2012/high_scalability-2012-05-16-Big_List_of_20_Common_Bottlenecks.html">1246 high scalability-2012-05-16-Big List of 20 Common Bottlenecks</a></p>
<p>18 0.6017437 <a title="462-lsi-18" href="../high_scalability-2010/high_scalability-2010-05-12-The_Rise_of_the_Virtual_Cellular_Machines.html">826 high scalability-2010-05-12-The Rise of the Virtual Cellular Machines</a></p>
<p>19 0.59777713 <a title="462-lsi-19" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>20 0.59716082 <a title="462-lsi-20" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.034), (2, 0.177), (10, 0.077), (40, 0.087), (61, 0.022), (77, 0.037), (79, 0.07), (87, 0.338), (94, 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91339886 <a title="462-lda-1" href="../high_scalability-2008/high_scalability-2008-09-25-HighScalability.com_Rated_16th_Best_Blog_for_Development_Managers.html">394 high scalability-2008-09-25-HighScalability.com Rated 16th Best Blog for Development Managers</a></p>
<p>Introduction: Jurgen Appelo of Noop.nl asked for nominations for top blogs and then
performed a sophisticated weighting of their popularity based on page range,
trust authority, Alexa rank, Google hits, and number of comments. When all the
results poured out of the blender HighScalability was ranked 16th. Not bad!
Joel on Software was number one, of course. The next few were: 2) Coding
Horror by Jeff Atwood 3) Seth's Blog by Seth Godin and 4) Paul Graham: Essays
Paul Graham. All excellent blogs. Very cool.</p><p>same-blog 2 0.80495054 <a title="462-lda-2" href="../high_scalability-2008/high_scalability-2008-12-06-Paper%3A_Real-world_Concurrency.html">462 high scalability-2008-12-06-Paper: Real-world Concurrency</a></p>
<p>Introduction: An excellent article by Bryan Cantrill and Jeff Bonwick on how to write multi-
threaded code. With more processors and no magic bullet solution for how to
use them, knowing how to write multiprocessor code that doesn't screw up your
system is still a valuable skill. Some topics:Know your cold paths from your
hot paths.Intuition is frequently wrong--be data intensive.Know when--and when
not--to break up a lock.Be wary of readers/writer locks.Consider per-CPU
locking.Know when to broadcast--and when to signal.Learn to debug
postmortem.Design your systems to be composable.Don't use a semaphore where a
mutex would suffice.Consider memory retiring to implement per-chain hash-table
locks.Be aware of false sharing.Consider using nonblocking synchronization
routines to monitor contention.When reacquiring locks, consider using
generation counts to detect state change.Use wait- and lock-free structures
only if you absolutely must.Prepare for the thrill of victory--and the agony
of defeat.While I</p><p>3 0.71592534 <a title="462-lda-3" href="../high_scalability-2013/high_scalability-2013-10-02-RFC_1925_-_The_Twelve_%28Timeless%29_Networking_Truths.html">1526 high scalability-2013-10-02-RFC 1925 - The Twelve (Timeless) Networking Truths</a></p>
<p>Introduction: The Twelve Networking Truths is one of a long series of timeless truths
documented in sacred April Fools'RFCs. Though issued in 1996, it's no less
true today.It's hard to pick a favorite because they are all good. But if I
had to pick, it would be:Some things in life can never be fully appreciated
nor understood unless experienced firsthand.As we grow comfortable behind
garden walls, clutching gadgets like lifelines and ideologies like shields,
empathy is the true social network. Network Working Group R.Callon, Editor
Request for Comments: 1925IOOFCategory: Informational 1 April 1996 The Twelve
Networking Truths Status of this Memo This memo provides information for the
Internet community. This memo does not specify an Internet standard of any
kind. Distribution of this memo is unlimited. Abstract This memo documents the
fundamental truths of networking for the Internet community. This memo does
not specify a standard, except in the sense that all standards must implicitly
follow the f</p><p>4 0.71413386 <a title="462-lda-4" href="../high_scalability-2009/high_scalability-2009-01-22-Coming_soon%3A_better_JRockit%2BCoherence_integration.html">499 high scalability-2009-01-22-Coming soon: better JRockit+Coherence integration</a></p>
<p>Introduction: At the Oracle Coherence Special Interest Group meeting today in London, Tomas
Nilsson, the product manager for JRockit RT and JRockit Mission Control spoke
about the future plans for JRockit and especially plans for improved Coherence
JRockit integration.</p><p>5 0.68147039 <a title="462-lda-5" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<p>Introduction: The paper MegaPipe: A New Programming Interface for Scalable Network I/O
(video,slides) hits the common theme that if you want to go faster you need a
better car design, not just a better driver. So that's why the authors started
with a clean-slate and designed a network API from the ground up with support
for concurrent I/O, a requirement for achieving high performance while scaling
to large numbers of connections per thread, multiple cores, etc.  What they
created is MegaPipe, "a new network programming API for message-oriented
workloads to avoid the performance issues of BSD Socket API."The result:
MegaPipe outperforms baseline Linux between 29% (for long connections) and
582% (for short connections). MegaPipe improves the performance of a modiÔ¨Åed
version of memcached between 15% and 320%. For a workload based on real-world
HTTP traces, MegaPipe boosts the throughput of nginx by 75%.What's this most
excellent and interesting paper about?Message-oriented network workloads,
where conn</p><p>6 0.61203593 <a title="462-lda-6" href="../high_scalability-2013/high_scalability-2013-01-04-Stuff_The_Internet_Says_On_Scalability_For_January_4%2C_2013.html">1381 high scalability-2013-01-04-Stuff The Internet Says On Scalability For January 4, 2013</a></p>
<p>7 0.60595399 <a title="462-lda-7" href="../high_scalability-2009/high_scalability-2009-06-01-Guess_How_Many_Users_it_Takes_to_Kill_Your_Site%3F.html">614 high scalability-2009-06-01-Guess How Many Users it Takes to Kill Your Site?</a></p>
<p>8 0.58435094 <a title="462-lda-8" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>9 0.55536562 <a title="462-lda-9" href="../high_scalability-2012/high_scalability-2012-04-27-Stuff_The_Internet_Says_On_Scalability_For_April_27%2C_2012.html">1235 high scalability-2012-04-27-Stuff The Internet Says On Scalability For April 27, 2012</a></p>
<p>10 0.54199696 <a title="462-lda-10" href="../high_scalability-2009/high_scalability-2009-08-18-Hardware_Architecture_Example_%28geographical_level_mapping_of_servers%29.html">683 high scalability-2009-08-18-Hardware Architecture Example (geographical level mapping of servers)</a></p>
<p>11 0.52418166 <a title="462-lda-11" href="../high_scalability-2008/high_scalability-2008-07-20-Strategy%3A_Front_S3_with_a_Caching_Proxy.html">353 high scalability-2008-07-20-Strategy: Front S3 with a Caching Proxy</a></p>
<p>12 0.51952028 <a title="462-lda-12" href="../high_scalability-2008/high_scalability-2008-05-27-Should_Twitter_be_an_All-You-Can-Eat_Buffet_or_a_Vending_Machine%3F.html">330 high scalability-2008-05-27-Should Twitter be an All-You-Can-Eat Buffet or a Vending Machine?</a></p>
<p>13 0.51857883 <a title="462-lda-13" href="../high_scalability-2013/high_scalability-2013-06-06-Paper%3A_Memory_Barriers%3A_a_Hardware_View_for_Software_Hackers.html">1471 high scalability-2013-06-06-Paper: Memory Barriers: a Hardware View for Software Hackers</a></p>
<p>14 0.51658714 <a title="462-lda-14" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>15 0.51485896 <a title="462-lda-15" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>16 0.51373065 <a title="462-lda-16" href="../high_scalability-2013/high_scalability-2013-03-07-It%27s_a_VM_Wasteland_-_A_Near_Optimal_Packing_of_VMs_to_Machines_Reduces_TCO_by_22%25.html">1419 high scalability-2013-03-07-It's a VM Wasteland - A Near Optimal Packing of VMs to Machines Reduces TCO by 22%</a></p>
<p>17 0.51285332 <a title="462-lda-17" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>18 0.51157737 <a title="462-lda-18" href="../high_scalability-2009/high_scalability-2009-01-04-Alternative_Memcache_Usage%3A_A_Highly_Scalable%2C_Highly_Available%2C_In-Memory_Shard_Index.html">482 high scalability-2009-01-04-Alternative Memcache Usage: A Highly Scalable, Highly Available, In-Memory Shard Index</a></p>
<p>19 0.51021063 <a title="462-lda-19" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>20 0.50820816 <a title="462-lda-20" href="../high_scalability-2013/high_scalability-2013-07-17-How_do_you_create_a_100th_Monkey_software_development_culture%3F.html">1492 high scalability-2013-07-17-How do you create a 100th Monkey software development culture?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
