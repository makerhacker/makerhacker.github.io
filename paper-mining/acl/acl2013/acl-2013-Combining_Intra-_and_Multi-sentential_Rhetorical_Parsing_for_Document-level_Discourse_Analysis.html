<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-85" href="#">acl2013-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</h1>
<br/><p>Source: <a title="acl-2013-85-pdf" href="http://aclweb.org/anthology//P/P13/P13-1048.pdf">pdf</a></p><p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>Reference: <a title="acl-2013-85-reference" href="../acl2013_reference/acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Qatar Computing Research Institute{ Qatar Foundation Doha, Qatar Abstract We propose a novel approach for developing a two-stage document-level discourse parser. [sent-5, score-0.47]
</p><p>2 Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. [sent-6, score-1.096]
</p><p>3 We present two approaches to combine these two stages of discourse parsing effectively. [sent-7, score-0.66]
</p><p>4 A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. [sent-8, score-0.575]
</p><p>5 , Elaboration, Contrast), forming larger discourse units (represented by internal  ∗This work was conducted at the University of British Columbia, Vancouver, Canada. [sent-18, score-0.61]
</p><p>6 Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. [sent-21, score-0.425]
</p><p>7 Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. [sent-22, score-1.74]
</p><p>8 While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al. [sent-23, score-1.13]
</p><p>9 , 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level  parsers (Hernault et al. [sent-24, score-0.736]
</p><p>10 This paper aims to reduce this performance gap and take discourse parsing one step further. [sent-26, score-0.66]
</p><p>11 First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequential dependencies between the DT constituents, which has been recently shown to be critical (Feng and Hirst, 2012). [sent-28, score-0.649]
</p><p>12 To address this limitation, as the first contribution, we propose a novel document-level discourse parser based on probabilistic discriminative parsing models, represented as Conditional Random Fields (CRFs) (Sutton et al. [sent-29, score-0.765]
</p><p>13 Second, existing parsers apply greedy and suboptimal parsing algorithms to build the DT for a  document. [sent-32, score-0.266]
</p><p>14 The second sentence has a well-formed discourse tree, but the first sentence does not have one. [sent-39, score-0.554]
</p><p>15 Third, existing discourse parsers do not discriminate between intra-sentential (i. [sent-41, score-0.546]
</p><p>16 , building the DTs for the individual sentences) and multisentential parsing (i. [sent-43, score-0.262]
</p><p>17 Two separate parsing models could exploit the fact that rhetorical relations are distributed differently intra-sententially vs. [sent-47, score-0.531]
</p><p>18 In order to develop a complete and robust discourse parser, we combine our intra-sentential and multi-sentential parsers in two different ways. [sent-52, score-0.546]
</p><p>19 However, this approach would disregard those cases where rhetorical structures violate sentence boundaries. [sent-54, score-0.337]
</p><p>20 It does not have a well-formed sub-tree because the unit containing EDUs 2 and 3 merges with the next sentence and only then is the resulting unit merged with EDU 1. [sent-56, score-0.348]
</p><p>21 Our second approach, in an attempt of dealing with these cases, builds sentence-level sub-trees  by applying the intra-sentential parser on a sliding window covering two adjacent sentences and by then consolidating the results produced by overlapping windows. [sent-57, score-0.421]
</p><p>22 After that, the multi-sentential parser takes all these sentence-level sub-trees and builds a full rhetorical parse for the document. [sent-58, score-0.369]
</p><p>23 Our final result compares very favorably to the result of state-of-the-art models in document-level discourse parsing. [sent-61, score-0.47]
</p><p>24 In the rest of the paper, after discussing related work in Section 2, we present our discourse parsing framework in Section 3. [sent-62, score-0.66]
</p><p>25 2  Related work  The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al. [sent-67, score-1.356]
</p><p>26 These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. [sent-69, score-0.544]
</p><p>27 However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). [sent-70, score-1.363]
</p><p>28 Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. [sent-71, score-0.47]
</p><p>29 Given the EDUs in a doc487  13250 5ElabortinJo tAributonSame-UniMIt Curloant-israe tn Eetxnpltain altion  Figure 2: Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios. [sent-74, score-0.273]
</p><p>30 In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label. [sent-76, score-0.401]
</p><p>31 On a different genre of instructional texts, Subba and Di-Eugenio (2009) propose a shift-reduce parser that relies on a classifier for relation labeling. [sent-79, score-0.323]
</p><p>32 In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser. [sent-81, score-0.47]
</p><p>33 3  Our Discourse Parsing Framework  Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i. [sent-82, score-1.334]
</p><p>34 , the labels or the discourse relations) in the resulting DT. [sent-86, score-0.505]
</p><p>35 Since we already have an accurate sentence-level discourse parser (Joty et al. [sent-87, score-0.575]
</p><p>36 , 2012), a straightforward approach to document-level parsing could be to simply apply this parser to the whole document. [sent-88, score-0.295]
</p><p>37 On the  one hand, it appears that rhetorical relations are distributed differently intra-sententially vs. [sent-93, score-0.314]
</p><p>38 For example, Figure 2 shows a comparison between the two distributions of six most 1For n + 1EDUs, the number of valid discourse trees is actually the Catalan number Cn. [sent-95, score-0.47]
</p><p>39 Based on these observations, our discourse parsing framework comprises two separate modules: an intra-sentential parser and a multisentential parser (Figure 3). [sent-103, score-0.942]
</p><p>40 First, the intrasentential parser produces one or more discourse sub-trees for each sentence. [sent-104, score-0.611]
</p><p>41 Then, the multisentential parser generates a full DT for the document from these sub-trees. [sent-105, score-0.21]
</p><p>42 Both of our parsers have the same two components: a parsing model assigns a probability to every possible DT, and a parsing algorithm identifies the most probable DT among the candidate DTs in that scenario. [sent-106, score-0.514]
</p><p>43 Staging multi-sentential parsing on top of  intra-sentential parsing in this way allows us to exploit the strong correlation between the text structure and the DT structure as explained in detail in Section 5. [sent-108, score-0.487]
</p><p>44 Before describing our parsing models and the parsing algorithm, we introduce some terminology that we will use throughout the paper. [sent-109, score-0.38]
</p><p>45 , 2012), a DT can be formally represented as a set of constituents of the form R[i, m, j] , referring to a rhetorical relation R between the discourse unit containing EDUs i through m and the unit containing EDUs m+1 through j. [sent-111, score-1.136]
</p><p>46 ity statuses of the discourse units involved, which can be one of Nucleus-Satellite (NS), SatelliteNucleus (SN) and Nucleus-Nucleus (NN). [sent-114, score-0.654]
</p><p>47 4  Parsing Models and Parsing Algorithm  The job of our intra-sentential and multi-sentential  parsing models is to assign a probability to each of the constituents of all possible DTs at the sentence level and at the document level, respectively. [sent-115, score-0.398]
</p><p>48 1 Intra-Sentential Parsing Model Recently, we proposed a novel parsing model for sentence-level discourse parsing (Joty et al. [sent-118, score-0.85]
</p><p>49 Below we briefly describe the parsing model, and show how it is applied to obtain the probabilities of all possible DT constituents at the sentence level. [sent-120, score-0.336]
</p><p>50 The observed nodes Uj in a sequence represent the dis-  course units (EDUs or larger units). [sent-123, score-0.214]
</p><p>51 The first layer of hidden nodes are the structure nodes, where Sj ∈{0, 1} denotes whether two adjacent discourse un∈its{ Uj−1 aennodt Uj hsheothueldr w beo caodnjanceecntted di or unroste. [sent-124, score-0.655]
</p><p>52 The connections between adjacent nodes in a hidden layer encode sequential dependencies between the respective hidden nodes, and can enforce constraints such as the fact that a Sj= 1must not follow a Sj−1= 1. [sent-131, score-0.206]
</p><p>53 To obtain the probability of the constituents of all candidate DTs for a sentence, we apply the parsing model recursively at different levels of the DT and compute the posterior marginals over the relation-structure pairs. [sent-133, score-0.392]
</p><p>54 At the first (bottom) level, when all the units are the EDUs, there is only one possible unit sequence to which we apply our DCRF model (Figure 5(a)). [sent-136, score-0.351]
</p><p>55 We compute the posterior marginals P(R2, S2=1 |e1, e2 , e3, e4, Θ), P(R3, S3=1 | e1, e2 , e3 , e4, Θ) 1a|ned P(R4, S4=1 |e1, e2 , e3 , e4, Θ) to obtain the probability =of1 ethe constituents R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4] , respectively. [sent-137, score-0.202]
</p><p>56 At the second level, there are three possible unit sequences (e1:2, e3, e4), (e1,e2:3, e4) and (e1,e2,e3:4). [sent-138, score-0.248]
</p><p>57 The posterior  marginals P(R3, S3=1 |e1:2,e3,e4,Θ), P(R2:3 S2:3=1|e1,e2:3,e4,Θ), P(R4, S4=1|e1,e2:3,e4,Θ) and P(R3:4, S3:4=1 |e1,e2,e3:4,Θ) computed from the three sequences correspond to the probability of the constituents R[1, 2, 3] , R[1, 1, 3], R[2, 3, 4] and R[2, 2, 4], respectively. [sent-140, score-0.268]
</p><p>58 Similarly, we attain the probability of the constituents R[1, 1, 4] , R[1, 2, 4] and R[1, 3, 4] by computing their respective posterior marginals from the three possible sequences at the third (top) level. [sent-141, score-0.297]
</p><p>59 (a) Only possible sequence at the first level, (b) Three possible sequences at the  second level, (c) Three possible sequences at the third level. [sent-143, score-0.248]
</p><p>60 Once we obtain the probability of all possible DT constituents, the discourse sub-trees for the sentences are built by applying an optimal probabilistic parsing algorithm (Section 4. [sent-151, score-0.749]
</p><p>61 However, forwards-backwards on a sequence containing T units costs O(TM2) time, where M is the number of relations in our relation set. [sent-158, score-0.306]
</p><p>62 This makes the chain-structured DCRF model impractical for multi-sentential parsing of long documents, since learning requires to run inference on every training sequence with an overall time complexity of O(TM2n3) per document. [sent-159, score-0.219]
</p><p>63 The two observed nodes Ut−1 and Ut are two adjacent discourse units. [sent-161, score-0.582]
</p><p>64 We apply our model to all possible adjacent units at all levels for the multi-sentential case, and 490  compute the posterior marginals of the relationstructure pairs P(Rt, St=1|Ut−1 , Ut, Θ) to obtain the probability of all possible DT constituents. [sent-177, score-0.392]
</p><p>65 3 Features Used in our Parsing Models Table 1 summarizes the features used in our parsing models, which are extracted from two adjacent units Ut−1 and Ut. [sent-179, score-0.397]
</p><p>66 It also includes the distances of the units from the beginning and end of the sentence (or text in the multi-sentential case). [sent-184, score-0.253]
</p><p>67 Text structural features indirectly capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the units. [sent-185, score-0.399]
</p><p>68 , because, although) carry informative clues for rhetorical relations (Marcu, 2000a). [sent-188, score-0.314]
</p><p>69 Rather than using a fixed list of discourse markers, we use an empirically learned lexical N-gram dictionary following (Joty  et al. [sent-189, score-0.47]
</p><p>70 8 Organizational featuresIntra & Multi-Sentential Number of EDUs in unit 1 (or unit 2). [sent-194, score-0.306]
</p><p>71 4 Text structural featuresMulti-Sentential Number of sentences in unit 1 (or unit 2). [sent-198, score-0.337]
</p><p>72 8 Lexical chain featuresMulti-Sentential Number of chains start in unit 1 and end in unit 2. [sent-207, score-0.43]
</p><p>73 Number of chains start (or end) in unit 1(or in unit 2). [sent-208, score-0.365]
</p><p>74 Number of chains skipping both unit 1 and unit 2. [sent-209, score-0.393]
</p><p>75 2 Substructure featuresIntra & Multi-Sentential  Root nodes of the left and right rhetorical sub-trees. [sent-212, score-0.276]
</p><p>76 Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). [sent-217, score-0.569]
</p><p>77 , 2012), we also encode contextual and rhetorical sub-structure features in our models. [sent-220, score-0.231]
</p><p>78 The rhetorical sub-structure features incorporate hierarchical  dependencies between DT constituents. [sent-221, score-0.231]
</p><p>79 4 Parsing Algorithm Given the probability of all possible DT constituents in the intra-sentential and multi-sentential scenarios, the job of the parsing algorithm is to find the most probable DT for that scenario. [sent-223, score-0.323]
</p><p>80 Specifically, with n discourse units, we use the upper-triangular portion of the n×n dynamic programming tgaubllaer D po. [sent-226, score-0.498]
</p><p>81 , 2010; Subba and Di-Eugenio, 2009; Marcu, 2000b), which use a greedy algorithm, our approach finds a discourse tree that is globally optimal. [sent-229, score-0.47]
</p><p>82 5 Document-level Parsing Approaches Now that we have presented our intra-sentential and our multi-sentential parsers, we are ready to describe how they can be effectively combined to perform document-level discourse analysis. [sent-230, score-0.47]
</p><p>83 Recall that a key motivation for a two-stage parsing is that it allows us to capture the correlation between text structure and discourse structure in a scalable, modular and flexible way. [sent-231, score-0.74]
</p><p>84 1 1S-1S (1Sentence-1 Sub-tree) A key finding from several previous studies on sentence-level discourse analysis is that most sentences have a well-formed discourse sub-tree in the full document-level DT (Joty et al. [sent-234, score-0.971]
</p><p>85 It first constructs a DT for every sentence using our intra-sentential  parser, and then it provides our multi-sentential parser with the sentence-level DTs to build the rhetorical parse for the whole document. [sent-238, score-0.405]
</p><p>86 2 Sliding Window While the assumption made by 1S-1S clearly simplifies the parsing process, it totally ignores the cases where discourse structures violate sentence boundaries. [sent-244, score-0.766]
</p><p>87 However, we notice that in most cases where discourse structures violate sentence boundaries, its units are merged with the units of its adjacent sentences, as in Figure 7(b). [sent-249, score-0.923]
</p><p>88 At the end, the multi-sentential parser takes all these sentence-level sub-trees for a document, and •  builds a full rhetorical parse for the document. [sent-271, score-0.369]
</p><p>89 In RST-DT, the original 25 rhetorical relations defined by (Mann and Thompson, 1988) are further divided into a set of 18 coarser relation classes with 78 finer-grained relations. [sent-277, score-0.393]
</p><p>90 2 Experimental Setup We experiment with our discourse parser on the two datasets using our two different parsing approaches, namely 1S-1S and the sliding window. [sent-283, score-0.87]
</p><p>91 Attaching the nuclearity statuses to these relations gives 76 distinct relations. [sent-300, score-0.271]
</p><p>92 3 Results and Error Analysis Table 2 presents F-score parsing results for our parsers and the existing systems on the two corpora. [sent-305, score-0.266]
</p><p>93 First, the Instructional corpus has a smaller amount of data with a larger set of relations (76 when nuclearity attached). [sent-322, score-0.227]
</p><p>94 is  A likely ex-  planation is that the Instructional corpus contains more leaky boundaries  (12%),  allowing the sliding  2Precision, Recall and F-score are the same when manual segmentation is used (see Marcu, (2000b), page 143). [sent-329, score-0.259]
</p><p>95 For example, when parsing was performed on the first sentence in Figure 1 in isolation using 1S-1S, our parser rightly identifies the Contrast relation between EDUs 2 and 3. [sent-351, score-0.391]
</p><p>96 But, when it is considered with its neighboring sentences by the sliding window, the parser labels it as Elaboration. [sent-352, score-0.276]
</p><p>97 , compositional semantics) to cope with the errors caused by semantic similarity between the rhetorical relations. [sent-372, score-0.231]
</p><p>98 7  Conclusion  In this paper, we have presented a novel discourse parser that applies an optimal parsing algorithm  to probabilities inferred from two CRF models: one for intra-sentential parsing and the other for multi-sentential parsing. [sent-373, score-0.955]
</p><p>99 The two models exploit their own informative feature sets and the distributional variations of the relations in the two parsing conditions. [sent-374, score-0.3]
</p><p>100 Empirical evaluations on two different genres demonstrate that our approach yields substantial improvement over existing methods in discourse parsing. [sent-376, score-0.47]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('discourse', 0.47), ('edus', 0.294), ('dt', 0.288), ('rhetorical', 0.231), ('joty', 0.198), ('dts', 0.191), ('parsing', 0.19), ('subba', 0.175), ('instructional', 0.164), ('unit', 0.153), ('nuclearity', 0.144), ('units', 0.14), ('tsp', 0.132), ('dtp', 0.108), ('leaky', 0.108), ('sliding', 0.105), ('parser', 0.105), ('hernault', 0.097), ('dtn', 0.09), ('relations', 0.083), ('hilda', 0.08), ('marcu', 0.077), ('parsers', 0.076), ('uj', 0.076), ('constituents', 0.075), ('multisentential', 0.072), ('sporleder', 0.07), ('adjacent', 0.067), ('sequences', 0.066), ('dominance', 0.066), ('dcrf', 0.064), ('sutton', 0.061), ('marginals', 0.061), ('chains', 0.059), ('ut', 0.059), ('relation', 0.054), ('featuresintra', 0.054), ('ux', 0.048), ('vliet', 0.048), ('boundaries', 0.046), ('nodes', 0.045), ('window', 0.044), ('sw', 0.044), ('carenini', 0.044), ('statuses', 0.044), ('markers', 0.043), ('sentence', 0.042), ('structure', 0.04), ('beginning', 0.04), ('elaboration', 0.039), ('lascarides', 0.038), ('posterior', 0.037), ('rst', 0.036), ('aenntde', 0.036), ('cmp', 0.036), ('consolidating', 0.036), ('intrasentential', 0.036), ('lethanh', 0.036), ('redeker', 0.036), ('staging', 0.036), ('rt', 0.036), ('labels', 0.035), ('chain', 0.034), ('qatar', 0.034), ('structures', 0.033), ('document', 0.033), ('builds', 0.033), ('hidden', 0.033), ('carlson', 0.032), ('ei', 0.032), ('cnd', 0.032), ('fisher', 0.032), ('end', 0.031), ('violate', 0.031), ('sentences', 0.031), ('mann', 0.03), ('pick', 0.03), ('columbia', 0.03), ('sequence', 0.029), ('organizational', 0.029), ('possible', 0.029), ('probability', 0.029), ('hirst', 0.029), ('soricut', 0.028), ('sequential', 0.028), ('dynamic', 0.028), ('manuals', 0.028), ('mislead', 0.028), ('skipping', 0.028), ('verberne', 0.028), ('crf', 0.027), ('constructs', 0.027), ('exploit', 0.027), ('vancouver', 0.026), ('mehdad', 0.026), ('coarser', 0.025), ('potsdam', 0.025), ('documents', 0.025), ('sj', 0.025), ('british', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999899 <a title="85-tfidf-1" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>2 0.38866159 <a title="85-tfidf-2" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>3 0.30954111 <a title="85-tfidf-3" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>4 0.2400559 <a title="85-tfidf-4" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>Author: Mei Tu ; Yu Zhou ; Chengqing Zong</p><p>Abstract: Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 1</p><p>5 0.19730335 <a title="85-tfidf-5" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>Author: Or Biran ; Kathleen McKeown</p><p>Abstract: We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.</p><p>6 0.11439528 <a title="85-tfidf-6" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>7 0.11404721 <a title="85-tfidf-7" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>8 0.11022759 <a title="85-tfidf-8" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>9 0.10684102 <a title="85-tfidf-9" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>10 0.10292209 <a title="85-tfidf-10" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>11 0.097641818 <a title="85-tfidf-11" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>12 0.08844173 <a title="85-tfidf-12" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>13 0.084581085 <a title="85-tfidf-13" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>14 0.081232481 <a title="85-tfidf-14" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>15 0.080389962 <a title="85-tfidf-15" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>16 0.073326811 <a title="85-tfidf-16" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>17 0.072701015 <a title="85-tfidf-17" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>18 0.06999021 <a title="85-tfidf-18" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>19 0.067461573 <a title="85-tfidf-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.066635862 <a title="85-tfidf-20" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.202), (1, 0.009), (2, -0.149), (3, 0.028), (4, -0.096), (5, 0.086), (6, 0.051), (7, 0.015), (8, -0.027), (9, 0.141), (10, 0.155), (11, 0.04), (12, -0.128), (13, 0.095), (14, 0.059), (15, -0.094), (16, 0.082), (17, -0.191), (18, -0.23), (19, -0.274), (20, -0.0), (21, 0.12), (22, 0.048), (23, -0.135), (24, -0.023), (25, 0.035), (26, 0.101), (27, 0.089), (28, 0.041), (29, 0.031), (30, 0.084), (31, -0.005), (32, 0.005), (33, -0.014), (34, 0.029), (35, -0.018), (36, -0.047), (37, -0.039), (38, -0.025), (39, 0.071), (40, 0.017), (41, -0.065), (42, -0.02), (43, -0.06), (44, 0.032), (45, 0.012), (46, -0.081), (47, -0.029), (48, 0.04), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95307297 <a title="85-lsi-1" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>2 0.90008909 <a title="85-lsi-2" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>3 0.80774176 <a title="85-lsi-3" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>Author: Or Biran ; Kathleen McKeown</p><p>Abstract: We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.</p><p>4 0.73147762 <a title="85-lsi-4" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>5 0.58060265 <a title="85-lsi-5" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>Author: Mei Tu ; Yu Zhou ; Chengqing Zong</p><p>Abstract: Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 1</p><p>6 0.52681839 <a title="85-lsi-6" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>7 0.5182361 <a title="85-lsi-7" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>8 0.46777612 <a title="85-lsi-8" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>9 0.40383565 <a title="85-lsi-9" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>10 0.40258619 <a title="85-lsi-10" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>11 0.38914835 <a title="85-lsi-11" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>12 0.38909379 <a title="85-lsi-12" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>13 0.38184103 <a title="85-lsi-13" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>14 0.37212652 <a title="85-lsi-14" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>15 0.36777514 <a title="85-lsi-15" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>16 0.36067969 <a title="85-lsi-16" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<p>17 0.3572197 <a title="85-lsi-17" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>18 0.35558057 <a title="85-lsi-18" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>19 0.35193154 <a title="85-lsi-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.34959152 <a title="85-lsi-20" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.072), (6, 0.032), (11, 0.078), (18, 0.224), (24, 0.089), (26, 0.027), (28, 0.011), (35, 0.101), (42, 0.05), (48, 0.056), (70, 0.051), (88, 0.053), (90, 0.028), (95, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85081571 <a title="85-lda-1" href="./acl-2013-Public_Dialogue%3A_Analysis_of_Tolerance_in_Online_Discussions.html">287 acl-2013-Public Dialogue: Analysis of Tolerance in Online Discussions</a></p>
<p>Author: Arjun Mukherjee ; Vivek Venkataraman ; Bing Liu ; Sharon Meraz</p><p>Abstract: Social media platforms have enabled people to freely express their views and discuss issues of interest with others. While it is important to discover the topics in discussions, it is equally useful to mine the nature of such discussions or debates and the behavior of the participants. There are many questions that can be asked. One key question is whether the participants give reasoned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies. The central idea of this question is tolerance, which is a key concept in the field of communications. In this work, we perform a computational study of tolerance in the context of online discussions. We aim to identify tolerant vs. intolerant participants and investigate how disagreement affects tolerance in discussions in a quantitative framework. To the best of our knowledge, this is the first such study. Our experiments using real-life discussions demonstrate the effective- ness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions.</p><p>2 0.80969357 <a title="85-lda-2" href="./acl-2013-Diathesis_alternation_approximation_for_verb_clustering.html">119 acl-2013-Diathesis alternation approximation for verb clustering</a></p>
<p>Author: Lin Sun ; Diana McCarthy ; Anna Korhonen</p><p>Abstract: Although diathesis alternations have been used as features for manual verb classification, and there is recent work on incorporating such features in computational models of human language acquisition, work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process. This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. Our alternation-based approach is particularly adept at leveraging information from less frequent data.</p><p>same-paper 3 0.80555761 <a title="85-lda-3" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>4 0.66143793 <a title="85-lda-4" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>Author: Olivier Ferret</p><p>Abstract: Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</p><p>5 0.6502291 <a title="85-lda-5" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>Author: Joanne Boisson ; Ting-Hui Kao ; Jian-Cheng Wu ; Tzu-Hsi Yen ; Jason S. Chang</p><p>Abstract: In this paper, we introduce a Web-scale linguistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and additional regular expression (RE) operators. In our approach, we incorporate inverted file indexing, PoS information from BNC, and semantic indexing based on Latent Dirichlet Allocation with Google Web 1T. The method involves parsing the query to transforming it into several keyword retrieval commands. Word chunks are retrieved with counts, further filtering the chunks with the query as a RE, and finally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided. In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehensive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major languages in the world. 1</p><p>6 0.64977324 <a title="85-lda-6" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>7 0.64765203 <a title="85-lda-7" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>8 0.64488769 <a title="85-lda-8" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>9 0.64368671 <a title="85-lda-9" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>10 0.6423189 <a title="85-lda-10" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>11 0.64219457 <a title="85-lda-11" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>12 0.64186573 <a title="85-lda-12" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>13 0.64108241 <a title="85-lda-13" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>14 0.63966936 <a title="85-lda-14" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>15 0.63900328 <a title="85-lda-15" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>16 0.63887614 <a title="85-lda-16" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>17 0.63712311 <a title="85-lda-17" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>18 0.63690162 <a title="85-lda-18" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>19 0.63662326 <a title="85-lda-19" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>20 0.6358366 <a title="85-lda-20" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
