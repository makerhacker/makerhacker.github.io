<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-68" href="#">nips2005-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</h1>
<br/><p>Source: <a title="nips-2005-68-pdf" href="http://papers.nips.cc/paper/2784-factorial-switching-kalman-filters-for-condition-monitoring-in-neonatal-intensive-care.pdf">pdf</a></p><p>Author: Christopher Williams, John Quinn, Neil Mcintosh</p><p>Abstract: The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns. 1</p><p>Reference: <a title="nips-2005-68-reference" href="../nips2005_reference/nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. [sent-14, score-0.582]
</p><p>2 The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. [sent-15, score-0.207]
</p><p>3 We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns. [sent-16, score-0.358]
</p><p>4 1  Introduction  In a neonatal intensive care unit (NICU), an infant‚Äôs vital signs, including heart rate, blood pressures, blood gas properties and temperatures, are continuously monitored and displayed at the cotside. [sent-17, score-0.997]
</p><p>5 The potential factors include handling of the baby, different cardiovascular and respiratory conditions, the effects of drugs which have been administered, and the setup of the monitoring equipment. [sent-19, score-0.3]
</p><p>6 Each factor has an effect on the dynamics of the observations, some by affecting the physiology of the baby (such as an oxygen desaturation), and some by overwriting the measurements with artifactual values (such as a probe dropout). [sent-20, score-1.13]
</p><p>7 There are a number of hidden factors; these are discrete variables, modelling for example if the baby is in a normal respiratory state or not, or if a probe is disconnected or not. [sent-23, score-0.864]
</p><p>8 The state of baby denotes continuous-valued quantities; this models the true values of infant‚Äôs physiological variables, but also has dimensions to model certain artifact processes (see below). [sent-24, score-0.695]
</p><p>9 The observations are those readings obtained from the monitoring equipment, and are subject to corruption by artifact etc. [sent-25, score-0.364]
</p><p>10 By describing the dynamical regime associated with each combination of factors as a linear Gaussian model we obtain a FSKF, which extends the Switching Kalman Filter (see e. [sent-26, score-0.124]
</p><p>11 With this method we can infer the  value of each factor and estimate the true values of vital signs during the times that the measurements are obscured by artifact. [sent-29, score-0.174]
</p><p>12 By using an interpretable hidden state structure for this application, domain knowledge can be used to set some of the parameters. [sent-30, score-0.102]
</p><p>13 This paper demonstrates an application of the FSKF to NICU monitoring data. [sent-31, score-0.147]
</p><p>14 ft affect the hidden continuous state xt and the observations yt . [sent-39, score-0.334]
</p><p>15 The factor f (m) can take on K (m) different values. [sent-40, score-0.074]
</p><p>16 For example, a simple factor is ‚ÄòECG probe dropout‚Äô, taking on two possible values, ‚Äòdropped out‚Äô or ‚Äònormal‚Äô. [sent-41, score-0.363]
</p><p>17 As factors in this application can affect the observations either by altering the baby‚Äôs physiology or overwriting them with artifactual values, the hidden state vector xt contains information on both the ‚Äòtrue‚Äô physiological condition of the baby and on the levels of any artifactual processes. [sent-42, score-1.261]
</p><p>18 The dynamical regime at time t is controlled by the ‚Äòswitch‚Äô variable st , which is the cross product of the individual factors, (1)  st = ft  (M )  ‚äó . [sent-43, score-0.365]
</p><p>19 (1)  For a given setting of st , the hidden continuous state and the observations are related by: xt ‚àº N (A(st )xt‚àí1 + d(st ), Q(st )),  yt ‚àº N (H(st )xt , R(st )),  (2)  where as in the SKF the system dynamics and observation process are dependent on the switch variable. [sent-47, score-0.504]
</p><p>20 The factors are taken to be a priori independent and Ô¨Årst-order Markovian, so that M (m)  p(st |st‚àí1 ) =  p(ft  (m)  |ft‚àí1 ) . [sent-49, score-0.124]
</p><p>21 1  Application-speciÔ¨Åc setup  The continuous hidden state vector x contains two types of values, the true physiological values, xp , and those of artifactual processes, xa . [sent-51, score-0.483]
</p><p>22 The true values are modelled as independent autoregressive processes, described in more detail in section 3. [sent-52, score-0.086]
</p><p>23 To represent this as a state space, the vector xt has to contain the value of the current state and store the value of the states at previous times. [sent-53, score-0.183]
</p><p>24 Note that artifact state values can be affected by physiological state, but not the other way round. [sent-54, score-0.304]
</p><p>25 For example, one factor we model is the arterial blood sample, seen in Figure 1(b), lower panel. [sent-55, score-0.624]
</p><p>26 This occurs when a three-way valve is closed in the baby‚Äôs arterial line, in order for a clinician to draw blood for a sample. [sent-56, score-0.666]
</p><p>27 While the valve is closed a pump works against the pressure sensor, causing the systolic and diastolic blood pressure measurements to rise artiÔ¨Åcially. [sent-57, score-0.786]
</p><p>28 The artifactual values in this case always start at around the value of the baby‚Äôs diastolic blood pressure. [sent-58, score-0.632]
</p><p>29 The factors modelled in these experiments are listed in Table 1. [sent-59, score-0.157]
</p><p>30 The dropout factors represent the case where probes are disconnected and measurements fall to zero on the channels supplied by that probe. [sent-60, score-0.58]
</p><p>31 In this case, the true physiological values are completely hidden. [sent-61, score-0.154]
</p><p>32 BP  60 50  Artifactual state  40 Blood sample ECG dropout  Observations  (a)  0  200  400  600  800  1000  1200  (b)  Figure 1: (a) shows a graphical representation of a Factorial Switching Kalman Filter, with M = 2 factors. [sent-63, score-0.325]
</p><p>33 Panel (b) shows ECG dropout and arterial blood sample events occurring simultaneously. [sent-65, score-0.813]
</p><p>34 BP denotes the systolic blood pressure, and times are in seconds. [sent-67, score-0.418]
</p><p>35 The traces at the bottom show the inferred duration of the arterial blood sample and ECG dropout events. [sent-70, score-0.844]
</p><p>36 The transcutaneous probe (TCP) provides measurements of the partial pressure of oxygen (TcPO2 ) and carbon dioxide (TcPCO2 ) in the baby‚Äôs blood, and is recalibrated every few hours. [sent-71, score-0.512]
</p><p>37 As explained above, when an arterial blood sample is being taken one sees a characteristic ramp in the blood pressure measurements. [sent-73, score-1.033]
</p><p>38 Temperature probe disconnection frequently occurs in conjunction with handling. [sent-74, score-0.355]
</p><p>39 The core temperature probe is under the baby and can come off when the baby is turned over for an examination, causing the readings to drop to the ambient temperature level of the incubator over the course of a few minutes. [sent-75, score-1.585]
</p><p>40 When the probe is reapplied, the measurements gradually return to the true level of the baby‚Äôs core temperature. [sent-76, score-0.446]
</p><p>41 Bradycardia is a genuine physiological occurrence where the heart rate temporarily drops, often with a characteristic curve, then a systemic reaction brings the measurements back to the baseline. [sent-77, score-0.332]
</p><p>42 The Ô¨Ånal factor models opening of the portals on the baby‚Äôs incubator. [sent-78, score-0.177]
</p><p>43 Because the environment within the incubator is closely regulated, an intervention can be inferred from a fall in the incubator humidity measurements. [sent-79, score-0.504]
</p><p>44 While the portals are open and a clinician is handling the baby, we expect increased variability in the measurements from the probes that are still attached. [sent-80, score-0.232]
</p><p>45 2  Inference  For the application of real time clinical monitoring, we are interested in Ô¨Åltering, inferring xt and st from the observations y1:t . [sent-82, score-0.285]
</p><p>46 However, the time taken for exact inference of the posterior p(xt , st |y1:t ) scales exponentially with t, making it intractable. [sent-83, score-0.183]
</p><p>47 This is because the probabilities of having moved between every possible combination of switch settings in times t ‚àí 1 and t are needed to calculate the posterior at time t. [sent-84, score-0.085]
</p><p>48 Hence the number of  FACTOR 5 Probe dropout factors: pulse oximeter, ECG, arterial line, temperature probe, transcutaneous probe  P OSSIBLE SETTINGS 1. [sent-85, score-0.847]
</p><p>49 Normal  Arterial blood sample Temperature probe disconnection  1. [sent-91, score-0.754]
</p><p>50 Gaussians needed to represent the posterior exactly at each time step increases by a factor M of K, the number of cross-product switch settings, where K = m=1 K (m) . [sent-102, score-0.132]
</p><p>51 In this technique a number of particles are propagated through each time step, each with a switch state st and an estimate of the mean and variance of xt . [sent-107, score-0.351]
</p><p>52 A value for the switch state st+1 is obtained for each particle by sampling from the transition probabilities, after which Kalman updates are performed and a likelihood value can be calculated. [sent-108, score-0.169]
</p><p>53 Both inference methods can be speeded up by considering the dropout factors. [sent-111, score-0.278]
</p><p>54 Because a probe dropout always results in an observation of zero on the corresponding measurement channels, the value of yt can be examined at each step. [sent-112, score-0.573]
</p><p>55 If it is not equal to zero then we know that the likelihood of a dropout factor being active will be very low, so there is no need to calculate it explicitly. [sent-113, score-0.306]
</p><p>56 Similarly, if any of the observations are zero then we only perform Kalman updates and calculate likelihoods for those switch states with the appropriate dropout setting. [sent-114, score-0.346]
</p><p>57 In [5], the authors used a 2-factor FSKF in a speech recognition application; the two factors corresponded to (i) phones and (ii) the phone-to-spectrum transformation. [sent-119, score-0.124]
</p><p>58 There has also been much prior work on condition monitoring in intensive care; here we give a brief review of some of these studies and the relationship to our own work. [sent-120, score-0.205]
</p><p>59 The speciÔ¨Åc problem of artifact detection in physiological time series data has been approached in a number of ways. [sent-121, score-0.242]
</p><p>60 Hoare and Beatty [4] describe the use of time series analysis techniques  (ARIMA models, moving average and Kalman Ô¨Ålters) to predict the next point in a patient‚Äôs monitoring trace. [sent-123, score-0.147]
</p><p>61 Our application of a model with factorial state extends this work by explaining the speciÔ¨Åc cause of an artifact, rather than just the fact that a certain data point is artifactual or not. [sent-125, score-0.348]
</p><p>62 We are not aware of other work in condition monitoring using a FSKF. [sent-126, score-0.147]
</p><p>63 Factor dynamics: Using equation 3 we can calculate the state transition probabilities from (m) (m) the transition probabilities for individual state variables, P (ft = a|ft‚àí1 = b). [sent-128, score-0.178]
</p><p>64 The es(m)  (m)  K (m)  timates for these are given by P (ft = a|ft‚àí1 = b) = (nba + c) / c=1 (nbc + c) , where nba is the number of transitions from state b to state a in the training data. [sent-129, score-0.157]
</p><p>65 The factor dynamics can be used to create left-to-right models, e. [sent-133, score-0.14]
</p><p>66 for passing through the sequence O2 high, CO2 low; CO2 ‚Üí 0; equilibration in the TCP recalibration case. [sent-135, score-0.091]
</p><p>67 non-normal), the baby is said to be in a stable condition and has some capacity for self-regulation. [sent-138, score-0.364]
</p><p>68 Most channels vary around reference ranges when the baby is stable and are well Ô¨Åtted by AR(2) models. [sent-140, score-0.427]
</p><p>69 Heart rate and blood pressure observation channels are more volatile and stationarity is improved after differencing. [sent-141, score-0.515]
</p><p>70 1(b), lower panel) due to probe error and quantization effects. [sent-147, score-0.289]
</p><p>71 Above we have modelled the dynamics for a baby in the stable condition; we now describe some of the system models used when the factors are active (i. [sent-154, score-0.587]
</p><p>72 The drop and rise in temperature measurements caused by a temperature probe disconnection closely resemble exponential decay and can be therefore be Ô¨Åtted with an AR(1) process. [sent-157, score-0.65]
</p><p>73 The dynamics corresponding to the bradycardia factor are set by Ô¨Ånding the mean slope of the fall and rise in heart rate, which is used for the drift term d, then Ô¨Åtting an AR(1) process to the residuals. [sent-159, score-0.444]
</p><p>74 The arterial blood sample dynamics are modelled with linear drift; note that the variable in xa corresponding to the value of the arterial blood sample is tied  FHMM GS RBPF  Blood sample AUC EER 0. [sent-160, score-1.321]
</p><p>75 to the diastolic blood pressure value while the factor is inactive. [sent-195, score-0.592]
</p><p>76 We also use linear drift to model the drop in incubator humidity measurements corresponding to a clinician opening the incubator portals. [sent-196, score-0.722]
</p><p>77 We assume that the measurement noise from each probe is the same for physiological and artifactual readings, for example if the core temperature probe is attached to the baby‚Äôs skin or is reading ambient incubator temperature. [sent-197, score-1.312]
</p><p>78 The factors are arranged in a partially ordered set, where later factors overwrite the dynamics A, Q, d or observations H, R on at least one channel of their predecessor. [sent-201, score-0.37]
</p><p>79 The data for each infant was collected every second for 24 hours, on nine channels: heart rate, systolic and diastolic blood pressures, TcPO2 , TcPCO2 , O2 saturation, core temperature and incubator temperature and humidity. [sent-205, score-1.135]
</p><p>80 These infants were the Ô¨Årst 8 in the NICU database who satisÔ¨Åed the age criteria and were monitored on all 8 channels for some 24 hour period within their Ô¨Årst week. [sent-206, score-0.102]
</p><p>81 The test data was annotated with the times of occurrences of the factors in Table 1 by a clinical expert and one of the authors. [sent-208, score-0.157]
</p><p>82 In Figure 1(b) two factors, arterial blood sample and ECG dropout are simultaneously active, and the inference works nicely in this case, with growing uncertainty about the true value of the heart-rate and blood pressure channels when artifactual readings are observed. [sent-210, score-1.645]
</p><p>83 The upper panel in Ô¨Ågure 2(a) shows two examples of bradycardia being detected. [sent-211, score-0.19]
</p><p>84 In the lower panel, the model correctly infers the times that a clinician enters the incubator and replaces a disconnected core temperature probe. [sent-212, score-0.499]
</p><p>85 Figure 2(b) illustrates the simultaneous detection of a TCP artifact (the TCP recal state plotted is obtained by summing the probabilities of  80  200 Sys. [sent-213, score-0.237]
</p><p>86 Plot (b) shows the inference of two simultaneous artifact processes, arterial blood sampling and TCP recalibration. [sent-217, score-0.711]
</p><p>87 the three non-normal TCP states) and a blood sample spike. [sent-219, score-0.399]
</p><p>88 The inferred probabilities for each factor were compared with the gold standard which has a binary value for each factor setting at each time point. [sent-221, score-0.206]
</p><p>89 Inference was done using the Gaussian sum approximation and RBPF, where the number of particles was set so that the two inference methods had the same execution time. [sent-222, score-0.081]
</p><p>90 As a baseline we also used a Factorial Hidden Markov Model (FHMM) to infer when each factor was active. [sent-223, score-0.099]
</p><p>91 This model has the same factor structure as the FSKF, without any hidden continuous state. [sent-224, score-0.114]
</p><p>92 In particular, the inferred times of blood samples and incubator opening were reliably detected. [sent-228, score-0.667]
</p><p>93 The lower performance of the FHMM, which has no knowledge of the dynamics, illustrates the difÔ¨Åculty caused by baseline physiological levels changing over time and between babies. [sent-229, score-0.152]
</p><p>94 For blood sampling and opening of the incubator the performance was worse than the baseline model, though in detecting bradycardia the performance was marginally higher than for inferences made using either the FHMM or the Gaussian Sum approximation. [sent-231, score-0.826]
</p><p>95 Execution times for inference on 24 hours of monitoring data with the set of factors listed in Table 1 on a 3. [sent-232, score-0.342]
</p><p>96 5  Discussion  In this paper we have shown that the FSKF model can be applied successfully to complex monitoring data from a neonatal intensive care unit. [sent-234, score-0.309]
</p><p>97 Also, there are additional factors that can be incorporated into the model, for example to model a pneumothorax event, where air becomes trapped inside the chest between the chest wall and the lung, causing the lung to collapse. [sent-237, score-0.247]
</p><p>98 This work was funded in part by a grant from the premature baby charity BLISS. [sent-240, score-0.364]
</p><p>99 Automatic artifact identiÔ¨Åcation in anaesthesia patient record keeping: a comparison of techniques. [sent-266, score-0.115]
</p><p>100 A mixed level switching dynamic system for continuous speech recognition. [sent-271, score-0.084]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blood', 0.368), ('baby', 0.364), ('probe', 0.289), ('dropout', 0.232), ('incubator', 0.215), ('artifactual', 0.198), ('arterial', 0.182), ('bradycardia', 0.165), ('tcp', 0.165), ('monitoring', 0.147), ('st', 0.137), ('ecg', 0.132), ('fskf', 0.132), ('physiological', 0.127), ('factors', 0.124), ('kalman', 0.121), ('artifact', 0.115), ('temperature', 0.111), ('eer', 0.099), ('heart', 0.099), ('ft', 0.091), ('factorial', 0.088), ('fhmm', 0.086), ('switching', 0.084), ('pressure', 0.084), ('arima', 0.083), ('clinician', 0.083), ('nicu', 0.083), ('factor', 0.074), ('measurements', 0.073), ('auc', 0.069), ('diastolic', 0.066), ('disconnection', 0.066), ('neonatal', 0.066), ('rbpf', 0.066), ('dynamics', 0.066), ('channels', 0.063), ('state', 0.062), ('ar', 0.06), ('xt', 0.059), ('switch', 0.058), ('intensive', 0.058), ('infant', 0.058), ('equilibration', 0.058), ('core', 0.057), ('observations', 0.056), ('opening', 0.053), ('edinburgh', 0.052), ('portals', 0.05), ('systolic', 0.05), ('particle', 0.049), ('normal', 0.047), ('readings', 0.046), ('inference', 0.046), ('filter', 0.045), ('hr', 0.044), ('humidity', 0.043), ('drift', 0.04), ('hidden', 0.04), ('infants', 0.039), ('care', 0.038), ('bp', 0.035), ('particles', 0.035), ('chest', 0.033), ('genuine', 0.033), ('hoare', 0.033), ('incu', 0.033), ('nba', 0.033), ('overwrites', 0.033), ('overwriting', 0.033), ('oxygen', 0.033), ('recal', 0.033), ('recalibration', 0.033), ('skf', 0.033), ('transcutaneous', 0.033), ('valve', 0.033), ('clinical', 0.033), ('disconnected', 0.033), ('ltering', 0.033), ('modelled', 0.033), ('inferred', 0.031), ('sample', 0.031), ('respiratory', 0.029), ('supplied', 0.029), ('xa', 0.029), ('lung', 0.029), ('gs', 0.029), ('pressures', 0.029), ('tc', 0.029), ('causing', 0.028), ('true', 0.027), ('probabilities', 0.027), ('equipment', 0.026), ('autoregressive', 0.026), ('probes', 0.026), ('measurement', 0.026), ('yt', 0.026), ('baseline', 0.025), ('panel', 0.025), ('hours', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="68-tfidf-1" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>Author: Christopher Williams, John Quinn, Neil Mcintosh</p><p>Abstract: The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns. 1</p><p>2 0.070446789 <a title="68-tfidf-2" href="./nips-2005-Learning_Influence_among_Interacting_Markov_Chains.html">111 nips-2005-Learning Influence among Interacting Markov Chains</a></p>
<p>Author: Dong Zhang, Daniel Gatica-perez, Samy Bengio, Deb Roy</p><p>Abstract: We present a model that learns the inÔ¨Çuence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model. 1</p><p>3 0.053494424 <a title="68-tfidf-3" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>Author: Deepak Verma, Rajesh P. Rao</p><p>Abstract: Humans are extremely adept at learning new skills by imitating the actions of others. A progression of imitative abilities has been observed in children, ranging from imitation of simple body movements to goalbased imitation based on inferring intent. In this paper, we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment. We Ô¨Årst describe algorithms for planning actions to achieve a goal state using probabilistic inference. We then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment. The resulting graphical model is then shown to be powerful enough to allow goal-based imitation. Using a simple maze navigation task, we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete.</p><p>4 0.05339393 <a title="68-tfidf-4" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>Author: Daichi Mochihashi, Yuji Matsumoto</p><p>Abstract: Long-distance language modeling is important not only in speech recognition and machine translation, but also in high-dimensional discrete sequence modeling in general. However, the problem of context length has almost been neglected so far and a na¬®ve bag-of-words history has been ƒ± employed in natural language processing. In contrast, in this paper we view topic shifts within a text as a latent stochastic process to give an explicit probabilistic generative model that has partial exchangeability. We propose an online inference algorithm using particle Ô¨Ålters to recognize topic shifts to employ the most appropriate length of context automatically. Experiments on the BNC corpus showed consistent improvement over previous methods involving no chronological order. 1</p><p>5 0.051062435 <a title="68-tfidf-5" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>Author: Afsheen Afshar, Gopal Santhanam, Stephen I. Ryu, Maneesh Sahani, Byron M. Yu, Krishna V. Shenoy</p><p>Abstract: Spiking activity from neurophysiological experiments often exhibits dynamics beyond that driven by external stimulation, presumably reÔ¨Çecting the extensive recurrence of neural circuitry. Characterizing these dynamics may reveal important features of neural computation, particularly during internally-driven cognitive operations. For example, the activity of premotor cortex (PMd) neurons during an instructed delay period separating movement-target speciÔ¨Åcation and a movementinitiation cue is believed to be involved in motor planning. We show that the dynamics underlying this activity can be captured by a lowdimensional non-linear dynamical systems model, with underlying recurrent structure and stochastic point-process output. We present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories. These methods are applied to characterize the dynamics in PMd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks. 1</p><p>6 0.050875194 <a title="68-tfidf-6" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>7 0.048761576 <a title="68-tfidf-7" href="./nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">191 nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<p>8 0.047582503 <a title="68-tfidf-8" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>9 0.04755792 <a title="68-tfidf-9" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>10 0.047220893 <a title="68-tfidf-10" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>11 0.04602446 <a title="68-tfidf-11" href="./nips-2005-Efficient_estimation_of_hidden_state_dynamics_from_spike_trains.html">64 nips-2005-Efficient estimation of hidden state dynamics from spike trains</a></p>
<p>12 0.045286696 <a title="68-tfidf-12" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>13 0.044624325 <a title="68-tfidf-13" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>14 0.042304844 <a title="68-tfidf-14" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>15 0.041790664 <a title="68-tfidf-15" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>16 0.041465499 <a title="68-tfidf-16" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>17 0.039713535 <a title="68-tfidf-17" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>18 0.03959813 <a title="68-tfidf-18" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>19 0.037588645 <a title="68-tfidf-19" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>20 0.037435155 <a title="68-tfidf-20" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.018), (2, 0.076), (3, 0.016), (4, 0.043), (5, -0.073), (6, -0.002), (7, -0.02), (8, 0.022), (9, 0.034), (10, -0.048), (11, 0.025), (12, -0.005), (13, -0.056), (14, 0.014), (15, -0.004), (16, -0.025), (17, 0.065), (18, -0.022), (19, -0.018), (20, 0.011), (21, -0.042), (22, 0.087), (23, 0.042), (24, -0.077), (25, -0.085), (26, 0.079), (27, -0.001), (28, -0.03), (29, 0.02), (30, -0.033), (31, -0.057), (32, -0.022), (33, 0.019), (34, 0.025), (35, -0.019), (36, 0.084), (37, -0.045), (38, 0.032), (39, -0.095), (40, -0.062), (41, 0.098), (42, -0.049), (43, -0.082), (44, 0.045), (45, 0.044), (46, 0.146), (47, 0.007), (48, -0.012), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92034662 <a title="68-lsi-1" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>Author: Christopher Williams, John Quinn, Neil Mcintosh</p><p>Abstract: The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns. 1</p><p>2 0.57103211 <a title="68-lsi-2" href="./nips-2005-Learning_Influence_among_Interacting_Markov_Chains.html">111 nips-2005-Learning Influence among Interacting Markov Chains</a></p>
<p>Author: Dong Zhang, Daniel Gatica-perez, Samy Bengio, Deb Roy</p><p>Abstract: We present a model that learns the inÔ¨Çuence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model. 1</p><p>3 0.48047996 <a title="68-lsi-3" href="./nips-2005-Learning_vehicular_dynamics%2C_with_application_to_modeling_helicopters.html">120 nips-2005-Learning vehicular dynamics, with application to modeling helicopters</a></p>
<p>Author: Pieter Abbeel, Varun Ganapathi, Andrew Y. Ng</p><p>Abstract: We consider the problem of modeling a helicopter‚Äôs dynamics based on state-action trajectories collected from it. The contribution of this paper is two-fold. First, we consider the linear models such as learned by CIFER (the industry standard in helicopter identiÔ¨Åcation), and show that the linear parameterization makes certain properties of dynamical systems, such as inertia, fundamentally difÔ¨Åcult to capture. We propose an alternative, acceleration based, parameterization that does not suffer from this deÔ¨Åciency, and that can be learned as efÔ¨Åciently from data. Second, a Markov decision process model of a helicopter‚Äôs dynamics would explicitly model only the one-step transitions, but we are often interested in a model‚Äôs predictive performance over longer timescales. In this paper, we present an efÔ¨Åcient algorithm for (approximately) minimizing the prediction error over long time scales. We present empirical results on two different helicopters. Although this work was motivated by the problem of modeling helicopters, the ideas presented here are general, and can be applied to modeling large classes of vehicular dynamics. 1</p><p>4 0.423848 <a title="68-lsi-4" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>Author: Cristian Sminchisescu, Atul Kanujia, Zhiguo Li, Dimitris Metaxas</p><p>Abstract: We present a conditional temporal probabilistic framework for reconstructing 3D human motion in monocular video based on descriptors encoding image silhouette observations. For computational ef√Ñ?ƒπ≈° ciency we restrict visual inference to low-dimensional kernel induced non-linear state spaces. Our methodology (kBME) combines kernel PCA-based non-linear dimensionality reduction (kPCA) and Conditional Bayesian Mixture of Experts (BME) in order to learn complex multivalued predictors between observations and model hidden states. This is necessary for accurate, inverse, visual perception inferences, where several probable, distant 3D solutions exist due to noise or the uncertainty of monocular perspective projection. Low-dimensional models are appropriate because many visual processes exhibit strong non-linear correlations in both the image observations and the target, hidden state variables. The learned predictors are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty. We study several predictors and empirically show that the proposed algorithm positively compares with techniques based on regression, Kernel Dependency Estimation (KDE) or PCA alone, and gives results competitive to those of high-dimensional mixture predictors at a fraction of their computational cost. We show that the method successfully reconstructs the complex 3D motion of humans in real monocular video sequences. 1 Introduction and Related Work We consider the problem of inferring 3D articulated human motion from monocular video. This research topic has applications for scene understanding including human-computer interfaces, markerless human motion capture, entertainment and surveillance. A monocular approach is relevant because in real-world settings the human body parts are rarely completely observed even when using multiple cameras. This is due to occlusions form other people or objects in the scene. A robust system has to necessarily deal with incomplete, ambiguous and uncertain measurements. Methods for 3D human motion reconstruction can be classi√Ñ?ƒπ≈° ed as generative and discriminative. They both require a state representation, namely a 3D human model with kinematics (joint angles) or shape (surfaces or joint positions) and they both use a set of image features as observations for state inference. The computational goal in both cases is the conditional distribution for the model state given image observations. Generative model-based approaches [6, 16, 14, 13] have been demonstrated to √Ñ?ƒπ≈°&sbquo;exibly reconstruct complex unknown human motions and to naturally handle problem constraints. However it is dif√Ñ?ƒπ≈° cult to construct reliable observation likelihoods due to the complexity of modeling human appearance. This varies widely due to different clothing and deformation, body proportions or lighting conditions. Besides being somewhat indirect, the generative approach further imposes strict conditional independence assumptions on the temporal observations given the states in order to ensure computational tractability. Due to these factors inference is expensive and produces highly multimodal state distributions [6, 16, 13]. Generative inference algorithms require complex annealing schedules [6, 13] or systematic non-linear search for local optima [16] in order to ensure continuing tracking. These dif√Ñ?ƒπ≈° culties motivate the advent of a complementary class of discriminative algorithms [10, 12, 18, 2], that approximate the state conditional directly, in order to simplify inference. However, inverse, observation-to-state multivalued mappings are dif√Ñ?ƒπ≈° cult to learn (see e.g. √Ñ?ƒπ≈° g. 1a) and a probabilistic temporal setting is necessary. In an earlier paper [15] we introduced a probabilistic discriminative framework for human motion reconstruction. Because the method operates in the originally selected state and observation spaces that can be task generic, therefore redundant and often high-dimensional, inference is more expensive and can be less robust. To summarize, reconstructing 3D human motion in a Figure 1: (a, Left) Example of 180o ambiguity in predicting 3D human poses from silhouette image features (center). It is essential that multiple plausible solutions (e.g. F 1 and F2 ) are correctly represented and tracked over time. A single state predictor will either average the distant solutions or zig-zag between them, see also tables 1 and 2. (b, Right) A conditional chain model. The local distributions p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) or p(yt |zt ) are learned as in √Ñ?ƒπ≈° g. 2. For inference, the predicted local state conditional is recursively combined with the √Ñ?ƒπ≈° ltered prior c.f . (1). conditional temporal framework poses the following dif√Ñ?ƒπ≈° culties: (i) The mapping between temporal observations and states is multivalued (i.e. the local conditional distributions to be learned are multimodal), therefore it cannot be accurately represented using global function approximations. (ii) Human models have multivariate, high-dimensional continuous states of 50 or more human joint angles. The temporal state conditionals are multimodal which makes ef√Ñ?ƒπ≈° cient Kalman √Ñ?ƒπ≈° ltering algorithms inapplicable. General inference methods (particle √Ñ?ƒπ≈° lters, mixtures) have to be used instead, but these are expensive for high-dimensional models (e.g. when reconstructing the motion of several people that operate in a joint state space). (iii) The components of the human state and of the silhouette observation vector exhibit strong correlations, because many repetitive human activities like walking or running have low intrinsic dimensionality. It appears wasteful to work with high-dimensional states of 50+ joint angles. Even if the space were truly high-dimensional, predicting correlated state dimensions independently may still be suboptimal. In this paper we present a conditional temporal estimation algorithm that restricts visual inference to low-dimensional, kernel induced state spaces. To exploit correlations among observations and among state variables, we model the local, temporal conditional distributions using ideas from Kernel PCA [11, 19] and conditional mixture modeling [7, 5], here adapted to produce multiple probabilistic predictions. The corresponding predictor is referred to as a Conditional Bayesian Mixture of Low-dimensional Kernel-Induced Experts (kBME). By integrating it within a conditional graphical model framework (√Ñ?ƒπ≈° g. 1b), we can exploit temporal constraints probabilistically. We demonstrate that this methodology is effective for reconstructing the 3D motion of multiple people in monocular video. Our contribution w.r.t. [15] is a probabilistic conditional inference framework that operates over a non-linear, kernel-induced low-dimensional state spaces, and a set of experiments (on both real and arti√Ñ?ƒπ≈° cial image sequences) that show how the proposed framework positively compares with powerful predictors based on KDE, PCA, or with the high-dimensional models of [15] at a fraction of their cost. 2 Probabilistic Inference in a Kernel Induced State Space We work with conditional graphical models with a chain structure [9], as shown in √Ñ?ƒπ≈° g. 1b, These have continuous temporal states yt , t = 1 . . . T , observations zt . For compactness, we denote joint states Yt = (y1 , y2 , . . . , yt ) or joint observations Zt = (z1 , . . . , zt ). Learning and inference are based on local conditionals: p(yt |zt ) and p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), with yt and zt being low-dimensional, kernel induced representations of some initial model having state xt and observation rt . We obtain zt , yt from rt , xt using kernel PCA [11, 19]. Inference is performed in a low-dimensional, non-linear, kernel induced latent state space (see √Ñ?ƒπ≈° g. 1b and √Ñ?ƒπ≈° g. 2 and (1)). For display or error reporting, we compute the original conditional p(x|r), or a temporally √Ñ?ƒπ≈° ltered version p(xt |Rt ), Rt = (r1 , r2 , . . . , rt ), using a learned pre-image state map [3]. 2.1 Density Propagation for Continuous Conditional Chains For online √Ñ?ƒπ≈° ltering, we compute the optimal distribution p(yt |Zt ) for the state yt , conditioned by observations Zt up to time t. The √Ñ?ƒπ≈° ltered density can be recursively derived as: p(yt |Zt ) = p(yt |ytƒÇÀò&circ;&rsquo;1 , zt )p(ytƒÇÀò&circ;&rsquo;1 |ZtƒÇÀò&circ;&rsquo;1 ) (1) ytƒÇÀò&circ;&rsquo;1 We compute using a conditional mixture for p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) (a Bayesian mixture of experts c.f . ƒÇ&sbquo;√Ç¬ß2.2) and the prior p(ytƒÇÀò&circ;&rsquo;1 |ZtƒÇÀò&circ;&rsquo;1 ), each having, say M components. We integrate M 2 pairwise products of Gaussians analytically. The means of the expanded posterior are clustered and the centers are used to initialize a reduced M -component Kullback-Leibler approximation that is re√Ñ?ƒπ≈° ned using gradient descent [15]. The propagation rule (1) is similar to the one used for discrete state labels [9], but here we work with multivariate continuous state spaces and represent the local multimodal state conditionals using kBME (√Ñ?ƒπ≈° g. 2), and not log-linear models [9] (these would require intractable normalization). This complex continuous model rules out inference based on Kalman √Ñ?ƒπ≈° ltering or dynamic programming [9]. 2.2 Learning Bayesian Mixtures over Kernel Induced State Spaces (kBME) In order to model conditional mappings between low-dimensional non-linear spaces we rely on kernel dimensionality reduction and conditional mixture predictors. The authors of KDE [19] propose a powerful structured unimodal predictor. This works by decorrelating the output using kernel PCA and learning a ridge regressor between the input and each decorrelated output dimension. Our procedure is also based on kernel PCA but takes into account the structure of the studied visual problem where both inputs and outputs are likely to be low-dimensional and the mapping between them multivalued. The output variables xi are projected onto the column vectors of the principal space in order to obtain their principal coordinates y i . A z ƒÇÀò&circ;&circ; P(Fr ) O p(y|z) kP CA ƒÇ≈Ωƒπ&scaron;r (r) ƒÇÀò&Scaron;&sbquo; Fr O / y ƒÇÀò&circ;&circ; P(Fx ) O QQQ QQQ QQQ kP CA QQQ Q( ƒÇ≈Ωƒπ&scaron;x (x) ƒÇÀò&Scaron;&sbquo; Fx x ƒÇÀò&permil;&circ; PreImage(y) O ƒÇ≈Ωƒπ&scaron;r ƒÇ≈Ωƒπ&scaron;x r ƒÇÀò&circ;&circ; R ƒÇÀò&Scaron;&sbquo; Rr x ƒÇÀò&circ;&circ; X ƒÇÀò&Scaron;&sbquo; Rx  p(x|r) ƒÇÀò&permil;&circ; p(x|y) Figure 2: The learned low-dimensional predictor, kBME, for computing p(x|r) ƒÇÀò&permil;√Ñ&bdquo; p(xt |rt ), ƒÇÀò&circ;&euro;t. (We similarly learn p(xt |xtƒÇÀò&circ;&rsquo;1 , rt ), with input (x, r) instead of r ƒÇÀò&euro;&ldquo; here we illustrate only p(x|r) for clarity.) The input r and the output x are decorrelated using Kernel PCA to obtain z and y respectively. The kernels used for the input and output are ƒÇ≈Ωƒπ&scaron; r and ƒÇ≈Ωƒπ&scaron;x , with induced feature spaces Fr and Fx , respectively. Their principal subspaces obtained by kernel PCA are denoted by P(Fr ) and P(Fx ), respectively. A conditional Bayesian mixture of experts p(y|z) is learned using the low-dimensional representation (z, y). Using learned local conditionals of the form p(yt |zt ) or p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), temporal inference can be ef√Ñ?ƒπ≈° ciently performed in a low-dimensional kernel induced state space (see e.g. (1) and √Ñ?ƒπ≈° g. 1b). For visualization and error measurement, the √Ñ?ƒπ≈° ltered density, e.g. p(yt |Zt ), can be mapped back to p(xt |Rt ) using the pre-image c.f . (3). similar procedure is performed on the inputs ri to obtain zi . In order to relate the reduced feature spaces of z and y (P(Fr ) and P(Fx )), we estimate a probability distribution over mappings from training pairs (zi , yi ). We use a conditional Bayesian mixture of experts (BME) [7, 5] in order to account for ambiguity when mapping similar, possibly identical reduced feature inputs to very different feature outputs, as common in our problem (√Ñ?ƒπ≈° g. 1a). This gives a model that is a conditional mixture of low-dimensional kernel-induced experts (kBME): M g(z|ƒÇ≈Ω√Ç¬¥ j )N (y|Wj z, ƒÇ≈Ωƒπ j ) p(y|z) = (2) j=1 where g(z|ƒÇ≈Ω√Ç¬¥ j ) is a softmax function parameterized by ƒÇ≈Ω√Ç¬¥ j and (Wj , ƒÇ≈Ωƒπ j ) are the parameters and the output covariance of expert j, here a linear regressor. As in many Bayesian settings [17, 5], the weights of the experts and of the gates, Wj and ƒÇ≈Ω√Ç¬¥ j , are controlled by hierarchical priors, typically Gaussians with 0 mean, and having inverse variance hyperparameters controlled by a second level of Gamma distributions. We learn this model using a double-loop EM and employ ML-II type approximations [8, 17] with greedy (weight) subset selection [17, 15]. Finally, the kBME algorithm requires the computation of pre-images in order to recover the state distribution x from itƒÇÀò&euro;&trade;s image y ƒÇÀò&circ;&circ; P(Fx ). This is a closed form computation for polynomial kernels of odd degree. For more general kernels optimization or learning (regression based) methods are necessary [3]. Following [3, 19], we use a sparse Bayesian kernel regressor to learn the pre-image. This is based on training data (xi , yi ): p(x|y) = N (x|AƒÇ≈Ωƒπ&scaron;y (y), ƒÇÀò&bdquo;ƒπ&scaron;) (3) with parameters and covariances (A, ƒÇÀò&bdquo;ƒπ&scaron;). Since temporal inference is performed in the low-dimensional kernel induced state space, the pre-image function needs to be calculated only for visualizing results or for the purpose of error reporting. Propagating the result from the reduced feature space P(Fx ) to the output space X pro- duces a Gaussian mixture with M elements, having coef√Ñ?ƒπ≈° cients g(z|ƒÇ≈Ω√Ç¬¥ j ) and components N (x|AƒÇ≈Ωƒπ&scaron;y (Wj z), AJƒÇ≈Ωƒπ&scaron;y ƒÇ≈Ωƒπ j JƒÇ≈Ωƒπ&scaron;y A + ƒÇÀò&bdquo;ƒπ&scaron;), where JƒÇ≈Ωƒπ&scaron;y is the Jacobian of the mapping ƒÇ≈Ωƒπ&scaron;y . 3 Experiments We run experiments on both real image sequences (√Ñ?ƒπ≈° g. 5 and √Ñ?ƒπ≈° g. 6) and on sequences where silhouettes were arti√Ñ?ƒπ≈° cially rendered. The prediction error is reported in degrees (for mixture of experts, this is w.r.t. the most probable one, but see also √Ñ?ƒπ≈° g. 4a), and normalized per joint angle, per frame. The models are learned using standard cross-validation. Pre-images are learned using kernel regressors and have average error 1.7o . Training Set and Model State Representation: For training we gather pairs of 3D human poses together with their image projections, here silhouettes, using the graphics package Maya. We use realistically rendered computer graphics human surface models which we animate using human motion capture [1]. Our original human representation (x) is based on articulated skeletons with spherical joints and has 56 skeletal d.o.f. including global translation. The database consists of 8000 samples of human activities including walking, running, turns, jumps, gestures in conversations, quarreling and pantomime. Image Descriptors: We work with image silhouettes obtained using statistical background subtraction (with foreground and background models). Silhouettes are informative for pose estimation although prone to ambiguities (e.g. the left / right limb assignment in side views) or occasional lack of observability of some of the d.o.f. (e.g. 180o ambiguities in the global azimuthal orientation for frontal views, e.g. √Ñ?ƒπ≈° g. 1a). These are multiplied by intrinsic forward / backward monocular ambiguities [16]. As observations r, we use shape contexts extracted on the silhouette [4] (5 radial, 12 angular bins, size range 1/8 to 3 on log scale). The features are computed at different scales and sizes for points sampled on the silhouette. To work in a common coordinate system, we cluster all features in the training set into K = 50 clusters. To compute the representation of a new shape feature (a point on the silhouette), we ƒÇÀò&euro;&tilde;projectƒÇÀò&euro;&trade; onto the common basis by (inverse distance) weighted voting into the cluster centers. To obtain the representation (r) for a new silhouette we regularly sample 200 points on it and add all their feature vectors into a feature histogram. Because the representation uses overlapping features of the observation the elements of the descriptor are not independent. However, a conditional temporal framework (√Ñ?ƒπ≈° g. 1b) √Ñ?ƒπ≈°&sbquo;exibly accommodates this. For experiments, we use Gaussian kernels for the joint angle feature space and dot product kernels for the observation feature space. We learn state conditionals for p(yt |zt ) and p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) using 6 dimensions for the joint angle kernel induced state space and 25 dimensions for the observation induced feature space, respectively. In √Ñ?ƒπ≈° g. 3b) we show an evaluation of the ef√Ñ?ƒπ≈° cacy of our kBME predictor for different dimensions in the joint angle kernel induced state space (the observation feature space dimension is here 50). On the analyzed dancing sequence, that involves complex motions of the arms and the legs, the non-linear model signi√Ñ?ƒπ≈° cantly outperforms alternative PCA methods and gives good predictions for compact, low-dimensional models.1 In tables 1 and 2, as well as √Ñ?ƒπ≈° g. 4, we perform quantitative experiments on arti√Ñ?ƒπ≈° cially rendered silhouettes. 3D ground truth joint angles are available and this allows a more 1 Running times: On a Pentium 4 PC (3 GHz, 2 GB RAM), a full dimensional BME model with 5 experts takes 802s to train p(xt |xtƒÇÀò&circ;&rsquo;1 , rt ), whereas a kBME (including the pre-image) takes 95s to train p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ). The prediction time is 13.7s for BME and 8.7s (including the pre-image cost 1.04s) for kBME. The integration in (1) takes 2.67s for BME and 0.31s for kBME. The speed-up for kBME is signi√Ñ?ƒπ≈° cant and likely to increase with original models having higher dimensionality. Prediction Error Number of Clusters 100 1000 100 10 1 1 2 3 4 5 6 7 8 Degree of Multimodality kBME KDE_RVM PCA_BME PCA_RVM 10 1 0 20 40 Number of Dimensions 60 Figure 3: (a, Left) Analysis of ƒÇÀò&euro;&tilde;multimodalityƒÇÀò&euro;&trade; for a training set. The input zt dimension is 25, the output yt dimension is 6, both reduced using kPCA. We cluster independently in (ytƒÇÀò&circ;&rsquo;1 , zt ) and yt using many clusters (2100) to simulate small input perturbations and we histogram the yt clusters falling within each cluster in (ytƒÇÀò&circ;&rsquo;1 , zt ). This gives intuition on the degree of ambiguity in modeling p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), for small perturbations in the input. (b, Right) Evaluation of dimensionality reduction methods for an arti√Ñ?ƒπ≈° cial dancing sequence (models trained on 300 samples). The kBME is our model ƒÇ&sbquo;√Ç¬ß2.2, whereas the KDE-RVM is a KDE model learned with a Relevance Vector Machine (RVM) [17] feature space map. PCA-BME and PCA-RVM are models where the mappings between feature spaces (obtained using PCA) is learned using a BME and a RVM. The non-linearity is signi√Ñ?ƒπ≈° cant. Kernel-based methods outperform PCA and give low prediction error for 5-6d models. systematic evaluation. Notice that the kernelized low-dimensional models generally outperform the PCA ones. At the same time, they give results competitive to the ones of high-dimensional BME predictors, while being lower-dimensional and therefore signi√Ñ?ƒπ≈° cantly less expensive for inference, e.g. the integral in (1). In √Ñ?ƒπ≈° g. 5 and √Ñ?ƒπ≈° g. 6 we show human motion reconstruction results for two real image sequences. Fig. 5 shows the good quality reconstruction of a person performing an agile jump. (Given the missing observations in a side view, 3D inference for the occluded body parts would not be possible without using prior knowledge!) For this sequence we do inference using conditionals having 5 modes and reduced 6d states. We initialize tracking using p(yt |zt ), whereas for inference we use p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) within (1). In the second sequence in √Ñ?ƒπ≈° g. 6, we simultaneously reconstruct the motion of two people mimicking domestic activities, namely washing a window and picking an object. Here we do inference over a product, 12-dimensional state space consisting of the joint 6d state of each person. We obtain good 3D reconstruction results, using only 5 hypotheses. Notice however, that the results are not perfect, there are small errors in the elbow and the bending of the knee for the subject at the l.h.s., and in the different wrist orientations for the subject at the r.h.s. This re√Ñ?ƒπ≈°&sbquo;ects the bias of our training set. Walk and turn Conversation Run and turn left KDE-RR 10.46 7.95 5.22 RVM 4.95 4.96 5.02 KDE-RVM 7.57 6.31 6.25 BME 4.27 4.15 5.01 kBME 4.69 4.79 4.92 Table 1: Comparison of average joint angle prediction error for different models. All kPCA-based models use 6 output dimensions. Testing is done on 100 video frames for each sequence, the inputs are arti√Ñ?ƒπ≈° cially generated silhouettes, not in the training set. 3D joint angle ground truth is used for evaluation. KDE-RR is a KDE model with ridge regression (RR) for the feature space mapping, KDE-RVM uses an RVM. BME uses a Bayesian mixture of experts with no dimensionality reduction. kBME is our proposed model. kPCAbased methods use kernel regressors to compute pre-images. Expert Prediction Frequency ƒÇÀò&circ;&rsquo; Closest to Ground truth Frequency ƒÇÀò&circ;&rsquo; Close to ground truth 30 25 20 15 10 5 0 1 2 3 4 Expert Number 14 10 8 6 4 2 0 5 1st Probable Prev Output 2nd Probable Prev Output 3rd Probable Prev Output 4th Probable Prev Output 5th Probable Prev Output 12 1 2 3 4 Current Expert 5 Figure 4: (a, Left) Histogram showing the accuracy of various expert predictors: how many times the expert ranked as the k-th most probable by the model (horizontal axis) is closest to the ground truth. The model is consistent (the most probable expert indeed is the most accurate most frequently), but occasionally less probable experts are better. (b, Right) Histograms show the dynamics of p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), i.e. how the probability mass is redistributed among experts between two successive time steps, in a conversation sequence. Walk and turn back Run and turn KDE-RR 7.59 17.7 RVM 6.9 16.8 KDE-RVM 7.15 16.08 BME 3.6 8.2 kBME 3.72 8.01 Table 2: Joint angle prediction error computed for two complex sequences with walks, runs and turns, thus more ambiguity (100 frames). Models have 6 state dimensions. Unimodal predictors average competing solutions. kBME has signi√Ñ?ƒπ≈° cantly lower error. Figure 5: Reconstruction of a jump (selected frames). Top: original image sequence. Middle: extracted silhouettes. Bottom: 3D reconstruction seen from a synthetic viewpoint. 4 Conclusion We have presented a probabilistic framework for conditional inference in latent kernelinduced low-dimensional state spaces. Our approach has the following properties: (a) Figure 6: Reconstructing the activities of 2 people operating in an 12-d state space (each person has its own 6d state). Top: original image sequence. Bottom: 3D reconstruction seen from a synthetic viewpoint. Accounts for non-linear correlations among input or output variables, by using kernel nonlinear dimensionality reduction (kPCA); (b) Learns probability distributions over mappings between low-dimensional state spaces using conditional Bayesian mixture of experts, as required for accurate prediction. In the resulting low-dimensional kBME predictor ambiguities and multiple solutions common in visual, inverse perception problems are accurately represented. (c) Works in a continuous, conditional temporal probabilistic setting and offers a formal management of uncertainty. We show comparisons that demonstrate how the proposed approach outperforms regression, PCA or KDE alone for reconstructing the 3D human motion in monocular video. Future work we will investigate scaling aspects for large training sets and alternative structured prediction methods. References [1] CMU Human Motion DataBase. Online at http://mocap.cs.cmu.edu/search.html, 2003. [2] A. Agarwal and B. Triggs. 3d human pose from silhouettes by Relevance Vector Regression. In CVPR, 2004. [3] G. Bakir, J. Weston, and B. Scholkopf. Learning to √Ñ?ƒπ≈° nd pre-images. In NIPS, 2004. [4] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. PAMI, 24, 2002. [5] C. Bishop and M. Svensen. Bayesian mixtures of experts. In UAI, 2003. [6] J. Deutscher, A. Blake, and I. Reid. Articulated Body Motion Capture by Annealed Particle Filtering. In CVPR, 2000. [7] M. Jordan and R. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation, (6):181ƒÇÀò&euro;&ldquo;214, 1994. [8] D. Mackay. Bayesian interpolation. Neural Computation, 4(5):720ƒÇÀò&euro;&ldquo;736, 1992. [9] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction and segmentation. In ICML, 2000. [10] R. Rosales and S. Sclaroff. Learning Body Pose Via Specialized Maps. In NIPS, 2002. [11] B. SchƒÇ&sbquo;√Ç¬® lkopf, A. Smola, and K. MƒÇ&sbquo;√Ç¬® ller. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Computation, 10:1299ƒÇÀò&euro;&ldquo;1319, 1998. [12] G. Shakhnarovich, P. Viola, and T. Darrell. Fast Pose Estimation with Parameter Sensitive Hashing. In ICCV, 2003. [13] L. Sigal, S. Bhatia, S. Roth, M. Black, and M. Isard. Tracking Loose-limbed People. In CVPR, 2004. [14] C. Sminchisescu and A. Jepson. Generative Modeling for Continuous Non-Linearly Embedded Visual Inference. In ICML, pages 759ƒÇÀò&euro;&ldquo;766, Banff, 2004. [15] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Discriminative Density Propagation for 3D Human Motion Estimation. In CVPR, 2005. [16] C. Sminchisescu and B. Triggs. Kinematic Jump Processes for Monocular 3D Human Tracking. In CVPR, volume 1, pages 69ƒÇÀò&euro;&ldquo;76, Madison, 2003. [17] M. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. JMLR, 2001. [18] C. Tomasi, S. Petrov, and A. Sastry. 3d tracking = classi√Ñ?ƒπ≈° cation + interpolation. In ICCV, 2003. [19] J. Weston, O. Chapelle, A. Elisseeff, B. Scholkopf, and V. Vapnik. Kernel dependency estimation. In NIPS, 2002.</p><p>5 0.40246734 <a title="68-lsi-5" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>Author: Juan J. Murillo-fuentes, Sebastian Caro, Fernando P√©rez-Cruz</p><p>Abstract: In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. 1</p><p>6 0.39692923 <a title="68-lsi-6" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>7 0.38442785 <a title="68-lsi-7" href="./nips-2005-Cyclic_Equilibria_in_Markov_Games.html">53 nips-2005-Cyclic Equilibria in Markov Games</a></p>
<p>8 0.3637175 <a title="68-lsi-8" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>9 0.35656887 <a title="68-lsi-9" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>10 0.35626376 <a title="68-lsi-10" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>11 0.35509536 <a title="68-lsi-11" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>12 0.34602174 <a title="68-lsi-12" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>13 0.34566307 <a title="68-lsi-13" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>14 0.3444767 <a title="68-lsi-14" href="./nips-2005-Prediction_and_Change_Detection.html">156 nips-2005-Prediction and Change Detection</a></p>
<p>15 0.33462164 <a title="68-lsi-15" href="./nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">191 nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<p>16 0.33310944 <a title="68-lsi-16" href="./nips-2005-A_Bayesian_Spatial_Scan_Statistic.html">4 nips-2005-A Bayesian Spatial Scan Statistic</a></p>
<p>17 0.33070225 <a title="68-lsi-17" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>18 0.32561597 <a title="68-lsi-18" href="./nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">174 nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<p>19 0.32250434 <a title="68-lsi-19" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>20 0.32173121 <a title="68-lsi-20" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.032), (10, 0.02), (27, 0.03), (31, 0.041), (34, 0.043), (35, 0.017), (39, 0.016), (41, 0.012), (42, 0.43), (44, 0.017), (55, 0.036), (57, 0.021), (69, 0.076), (73, 0.02), (88, 0.069), (91, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86951077 <a title="68-lda-1" href="./nips-2005-Silicon_growth_cones_map_silicon_retina.html">176 nips-2005-Silicon growth cones map silicon retina</a></p>
<p>Author: Brian Taba, Kwabena Boahen</p><p>Abstract: We demonstrate the Ô¨Årst fully hardware implementation of retinotopic self-organization, from photon transduction to neural map formation. A silicon retina transduces patterned illumination into correlated spike trains that drive a population of silicon growth cones to automatically wire a topographic mapping by migrating toward sources of a diffusible guidance cue that is released by postsynaptic spikes. We varied the pattern of illumination to steer growth cones projected by different retinal ganglion cell types to self-organize segregated or coordinated retinotopic maps. 1</p><p>same-paper 2 0.78775632 <a title="68-lda-2" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>Author: Christopher Williams, John Quinn, Neil Mcintosh</p><p>Abstract: The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns. 1</p><p>3 0.29172111 <a title="68-lda-3" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>Author: Jeremy Kubica, Joseph Masiero, Robert Jedicke, Andrew Connolly, Andrew W. Moore</p><p>Abstract: In this paper we consider the problem of Ô¨Ånding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efÔ¨Åciently linking faint asteroid detections, but is applicable to a range of spatial queries. We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches. We empirically show that this algorithm performs well using both simulated and astronomical data.</p><p>4 0.28878015 <a title="68-lda-4" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>Author: Kristina Klinkner, Cosma Shalizi, Marcelo Camperi</p><p>Abstract: Most nervous systems encode information about stimuli in the responding activity of large neuronal networks. This activity often manifests itself as dynamically coordinated sequences of action potentials. Since multiple electrode recordings are now a standard tool in neuroscience research, it is important to have a measure of such network-wide behavioral coordination and information sharing, applicable to multiple neural spike train data. We propose a new statistic, informational coherence, which measures how much better one unit can be predicted by knowing the dynamical state of another. We argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measures of synchronization and correlation. To Ô¨Ånd the dynamical states, we use a recently-introduced algorithm which reconstructs effective state spaces from stochastic time series. We then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi-information. We illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythms. Much of the most important information in neural systems is shared over multiple neurons or cortical areas, in such forms as population codes and distributed representations [1]. On behavioral time scales, neural information is stored in temporal patterns of activity as opposed to static markers; therefore, as information is shared between neurons or brain regions, it is physically instantiated as coordination between entire sequences of neural spikes. Furthermore, neural systems and regions of the brain often require coordinated neural activity to perform important functions; acting in concert requires multiple neurons or cortical areas to share information [2]. Thus, if we want to measure the dynamic network-wide behavior of neurons and test hypotheses about them, we need reliable, practical methods to detect and quantify behavioral coordination and the associated information sharing across multiple neural units. These would be especially useful in testing ideas about how particular forms of coordination relate to distributed coding (e.g., that of [3]). Current techniques to analyze relations among spike trains handle only pairs of neurons, so we further need a method which is extendible to analyze the coordination in the network, system, or region as a whole. Here we propose a new measure of behavioral coordination and information sharing, informational coherence, based on the notion of dynamical state. Section 1 argues that coordinated behavior in neural systems is often not captured by exist- ing measures of synchronization or correlation, and that something sensitive to nonlinear, stochastic, predictive relationships is needed. Section 2 deÔ¨Ånes informational coherence as the (normalized) mutual information between the dynamical states of two systems and explains how looking at the states, rather than just observables, fulÔ¨Ålls the needs laid out in Section 1. Since we rarely know the right states a prori, Section 2.1 brieÔ¨Çy describes how we reconstruct effective state spaces from data. Section 2.2 gives some details about how we calculate the informational coherence and approximate the global information stored in the network. Section 3 applies our method to a model system (a biophysically detailed conductance-based model) comparing our results to those of more familiar second-order statistics. In the interest of space, we omit proofs and a full discussion of the existing literature, giving only minimal references here; proofs and references will appear in a longer paper now in preparation. 1 Synchrony or Coherence? Most hypotheses which involve the idea that information sharing is reÔ¨Çected in coordinated activity across neural units invoke a very speciÔ¨Åc notion of coordinated activity, namely strict synchrony: the units should be doing exactly the same thing (e.g., spiking) at exactly the same time. Investigators then measure coordination by measuring how close the units come to being strictly synchronized (e.g., variance in spike times). From an informational point of view, there is no reason to favor strict synchrony over other kinds of coordination. One neuron consistently spiking 50 ms after another is just as informative a relationship as two simultaneously spiking, but such stable phase relations are missed by strict-synchrony approaches. Indeed, whatever the exact nature of the neural code, it uses temporally extended patterns of activity, and so information sharing should be reÔ¨Çected in coordination of those patterns, rather than just the instantaneous activity. There are three common ways of going beyond strict synchrony: cross-correlation and related second-order statistics, mutual information, and topological generalized synchrony. The cross-correlation function (the normalized covariance function; this includes, for present purposes, the joint peristimulus time histogram [2]), is one of the most widespread measures of synchronization. It can be efÔ¨Åciently calculated from observable series; it handles statistical as well as deterministic relationships between processes; by incorporating variable lags, it reduces the problem of phase locking. Fourier transformation of the covariance function Œ≥XY (h) yields the cross-spectrum FXY (ŒΩ), which in turn gives the 2 spectral coherence cXY (ŒΩ) = FXY (ŒΩ)/FX (ŒΩ)FY (ŒΩ), a normalized correlation between the Fourier components of X and Y . Integrated over frequencies, the spectral coherence measures, essentially, the degree of linear cross-predictability of the two series. ([4] applies spectral coherence to coordinated neural activity.) However, such second-order statistics only handle linear relationships. Since neural processes are known to be strongly nonlinear, there is little reason to think these statistics adequately measure coordination and synchrony in neural systems. Mutual information is attractive because it handles both nonlinear and stochastic relationships and has a very natural and appealing interpretation. Unfortunately, it often seems to fail in practice, being disappointingly small even between signals which are known to be tightly coupled [5]. The major reason is that the neural codes use distinct patterns of activity over time, rather than many different instantaneous actions, and the usual approach misses these extended patterns. Consider two neurons, one of which drives the other to spike 50 ms after it does, the driving neuron spiking once every 500 ms. These are very tightly coordinated, but whether the Ô¨Årst neuron spiked at time t conveys little information about what the second neuron is doing at t ‚Äî it‚Äôs not spiking, but it‚Äôs not spiking most of the time anyway. Mutual information calculated from the direct observations conÔ¨Çates the ‚Äúno spike‚Äù of the second neuron preparing to Ô¨Åre with its just-sitting-around ‚Äúno spike‚Äù. Here, mutual information could Ô¨Ånd the coordination if we used a 50 ms lag, but that won‚Äôt work in general. Take two rate-coding neurons with base-line Ô¨Åring rates of 1 Hz, and suppose that a stimulus excites one to 10 Hz and suppresses the other to 0.1 Hz. The spiking rates thus share a lot of information, but whether the one neuron spiked at t is uninformative about what the other neuron did then, and lagging won‚Äôt help. Generalized synchrony is based on the idea of establishing relationships between the states of the various units. ‚ÄúState‚Äù here is taken in the sense of physics, dynamics and control theory: the state at time t is a variable which Ô¨Åxes the distribution of observables at all times ‚â• t, rendering the past of the system irrelevant [6]. Knowing the state allows us to predict, as well as possible, how the system will evolve, and how it will respond to external forces [7]. Two coupled systems are said to exhibit generalized synchrony if the state of one system is given by a mapping from the state of the other. Applications to data employ statespace reconstruction [8]: if the state x ‚àà X evolves according to smooth, d-dimensional deterministic dynamics, and we observe a generic function y = f (x), then the space Y of time-delay vectors [y(t), y(t ‚àí œÑ ), ...y(t ‚àí (k ‚àí 1)œÑ )] is diffeomorphic to X if k > 2d, for generic choices of lag œÑ . The various versions of generalized synchrony differ on how, precisely, to quantify the mappings between reconstructed state spaces, but they all appear to be empirically equivalent to one another and to notions of phase synchronization based on Hilbert transforms [5]. Thus all of these measures accommodate nonlinear relationships, and are potentially very Ô¨Çexible. Unfortunately, there is essentially no reason to believe that neural systems have deterministic dynamics at experimentally-accessible levels of detail, much less that there are deterministic relationships among such states for different units. What we want, then, but none of these alternatives provides, is a quantity which measures predictive relationships among states, but allows those relationships to be nonlinear and stochastic. The next section introduces just such a measure, which we call ‚Äúinformational coherence‚Äù. 2 States and Informational Coherence There are alternatives to calculating the ‚Äúsurface‚Äù mutual information between the sequences of observations themselves (which, as described, fails to capture coordination). If we know that the units are phase oscillators, or rate coders, we can estimate their instantaneous phase or rate and, by calculating the mutual information between those variables, see how coordinated the units‚Äô patterns of activity are. However, phases and rates do not exhaust the repertoire of neural patterns and a more general, common scheme is desirable. The most general notion of ‚Äúpattern of activity‚Äù is simply that of the dynamical state of the system, in the sense mentioned above. We now formalize this. Assuming the usual notation for Shannon information [9], the information content of a state variable X is H[X] and the mutual information between X and Y is I[X; Y ]. As is well-known, I[X; Y ] ‚â§ min H[X], H[Y ]. We use this to normalize the mutual state information to a 0 ‚àí 1 scale, and this is the informational coherence (IC). œà(X, Y ) = I[X; Y ] , with 0/0 = 0 . min H[X], H[Y ] (1) œà can be interpreted as follows. I[X; Y ] is the Kullback-Leibler divergence between the joint distribution of X and Y , and the product of their marginal distributions [9], indicating the error involved in ignoring the dependence between X and Y . The mutual information between predictive, dynamical states thus gauges the error involved in assuming the two systems are independent, i.e., how much predictions could improve by taking into account the dependence. Hence it measures the amount of dynamically-relevant information shared between the two systems. œà simply normalizes this value, and indicates the degree to which two systems have coordinated patterns of behavior (cf. [10], although this only uses directly observable quantities). 2.1 Reconstruction and Estimation of Effective State Spaces As mentioned, the state space of a deterministic dynamical system can be reconstructed from a sequence of observations. This is the main tool of experimental nonlinear dynamics [8]; but the assumption of determinism is crucial and false, for almost any interesting neural system. While classical state-space reconstruction won‚Äôt work on stochastic processes, such processes do have state-space representations [11], and, in the special case of discretevalued, discrete-time series, there are ways to reconstruct the state space. Here we use the CSSR algorithm, introduced in [12] (code available at http://bactra.org/CSSR). This produces causal state models, which are stochastic automata capable of statistically-optimal nonlinear prediction; the state of the machine is a minimal sufÔ¨Åcient statistic for the future of the observable process[13].1 The basic idea is to form a set of states which should be (1) Markovian, (2) sufÔ¨Åcient statistics for the next observable, and (3) have deterministic transitions (in the automata-theory sense). The algorithm begins with a minimal, one-state, IID model, and checks whether these properties hold, by means of hypothesis tests. If they fail, the model is modiÔ¨Åed, generally but not always by adding more states, and the new model is checked again. Each state of the model corresponds to a distinct distribution over future events, i.e., to a statistical pattern of behavior. Under mild conditions, which do not involve prior knowledge of the state space, CSSR converges in probability to the unique causal state model of the data-generating process [12]. In practice, CSSR is quite fast (linear in the data size), and generalizes at least as well as training hidden Markov models with the EM algorithm and using cross-validation for selection, the standard heuristic [12]. One advantage of the causal state approach (which it shares with classical state-space reconstruction) is that state estimation is greatly simpliÔ¨Åed. In the general case of nonlinear state estimation, it is necessary to know not just the form of the stochastic dynamics in the state space and the observation function, but also their precise parametric values and the distribution of observation and driving noises. Estimating the state from the observable time series then becomes a computationally-intensive application of Bayes‚Äôs Rule [17]. Due to the way causal states are built as statistics of the data, with probability 1 there is a Ô¨Ånite time, t, at which the causal state at time t is certain. This is not just with some degree of belief or conÔ¨Ådence: because of the way the states are constructed, it is impossible for the process to be in any other state at that time. Once the causal state has been established, it can be updated recursively, i.e., the causal state at time t + 1 is an explicit function of the causal state at time t and the observation at t + 1. The causal state model can be automatically converted, therefore, into a Ô¨Ånite-state transducer which reads in an observation time series and outputs the corresponding series of states [18, 13]. (Our implementation of CSSR Ô¨Ålters its training data automatically.) The result is a new time series of states, from which all non-predictive components have been Ô¨Åltered out. 2.2 Estimating the Coherence Our algorithm for estimating the matrix of informational coherences is as follows. For each unit, we reconstruct the causal state model, and Ô¨Ålter the observable time series to produce a series of causal states. Then, for each pair of neurons, we construct a joint histogram of 1 Causal state models have the same expressive power as observable operator models [14] or predictive state representations [7], and greater power than variable-length Markov models [15, 16]. a b Figure 1: Rastergrams of neuronal spike-times in the network. Excitatory, pyramidal neurons (numbers 1 to 1000) are shown in green, inhibitory interneurons (numbers 1001 to 1300) in red. During the Ô¨Årst 10 seconds (a), the current connections among the pyramidal cells are suppressed and a gamma rhythm emerges (left). At t = 10s, those connections become active, leading to a beta rhythm (b, right). the state distribution, estimate the mutual information between the states, and normalize by the single-unit state informations. This gives a symmetric matrix of œà values. Even if two systems are independent, their estimated IC will, on average, be positive, because, while they should have zero mutual information, the empirical estimate of mutual information is non-negative. Thus, the signiÔ¨Åcance of IC values must be assessed against the null hypothesis of system independence. The easiest way to do so is to take the reconstructed state models for the two systems and run them forward, independently of one another, to generate a large number of simulated state sequences; from these calculate values of the IC. This procedure will approximate the sampling distribution of the IC under a null model which preserves the dynamics of each system, but not their interaction. We can then Ô¨Ånd p-values as usual. We omit them here to save space. 2.3 Approximating the Network Multi-Information There is broad agreement [2] that analyses of networks should not just be an analysis of pairs of neurons, averaged over pairs. Ideally, an analysis of information sharing in a network would look at the over-all structure of statistical dependence between the various units, reÔ¨Çected in the complete joint probability distribution P of the states. This would then allow us, for instance, to calculate the n-fold multi-information, I[X1 , X2 , . . . Xn ] ‚â° D(P ||Q), the Kullback-Leibler divergence between the joint distribution P and the product of marginal distributions Q, analogous to the pairwise mutual information [19]. Calculated over the predictive states, the multi-information would give the total amount of shared dynamical information in the system. Just as we normalized the mutual information I[X1 , X2 ] by its maximum possible value, min H[X1 ], H[X2 ], we normalize the multiinformation by its maximum, which is the smallest sum of n ‚àí 1 marginal entropies: I[X1 ; X2 ; . . . Xn ] ‚â§ min k H[Xn ] i=k Unfortunately, P is a distribution over a very high dimensional space and so, hard to estimate well without strong parametric constraints. We thus consider approximations. The lowest-order approximation treats all the units as independent; this is the distribution Q. One step up are tree distributions, where the global distribution is a function of the joint distributions of pairs of units. Not every pair of units needs to enter into such a distribution, though every unit must be part of some pair. Graphically, a tree distribution corresponds to a spanning tree, with edges linking units whose interactions enter into the global probability, and conversely spanning trees determine tree distributions. Writing ET for the set of pairs (i, j) and abbreviating X1 = x1 , X2 = x2 , . . . Xn = xn by X = x, one has n T (X = x) = (i,j)‚ààET T (Xi = xi , Xj = xj ) T (Xi = xi ) T (Xi = xi )T (Xj = xj ) i=1 (2) where the marginal distributions T (Xi ) and the pair distributions T (Xi , Xj ) are estimated by the empirical marginal and pair distributions. We must now pick edges ET so that T best approximates the true global distribution P . A natural approach is to minimize D(P ||T ), the divergence between P and its tree approximation. Chow and Liu [20] showed that the maximum-weight spanning tree gives the divergence-minimizing distribution, taking an edge‚Äôs weight to be the mutual information between the variables it links. There are three advantages to using the Chow-Liu approximation. (1) Estimating T from empirical probabilities gives a consistent maximum likelihood estimator of the ideal ChowLiu tree [20], with reasonable rates of convergence, so T can be reliably known even if P cannot. (2) There are efÔ¨Åcient algorithms for constructing maximum-weight spanning trees, such as Prim‚Äôs algorithm [21, sec. 23.2], which runs in time O(n2 + n log n). Thus, the approximation is computationally tractable. (3) The KL divergence of the Chow-Liu distribution from Q gives a lower bound on the network multi-information; that bound is just the sum of the mutual informations along the edges in the tree: I[X1 ; X2 ; . . . Xn ] ‚â• D(T ||Q) = I[Xi ; Xj ] (3) (i,j)‚ààET Even if we knew P exactly, Eq. 3 would be useful as an alternative to calculating D(P ||Q) directly, evaluating log P (x)/Q(x) for all the exponentially-many conÔ¨Ågurations x. It is natural to seek higher-order approximations to P , e.g., using three-way interactions not decomposable into pairwise interactions [22, 19]. But it is hard to do so effectively, because Ô¨Ånding the optimal approximation to P when such interactions are allowed is NP [23], and analytical formulas like Eq. 3 generally do not exist [19]. We therefore conÔ¨Åne ourselves to the Chow-Liu approximation here. 3 Example: A Model of Gamma and Beta Rhythms We use simulated data as a test case, instead of empirical multiple electrode recordings, which allows us to try the method on a system of over 1000 neurons and compare the measure against expected results. The model, taken from [24], was originally designed to study episodes of gamma (30‚Äì80Hz) and beta (12‚Äì30Hz) oscillations in the mammalian nervous system, which often occur successively with a spontaneous transition between them. More concretely, the rhythms studied were those displayed by in vitro hippocampal (CA1) slice preparations and by in vivo neocortical EEGs. The model contains two neuron populations: excitatory (AMPA) pyramidal neurons and inhibitory (GABAA ) interneurons, deÔ¨Åned by conductance-based Hodgkin-Huxley-style equations. Simulations were carried out in a network of 1000 pyramidal cells and 300 interneurons. Each cell was modeled as a one-compartment neuron with all-to-all coupling, endowed with the basic sodium and potassium spiking currents, an external applied current, and some Gaussian input noise. The Ô¨Årst 10 seconds of the simulation correspond to the gamma rhythm, in which only a group of neurons is made to spike via a linearly increasing applied current. The beta rhythm a b c d Figure 2: Heat-maps of coordination for the network, as measured by zero-lag cross-correlation (top row) and informational coherence (bottom), contrasting the gamma rhythm (left column) with the beta (right). Colors run from red (no coordination) through yellow to pale cream (maximum). (subsequent 10 seconds) is obtained by activating pyramidal-pyramidal recurrent connections (potentiated by Hebbian preprocessing as a result of synchrony during the gamma rhythm) and a slow outward after-hyper-polarization (AHP) current (the M-current), suppressed during gamma due to the metabotropic activation used in the generation of the rhythm. During the beta rhythm, pyramidal cells, silent during gamma rhythm, Ô¨Åre on a subset of interneurons cycles (Fig. 1). Fig. 2 compares zero-lag cross-correlation, a second-order method of quantifying coordination, with the informational coherence calculated from the reconstructed states. (In this simulation, we could have calculated the actual states of the model neurons directly, rather than reconstructing them, but for purposes of testing our method we did not.) Crosscorrelation Ô¨Ånds some of the relationships visible in Fig. 1, but is confused by, for instance, the phase shifts between pyramidal cells. (Surface mutual information, not shown, gives similar results.) Informational coherence, however, has no trouble recognizing the two populations as effectively coordinated blocks. The presence of dynamical noise, problematic for ordinary state reconstruction, is not an issue. The average IC is 0.411 (or 0.797 if the inactive, low-numbered neurons are excluded). The tree estimate of the global informational multi-information is 3243.7 bits, with a global coherence of 0.777. The right half of Fig. 2 repeats this analysis for the beta rhythm; in this stage, the average IC is 0.614, and the tree estimate of the global multi-information is 7377.7 bits, though the estimated global coherence falls very slightly to 0.742. This is because low-numbered neurons which were quiescent before are now active, contributing to the global information, but the over-all pattern is somewhat weaker and more noisy (as can be seen from Fig. 1b.) So, as expected, the total information content is higher, but the overall coordination across the network is lower. 4 Conclusion Informational coherence provides a measure of neural information sharing and coordinated activity which accommodates nonlinear, stochastic relationships between extended patterns of spiking. It is robust to dynamical noise and leads to a genuinely multivariate measure of global coordination across networks or regions. Applied to data from multi-electrode recordings, it should be a valuable tool in evaluating hypotheses about distributed neural representation and function. Acknowledgments Thanks to R. Haslinger, E. Ionides and S. Page; and for support to the Santa Fe Institute (under grants from Intel, the NSF and the MacArthur Foundation, and DARPA agreement F30602-00-2-0583), the Clare Booth Luce Foundation (KLK) and the James S. McDonnell Foundation (CRS). References [1] L. F. Abbott and T. J. Sejnowski, eds. Neural Codes and Distributed Representations. MIT Press, 1998. [2] E. N. Brown, R. E. Kass, and P. P. Mitra. Nature Neuroscience, 7:456‚Äì461, 2004. [3] D. H. Ballard, Z. Zhang, and R. P. N. Rao. In R. P. N. Rao, B. A. Olshausen, and M. S. Lewicki, eds., Probabilistic Models of the Brain, pp. 273‚Äì284, MIT Press, 2002. [4] D. R. Brillinger and A. E. P. Villa. In D. R. Brillinger, L. T. Fernholz, and S. Morgenthaler, eds., The Practice of Data Analysis, pp. 77‚Äì92. Princeton U.P., 1997. [5] R. Quian Quiroga et al. Physical Review E, 65:041903, 2002. [6] R. F. Streater. Statistical Dynamics. Imperial College Press, London. [7] M. L. Littman, R. S. Sutton, and S. Singh. In T. G. Dietterich, S. Becker, and Z. Ghahramani, eds., Advances in Neural Information Processing Systems 14, pp. 1555‚Äì1561. MIT Press, 2002. [8] H. Kantz and T. Schreiber. Nonlinear Time Series Analysis. Cambridge U.P., 1997. [9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991. [10] M. Palus et al. Physical Review E, 63:046211, 2001. [11] F. B. Knight. Annals of Probability, 3:573‚Äì596, 1975. [12] C. R. Shalizi and K. L. Shalizi. In M. Chickering and J. Halpern, eds., Uncertainty in ArtiÔ¨Åcial Intelligence: Proceedings of the Twentieth Conference, pp. 504‚Äì511. AUAI Press, 2004. [13] C. R. Shalizi and J. P. CrutchÔ¨Åeld. Journal of Statistical Physics, 104:817‚Äì819, 2001. [14] H. Jaeger. Neural Computation, 12:1371‚Äì1398, 2000. [15] D. Ron, Y. Singer, and N. Tishby. Machine Learning, 25:117‚Äì149, 1996. [16] P. B¬® hlmann and A. J. Wyner. Annals of Statistics, 27:480‚Äì513, 1999. u [17] N. U. Ahmed. Linear and Nonlinear Filtering for Scientists and Engineers. World ScientiÔ¨Åc, 1998. [18] D. R. Upper. PhD thesis, University of California, Berkeley, 1997. [19] E. Schneidman, S. Still, M. J. Berry, and W. Bialek. Physical Review Letters, 91:238701, 2003. [20] C. K. Chow and C. N. Liu. IEEE Transactions on Information Theory, IT-14:462‚Äì467, 1968. [21] T. H. Cormen et al. Introduction to Algorithms. 2nd ed. MIT Press, 2001. [22] S. Amari. IEEE Transacttions on Information Theory, 47:1701‚Äì1711, 2001. [23] S. Kirshner, P. Smyth, and A. Robertson. Tech. Rep. 04-04, UC Irvine, Information and Computer Science, 2004. [24] M. S. Olufsen et al. Journal of Computational Neuroscience, 14:33‚Äì54, 2003.</p><p>5 0.28707302 <a title="68-lda-5" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>Author: Matthias Oster, Shih-Chii Liu</p><p>Abstract: Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-Ô¨Åre neurons which receives spike trains as inputs. We show how we can conÔ¨Ågure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-Ô¨Åre neurons which have an innate variance in their operating parameters. 1</p><p>6 0.28658745 <a title="68-lda-6" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>7 0.28474337 <a title="68-lda-7" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>8 0.28316116 <a title="68-lda-8" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>9 0.28174627 <a title="68-lda-9" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>10 0.27954146 <a title="68-lda-10" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>11 0.276256 <a title="68-lda-11" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>12 0.27620536 <a title="68-lda-12" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>13 0.27459118 <a title="68-lda-13" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>14 0.27448982 <a title="68-lda-14" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>15 0.27436134 <a title="68-lda-15" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>16 0.27352628 <a title="68-lda-16" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>17 0.27321494 <a title="68-lda-17" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>18 0.27209142 <a title="68-lda-18" href="./nips-2005-Learning_Shared_Latent_Structure_for_Image_Synthesis_and_Robotic_Imitation.html">115 nips-2005-Learning Shared Latent Structure for Image Synthesis and Robotic Imitation</a></p>
<p>19 0.2718634 <a title="68-lda-19" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>20 0.27119428 <a title="68-lda-20" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
