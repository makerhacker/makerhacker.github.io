<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-2" href="#">nips2000-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</h1>
<br/><p>Source: <a title="nips-2000-2-pdf" href="http://papers.nips.cc/paper/1877-a-comparison-of-image-processing-techniques-for-visual-speech-recognition-applications.pdf">pdf</a></p><p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>Reference: <a title="nips-2000-2-reference" href="../nips2000_reference/nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. [sent-5, score-0.486]
</p><p>2 We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. [sent-6, score-0.767]
</p><p>3 This result is consistent with previous work on emotion and facial expression recognition. [sent-7, score-0.237]
</p><p>4 In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. [sent-8, score-0.308]
</p><p>5 1  Introduction  We study the performance of eight different methods for developing image representations based on the statistical properties of the images at hand. [sent-9, score-0.758]
</p><p>6 These methods are compared on their performance on a visual speech recognition task. [sent-10, score-0.33]
</p><p>7 While the representations developed are specific to visual speech recognition, the methods themselves are general purpose and applicable to other tasks. [sent-11, score-0.346]
</p><p>8 Our focus is on low-level data-driven methods based on the statistical properties of relatively untouched images, as opposed to approaches that work with contours or highly processed versions of the image. [sent-12, score-0.129]
</p><p>9 Padgett [8] and Bartlett [1] systematically studied statistical methods for developing representations on expression recognition tasks. [sent-13, score-0.409]
</p><p>10 They found that local wavelet-like representations consistently outperformed global representations, like eigenfaces. [sent-14, score-0.561]
</p><p>11 In this paper we also compare local versus global representations. [sent-15, score-0.357]
</p><p>12 In each panel, the "+" indicates the center of the lips, and the "0" indicates the center of the image. [sent-18, score-0.078]
</p><p>13 The location of the lips was automatically determined using Luettin et al. [sent-19, score-0.253]
</p><p>14 In addition to the comparison of local and global representations, we propose an unsupervised method for automatically selecting regions and variables of interest. [sent-22, score-0.503]
</p><p>15 2  Preprocessing and Recognition Engine  The task was recognition of the words "one", "two", "three" and "four" from the Tulips1 [7] database. [sent-23, score-0.143]
</p><p>16 The database consists on movies of 12 subjects each uttering the digits in English twice. [sent-24, score-0.078]
</p><p>17 While the number of words is limited, the database is challenging due to differences in illumination conditions, ethnicity and gender of the subjects. [sent-25, score-0.138]
</p><p>18 Image preprocessing consisted of the following steps: First the contour of the outer lips were tracked using point distribution models, a data-driven technique based on analysis ofthe gray-level statistics around lip contours [5]. [sent-26, score-0.429]
</p><p>19 The lip images were then normalized for translation and rotation. [sent-27, score-0.426]
</p><p>20 This was accomplished by first padding the image on all sides with 25 rows or columns of zeros, and modulating the images in the spatial frequency domain. [sent-28, score-0.492]
</p><p>21 The images were symmetrized with respect to the vertical axis going through the center of the lips. [sent-29, score-0.322]
</p><p>22 The images were cropped to 65 pixels vertically x 87 pixels horizontally (see Figure 1) and their intensity was normalized using logistic gain control [7]. [sent-31, score-0.5]
</p><p>23 Eight different techniques were used on the normalized database each of which developed a different image basis. [sent-32, score-0.41]
</p><p>24 This results in a form of soft histogram equalization; (4)  Global PCA PCA  Spectrum Global TCA lCA  Spectrum  Figure 2: Global decompositions for the normalized image dataset. [sent-34, score-0.33]
</p><p>25 Row 1: Global kernels of principal component analysis ordered with first eigenimage on left. [sent-35, score-0.264]
</p><p>26 Row 3: Global pixel space independent component kernels ordered according to projected variance. [sent-37, score-0.34]
</p><p>27 Row 4: Log magnitude spectrum of global independent components. [sent-38, score-0.292]
</p><p>28 Recognition: The scaled x(t) and 8(t) coefficients are fed to the HMM recognition engine. [sent-39, score-0.181]
</p><p>29 3  Global Methods  We first evaluated the performance of techniques based on the statistics of the entire lip images as opposed to portions of it. [sent-40, score-0.566]
</p><p>30 This global approach has been shown to provide good performance on face recognition [9], expression recognition [2], and gender recognition tasks [4]. [sent-41, score-0.882]
</p><p>31 In particular we compared the performance of principal component analysis (PCA) and two different versions of independent component analysis (ICA). [sent-42, score-0.249]
</p><p>32 1  Global PC A:  We tried image bases that consisted of the first 50, 100 and 150 eigenvectors of the pixelwise covariance matrix. [sent-44, score-0.541]
</p><p>33 The top row of Figure 2 shows the first 5 eigenvectors displayed as images, their magnitude spectrum is shown in the second row. [sent-47, score-0.275]
</p><p>34 These eigenimages have most of their energy localized in low and horizontal spatial frequencies and are typically non-local in the spatial domain (i. [sent-48, score-0.118]
</p><p>35 We tried two different ICA approaches:  ICA I: This method results in a non-orthogonal transformation of the bases developed via PCA. [sent-55, score-0.264]
</p><p>36 While such transformations do not change the underlying space of  Figure 3: Upper left: Lip patches (12 pixels x 12 pixels) from randomly chosen locations used to develop local PCA and local lCA kernels. [sent-56, score-0.628]
</p><p>37 Lower left: Four orthogonal images generated from a single local PCA kernel. [sent-57, score-0.406]
</p><p>38 Right: Top 10 Local PCA and lCA kernels ordered according to projected variance (highest at top left). [sent-58, score-0.306]
</p><p>39 Note how the lCA vectors tend to be more local and consistent with the receptive fields found in VI. [sent-59, score-0.17]
</p><p>40 the representation they may facilitate the job of the recognition engine by decreasing the statistical dependency amongst the coordinates. [sent-60, score-0.218]
</p><p>41 First each image in the database was projected onto the space spanned by the first 50 eigenvectors of the pixelwise covariance matrix. [sent-61, score-0.519]
</p><p>42 lCA II: A different approach to lCA was explored in [1] for face recognition tasks and by [6] for fMRI images. [sent-63, score-0.271]
</p><p>43 While in lCA-l the goal is to develop independent image coordinates, in rcA-II the goal is for the image bases themselves to be independent. [sent-64, score-0.64]
</p><p>44 Here independence of images is defined with respect to a probability space in which pixels are seen as outcomes and images as random vectors of such outcomes. [sent-65, score-0.464]
</p><p>45 The approach, which is described in detail in [6], resulted in a set of 50 images which were a non-orthogonal linear transformation of the first 50 eigenvectors of the pixelwise covariance matrix. [sent-66, score-0.337]
</p><p>46 The first 5 images (accounting for the largest amounts of projected variance) obtained via this approach to lCA are shown in the third row of Figure 2. [sent-67, score-0.378]
</p><p>47 As reported in [1] the images obtained using this method are more local than those obtained via PCA. [sent-69, score-0.459]
</p><p>48 [8] reported surprisingly good results on an emotion recognition tasks using PCA on random patches of the face instead of the entire face. [sent-71, score-0.679]
</p><p>49 Recent theoretical work also places emphasis on spatially localized, wavelet-like image bases. [sent-72, score-0.236]
</p><p>50 One potential advantage of spatially localized image bases is that they provide explicit information about where things are happening, not just about what is happening. [sent-73, score-0.521]
</p><p>51 This facilitates the work of recognition engines on some tasks but the theoretical reasons for this are unclear at this point. [sent-74, score-0.244]
</p><p>52 Local PCA and lCA kernels were developed based on a database of 18680 small patches (12 pixel x 12 pixel) chosen from random locations in the Tulip1s database. [sent-75, score-0.534]
</p><p>53 A sample of these random patches (superimposed on a lip image) is shown in the top panel of Figure 3. [sent-76, score-0.408]
</p><p>54 Hereafter we refer to the 12 pixel x 12 pixel images obtained  PCAKemeil  PCAKemel2  2  20  n "  ""  . [sent-77, score-0.318]
</p><p>55 LLL  "  , "  ICAKemel9  ~  " "  , " 41  10  '"  Figure 4: Kernel-location combinations chosen using unblocked variable selection. [sent-81, score-0.25]
</p><p>56 Bottom of each quadrant: Lip image convolved with corresponding local kernel, then downsampled. [sent-83, score-0.454]
</p><p>57 The numbers on the lip image indicate the order in which variables were chosen for the multiple regression procedure. [sent-84, score-0.644]
</p><p>58 There are no numbers on the right side of the lip images because only half of each lip image was used for the representation (since the images are symmetrized). [sent-85, score-1.088]
</p><p>59 Image bases were generated by centering a local peA or leA kernel onto different locations and padding the rest of the matrix with zeros, as displayed in Figure 3 (lower left panel). [sent-87, score-0.477]
</p><p>60 This results on bases images which are local in space (the energy is localized about a single patch) and shifted versions of each other. [sent-88, score-0.673]
</p><p>61 The process of obtaining image coordinates can be seen as a filtering operation followed by subsampling: First the images are filtered using a bank of filters whose impulse response are the kernels obtained via peA (or leA). [sent-89, score-0.964]
</p><p>62 The relevant coordinates are obtained by subsampling at 300 uniformly distributed locations (15 locations vertically by 20 locations horizontally). [sent-90, score-0.391]
</p><p>63 We explored four different filtering approaches: (1) Single linear shift invariant filter (LSI); (2) Single linear shift variant filter (LSV); (3) Bank of LSI filters with blocked selection; (4) Bank of LSI filters combined with unblocked selection. [sent-91, score-0.683]
</p><p>64 For the single-filter LSI approach, the images were convolved with a single local leA kernel or a local peA kernel. [sent-92, score-0.588]
</p><p>65 The top 5 local peA and leA kernels were each tested separately and the results obtained with the best of the 5 kernels were reported. [sent-93, score-0.498]
</p><p>66 For the single LSV-filtering approach different local peA kernels were derived for a total of 117 non-overlapping regions each of which occupied 5 x 5 pixels. [sent-94, score-0.346]
</p><p>67 Each region of the 934 images was projected onto the first principal component corresponding to that location. [sent-95, score-0.373]
</p><p>68 1  Automatic Selection of Focal Points  Padgett's [8] most successful method was based on outputs of local filters at manually selected focal regions. [sent-98, score-0.402]
</p><p>69 Their task was emotion recognition and the focal regions were the eyes and mouth. [sent-99, score-0.48]
</p><p>70 2  Table 1: Best generalization performance (% correct) Â± standard error of the mean for all image representations. [sent-121, score-0.272]
</p><p>71 Thus we developed a method for automatic selection of focal regions. [sent-123, score-0.214]
</p><p>72 First 10 filters were developed via local leA (or peA). [sent-124, score-0.366]
</p><p>73 Each image was filtered using the 10-filter bank and the outputs were subsampled at 150 locations for a 1500 dimensional representation (10 filters x 150 locations) of each of the images in the dataset. [sent-125, score-0.858]
</p><p>74 Regions and variables of interest were then selected using a stepwise forward multiple regression procedure. [sent-126, score-0.246]
</p><p>75 First we choose the variable that, when averaging across the entire database, best reconstructed the original images. [sent-127, score-0.16]
</p><p>76 Here best reconstruction is defined in terms of least squares using a multiple regression model. [sent-128, score-0.144]
</p><p>77 Once a variable is selected, it is "tenured" and we search for the variable which in combination with the tenured ones best reconstructs the image database. [sent-129, score-0.476]
</p><p>78 The procedure is stopped when the number of tenured variables reaches a criterion point. [sent-130, score-0.159]
</p><p>79 We compared performance using 50, 100, and 150 tenured variables and report results with the best of those three numbers. [sent-131, score-0.252]
</p><p>80 We tested two different selection procedures, one blocked by location and one in which location was not blocked. [sent-132, score-0.295]
</p><p>81 In the first method the selection was done in blocks of 10 variables where each block contained the outputs of all the filters at a specific location. [sent-133, score-0.275]
</p><p>82 If a location was chosen, the outputs of the 10 filters in that location were automatically included in the final image representation. [sent-134, score-0.524]
</p><p>83 In the second method selection of variables was not blocked by location. [sent-135, score-0.243]
</p><p>84 Figure 4 shows, for 2 local peA and 2 local leA kernels, the first 10 variables chosen for each particular kernel using the forward selection multiple regression procedure. [sent-136, score-0.597]
</p><p>85 The numbers on the lip images in this figure indicate the order in which particular kernel/location variables were chosen using the sequential regression procedure: "I" indicates the first variable chosen, "2" the second, etc. [sent-137, score-0.644]
</p><p>86 5  Results and Conclusions  Table 1 shows the best generalization performance (out of the 9 HMM architectures tested) for each of the eight image representation methods. [sent-138, score-0.391]
</p><p>87 The local decompositions significantly outperformed the global ones (t(106) = 4. [sent-139, score-0.519]
</p><p>88 The improved performance of local representations is consistent with current ideas on the importance of localized wavelet-like representations. [sent-142, score-0.423]
</p><p>89 However, it is unclear why local decompositions work better. [sent-143, score-0.324]
</p><p>90 One possibility is that these results apply only to this particular recognition engine and the problem at hand (i. [sent-144, score-0.218]
</p><p>91 Yet similar results with local representations were reported in [8] on an emotion classification task with a 3 layer backpropaga-  tion network and in [1] on an expression classification tasks with a nearest neighbor classifier. [sent-147, score-0.636]
</p><p>92 Another possible explanation for the advantage of local representations is that global unsupervised decompositions emphasize subject identity while local decompositions tend to hide it. [sent-148, score-0.851]
</p><p>93 We found some evidence consistent with this idea by testing global and local representations on a subject identification task (i. [sent-149, score-0.493]
</p><p>94 For this task the global representations outperformed the local ones. [sent-152, score-0.561]
</p><p>95 However this result is inconsistent with [8] which found local representations were better on emotion classification and on subject identification tasks. [sent-153, score-0.501]
</p><p>96 Another possibility is that local representations make more explicit information about where things are happening, not just what is happening, and such information turns out to be important for the task at hand. [sent-154, score-0.342]
</p><p>97 The image representations obtained using the bank of filter methods with unblocked selection yielded the best results. [sent-155, score-0.977]
</p><p>98 The stepwise regression technique used to select kernels and regions of interest led to substantial gains in recognition performance. [sent-156, score-0.517]
</p><p>99 In fact the highest generalization performance reported here (91. [sent-157, score-0.088]
</p><p>100 7% with the bank of filters using unblocked variable selection) surpassed the best published performance on this dataset [5]. [sent-158, score-0.583]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('image', 0.236), ('lip', 0.226), ('pea', 0.203), ('images', 0.2), ('lca', 0.195), ('emotion', 0.195), ('bank', 0.187), ('global', 0.187), ('local', 0.17), ('bases', 0.168), ('lips', 0.167), ('unblocked', 0.167), ('pca', 0.166), ('recognition', 0.143), ('representations', 0.136), ('ica', 0.135), ('blocked', 0.12), ('lea', 0.116), ('kernels', 0.114), ('padgett', 0.111), ('stepwise', 0.111), ('tenured', 0.111), ('lsi', 0.102), ('filters', 0.1), ('decompositions', 0.094), ('patches', 0.094), ('regression', 0.087), ('face', 0.087), ('pixelwise', 0.083), ('symmetrized', 0.083), ('locations', 0.083), ('localized', 0.081), ('focal', 0.08), ('filter', 0.08), ('database', 0.078), ('engine', 0.075), ('selection', 0.075), ('row', 0.073), ('happening', 0.072), ('diego', 0.071), ('outperformed', 0.068), ('projected', 0.068), ('entire', 0.067), ('pixels', 0.064), ('spectrum', 0.064), ('regions', 0.062), ('speech', 0.062), ('eight', 0.062), ('gender', 0.06), ('unclear', 0.06), ('pixel', 0.059), ('developed', 0.059), ('best', 0.057), ('san', 0.057), ('fmri', 0.056), ('lsv', 0.056), ('padding', 0.056), ('component', 0.054), ('coordinates', 0.054), ('eigenvectors', 0.054), ('versions', 0.054), ('outputs', 0.052), ('reported', 0.052), ('principal', 0.051), ('visual', 0.05), ('location', 0.05), ('developing', 0.049), ('logistic', 0.049), ('convolved', 0.048), ('salk', 0.048), ('subsampling', 0.048), ('zeros', 0.048), ('variables', 0.048), ('chosen', 0.047), ('ordered', 0.045), ('panel', 0.045), ('horizontally', 0.043), ('quadrant', 0.043), ('top', 0.043), ('expression', 0.042), ('tasks', 0.041), ('magnitude', 0.041), ('rca', 0.04), ('vertically', 0.04), ('gain', 0.04), ('methods', 0.039), ('center', 0.039), ('scaled', 0.038), ('horizontal', 0.037), ('techniques', 0.037), ('via', 0.037), ('automatically', 0.036), ('filtering', 0.036), ('orthogonal', 0.036), ('variable', 0.036), ('variance', 0.036), ('contours', 0.036), ('neurobiology', 0.036), ('things', 0.036), ('performance', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="2-tfidf-1" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>2 0.19976963 <a title="2-tfidf-2" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>3 0.18023469 <a title="2-tfidf-3" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>Author: Yee Whye Teh, Geoffrey E. Hinton</p><p>Abstract: We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.</p><p>4 0.16330032 <a title="2-tfidf-4" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>5 0.15269658 <a title="2-tfidf-5" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>Author: Michael E. Tipping</p><p>Abstract: 'Kernel' principal component analysis (PCA) is an elegant nonlinear generalisation of the popular linear data analysis method, where a kernel function implicitly defines a nonlinear transformation into a feature space wherein standard PCA is performed. Unfortunately, the technique is not 'sparse', since the components thus obtained are expressed in terms of kernels associated with every training vector. This paper shows that by approximating the covariance matrix in feature space by a reduced number of example vectors, using a maximum-likelihood approach, we may obtain a highly sparse form of kernel PCA without loss of effectiveness. 1</p><p>6 0.1192287 <a title="2-tfidf-6" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>7 0.11793606 <a title="2-tfidf-7" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>8 0.11479666 <a title="2-tfidf-8" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>9 0.11225614 <a title="2-tfidf-9" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>10 0.11015267 <a title="2-tfidf-10" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>11 0.10996415 <a title="2-tfidf-11" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>12 0.095556296 <a title="2-tfidf-12" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>13 0.090062395 <a title="2-tfidf-13" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>14 0.089303389 <a title="2-tfidf-14" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>15 0.088571459 <a title="2-tfidf-15" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>16 0.086785652 <a title="2-tfidf-16" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>17 0.086384237 <a title="2-tfidf-17" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>18 0.08621747 <a title="2-tfidf-18" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>19 0.085446954 <a title="2-tfidf-19" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>20 0.078832567 <a title="2-tfidf-20" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.266), (1, -0.112), (2, 0.091), (3, 0.25), (4, -0.094), (5, 0.135), (6, 0.066), (7, -0.122), (8, 0.092), (9, -0.022), (10, -0.172), (11, -0.184), (12, 0.02), (13, -0.002), (14, -0.175), (15, 0.014), (16, -0.043), (17, 0.134), (18, -0.139), (19, -0.045), (20, 0.005), (21, -0.19), (22, -0.11), (23, -0.047), (24, 0.005), (25, 0.036), (26, 0.06), (27, -0.108), (28, 0.047), (29, 0.002), (30, -0.011), (31, 0.072), (32, -0.051), (33, -0.103), (34, -0.063), (35, 0.048), (36, 0.053), (37, 0.011), (38, 0.039), (39, -0.025), (40, -0.046), (41, 0.074), (42, -0.049), (43, 0.037), (44, -0.016), (45, -0.066), (46, -0.027), (47, 0.056), (48, 0.085), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98378289 <a title="2-lsi-1" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>2 0.74681765 <a title="2-lsi-2" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>3 0.70759058 <a title="2-lsi-3" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>4 0.6129753 <a title="2-lsi-4" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>5 0.5017783 <a title="2-lsi-5" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>Author: Yee Whye Teh, Geoffrey E. Hinton</p><p>Abstract: We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.</p><p>6 0.45703223 <a title="2-lsi-6" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>7 0.43314794 <a title="2-lsi-7" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>8 0.42146191 <a title="2-lsi-8" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>9 0.40217337 <a title="2-lsi-9" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>10 0.39029953 <a title="2-lsi-10" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>11 0.37735856 <a title="2-lsi-11" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>12 0.37515551 <a title="2-lsi-12" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>13 0.37504837 <a title="2-lsi-13" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>14 0.37287569 <a title="2-lsi-14" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>15 0.35564962 <a title="2-lsi-15" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>16 0.34604716 <a title="2-lsi-16" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>17 0.32343358 <a title="2-lsi-17" href="./nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">61 nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>18 0.32052794 <a title="2-lsi-18" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>19 0.29959789 <a title="2-lsi-19" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>20 0.29515201 <a title="2-lsi-20" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.241), (10, 0.038), (17, 0.237), (32, 0.014), (33, 0.044), (36, 0.011), (55, 0.038), (62, 0.021), (65, 0.034), (67, 0.052), (75, 0.015), (76, 0.046), (79, 0.025), (81, 0.04), (90, 0.027), (91, 0.012), (97, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8745032 <a title="2-lda-1" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>2 0.8495751 <a title="2-lda-2" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>Author: Vladimir Pavlovic, James M. Rehg, John MacCormick</p><p>Abstract: The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. Effective models of human dynamics can be learned from motion capture data using switching linear dynamic system (SLDS) models. We present results for human motion synthesis, classification, and visual tracking using learned SLDS models. Since exact inference in SLDS is intractable, we present three approximate inference algorithms and compare their performance. In particular, a new variational inference algorithm is obtained by casting the SLDS model as a Dynamic Bayesian Network. Classification experiments show the superiority of SLDS over conventional HMM's for our problem domain.</p><p>3 0.71264505 <a title="2-lda-3" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>4 0.70997429 <a title="2-lda-4" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.</p><p>5 0.7045939 <a title="2-lda-5" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>Author: James M. Coughlan, Alan L. Yuille</p><p>Abstract: Preliminary work by the authors made use of the so-called</p><p>6 0.69434196 <a title="2-lda-6" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>7 0.69294792 <a title="2-lda-7" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>8 0.68815368 <a title="2-lda-8" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>9 0.68812275 <a title="2-lda-9" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>10 0.67620701 <a title="2-lda-10" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>11 0.66999394 <a title="2-lda-11" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>12 0.6695267 <a title="2-lda-12" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>13 0.66759801 <a title="2-lda-13" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>14 0.66707778 <a title="2-lda-14" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>15 0.6634239 <a title="2-lda-15" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>16 0.66258574 <a title="2-lda-16" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>17 0.65783238 <a title="2-lda-17" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>18 0.65734315 <a title="2-lda-18" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>19 0.65697849 <a title="2-lda-19" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>20 0.65537435 <a title="2-lda-20" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
