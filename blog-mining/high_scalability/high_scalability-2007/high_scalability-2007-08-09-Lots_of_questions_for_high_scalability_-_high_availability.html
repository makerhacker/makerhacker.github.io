<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-63" href="#">high_scalability-2007-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-63-html" href="http://highscalability.com//blog/2007/8/9/lots-of-questions-for-high-scalability-high-availability.html">html</a></p><p>Introduction: Hey,I do have a website that I would like to scale. Right now we have 10
servers but this does not scale well.I know how to deal with my apache web
servers but have problems with sql servers.I would like to use the "scale out"
system and add servers when we need. We have over 100Gb of data for mysql and
we tried to have around 20G per server. It works well except that if a server
goes down then 1/5 of the user can't access the website. We could use
replication but we would need to at least double sql servers to replicate each
server. And maybe in the future it's not gonna be enough we would need maybe 3
slaves per master ... well I don't really like this idea.I would prefer to
have 8 servers that all deal with data from the 5 servers we have right now
and then we could add new servers when we need. I looked at NFS but that does
not seem to be a good idea for SQL servers ? Can you confirm?</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hey,I do have a website that I would like to scale. [sent-1, score-0.367]
</p><p>2 Right now we have 10 servers but this does not scale well. [sent-2, score-0.452]
</p><p>3 I know how to deal with my apache web servers but have problems with sql servers. [sent-3, score-1.109]
</p><p>4 I would like to use the "scale out" system and add servers when we need. [sent-4, score-0.906]
</p><p>5 We have over 100Gb of data for mysql and we tried to have around 20G per server. [sent-5, score-0.44]
</p><p>6 It works well except that if a server goes down then 1/5 of the user can't access the website. [sent-6, score-0.605]
</p><p>7 We could use replication but we would need to at least double sql servers to replicate each server. [sent-7, score-1.587]
</p><p>8 And maybe in the future it's not gonna be enough we would need maybe 3 slaves per master . [sent-8, score-1.287]
</p><p>9 I would prefer to have 8 servers that all deal with data from the 5 servers we have right now and then we could add new servers when we need. [sent-12, score-2.17]
</p><p>10 I looked at NFS but that does not seem to be a good idea for SQL servers ? [sent-13, score-0.776]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('servers', 0.363), ('sql', 0.273), ('confirm', 0.258), ('gon', 0.252), ('na', 0.237), ('maybe', 0.228), ('deal', 0.213), ('would', 0.205), ('nfs', 0.191), ('looked', 0.175), ('prefer', 0.172), ('add', 0.17), ('slaves', 0.168), ('except', 0.164), ('replicate', 0.162), ('double', 0.158), ('tried', 0.146), ('right', 0.141), ('seem', 0.121), ('master', 0.115), ('apache', 0.108), ('per', 0.108), ('well', 0.108), ('least', 0.102), ('could', 0.101), ('replication', 0.093), ('goes', 0.092), ('scale', 0.089), ('future', 0.086), ('website', 0.082), ('like', 0.08), ('enough', 0.075), ('need', 0.074), ('works', 0.073), ('idea', 0.072), ('mysql', 0.069), ('around', 0.069), ('access', 0.068), ('ca', 0.066), ('problems', 0.06), ('user', 0.058), ('use', 0.056), ('know', 0.053), ('really', 0.052), ('data', 0.048), ('good', 0.045), ('server', 0.042), ('web', 0.039), ('system', 0.032), ('new', 0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="63-tfidf-1" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>Introduction: Hey,I do have a website that I would like to scale. Right now we have 10
servers but this does not scale well.I know how to deal with my apache web
servers but have problems with sql servers.I would like to use the "scale out"
system and add servers when we need. We have over 100Gb of data for mysql and
we tried to have around 20G per server. It works well except that if a server
goes down then 1/5 of the user can't access the website. We could use
replication but we would need to at least double sql servers to replicate each
server. And maybe in the future it's not gonna be enough we would need maybe 3
slaves per master ... well I don't really like this idea.I would prefer to
have 8 servers that all deal with data from the 5 servers we have right now
and then we could add new servers when we need. I looked at NFS but that does
not seem to be a good idea for SQL servers ? Can you confirm?</p><p>2 0.17531599 <a title="63-tfidf-2" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>Introduction: I've been trying to find a high availability file storage solution without
success. I tried GlusterFS which looks very promising but experienced problems
with stability and don't want something I can't easily control and rely on.
Other solutions are too complicated or have a SPOF.So I'm thinking of the
following setup:Two NFS servers, a primary and a warm backup. The primary
server will be rsynced with the warm backup every minute or two. I can do it
so frequently as a PHP script will know which directories have changed
recently from a database and only rsync those. Both servers will be NFS
mounted on a cluster of web servers as /mnt/nfs-primary (sym linked as
/home/websites) and /mnt/nfs-backup.I'll then use Ucarp
(http://www.ucarp.org/project/ucarp) to monitor both NFS servers availability
every couple of seconds and when one goes down, the Ucarp up script will be
set to change the symbolic link on all web servers for the /home/websites dir
from /mnt/nfs-primary to /mnt/nfs-backupThe</p><p>3 0.15290497 <a title="63-tfidf-3" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>Introduction: AmazinglyTechCrunchruns their website on one web server and one database
server, according to the fascinating surveyWhat the Webâ&euro;&trade;s most popular sites
are running onbyPingdom, a provider of uptime and response time
monitoring.Early we learnedPlentyOfFishcatches and releases many millions of
hits a day on just 1 web server and three database servers.Googleruns
aDalekarmy full of servers.YouSendIt, a company making it easy to send and
receive large files, has 24 web servers, 3 database servers, 170 storage
servers, and a few miscellaneous servers.Vimeo, a video sharing company, has
100 servers for streaming video, 4 web servers, and 2 database servers.Meebo,
an AJAX based instant messaging company, uses 40 servers to handle messaging,
over 40 web servers, and 10 servers for forums, jabber, testing, and so
on.FeedBurner, a news feed management company, has 70 web servers, 15 database
servers, and 10 miscellaneous servers. Now multiply FeedBurner's server count
by two because they maintain</p><p>4 0.14945945 <a title="63-tfidf-4" href="../high_scalability-2008/high_scalability-2008-04-29-High_performance_file_server.html">310 high scalability-2008-04-29-High performance file server</a></p>
<p>Introduction: What have bunch of applications which run on Debian servers, which processes
huge amount of data stored in a shared NFS drive.we have 3 applications
working as a pipeline, which process data stored in the NFS drive. The first
application processes the data and store the output in some folder in the NFS
drive, the second app in the pipeline process the data from the previous step
and so on.The data load to the pipeline is like 1 GBytes per minute. I think
the NFS drive is the bottleneck here.Would buying a specialized file server
improve the performance of data read write from the disk ?</p><p>5 0.14594287 <a title="63-tfidf-5" href="../high_scalability-2010/high_scalability-2010-08-23-6_Ways_to_Kill_Your_Servers_-__Learning_How_to_Scale_the_Hard_Way.html">884 high scalability-2010-08-23-6 Ways to Kill Your Servers -  Learning How to Scale the Hard Way</a></p>
<p>Introduction: This is a guest post by Steffen Konerow, author of theHigh Performance
Blog.Learning how to scale isn't easy without any prior experience. Nowadays
you have plenty of websites likehighscalability.comto get some inspiration,
but unfortunately there is no solution that fits all websites and needs. You
still have to think on your own to find a concept that works for your
requirements. So did I.A few years ago, my bosses came to me and said "We've
got a new project for you. It's the relaunch of a website that has already 1
million users a month. You have to build the website and make sure we'll be
able to grow afterwards". I was already an experienced coder, but not in these
dimensions, so I had to start learning how to scale - the hard way.The
software behind the website was a PHP content management system, based on
Smarty and MySQL. The first task was finding a proper hosting company who had
the experience and would also manage the servers for us. After some research
we found one, told t</p><p>6 0.14433417 <a title="63-tfidf-6" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>7 0.1437093 <a title="63-tfidf-7" href="../high_scalability-2008/high_scalability-2008-10-13-SQL_Server_2008_Database_Performance_and_Scalability.html">410 high scalability-2008-10-13-SQL Server 2008 Database Performance and Scalability</a></p>
<p>8 0.14084923 <a title="63-tfidf-8" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>9 0.13961914 <a title="63-tfidf-9" href="../high_scalability-2008/high_scalability-2008-04-10-Mysql_scalability_and_failover....html">302 high scalability-2008-04-10-Mysql scalability and failover...</a></p>
<p>10 0.13485512 <a title="63-tfidf-10" href="../high_scalability-2013/high_scalability-2013-11-19-We_Finally_Cracked_the_10K_Problem_-_This_Time_for_Managing_Servers_with_2000x_Servers_Managed_Per_Sysadmin.html">1550 high scalability-2013-11-19-We Finally Cracked the 10K Problem - This Time for Managing Servers with 2000x Servers Managed Per Sysadmin</a></p>
<p>11 0.13302837 <a title="63-tfidf-11" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<p>12 0.12967533 <a title="63-tfidf-12" href="../high_scalability-2011/high_scalability-2011-02-24-Strategy%3A_Eliminate_Unnecessary_SQL.html">995 high scalability-2011-02-24-Strategy: Eliminate Unnecessary SQL</a></p>
<p>13 0.12431508 <a title="63-tfidf-13" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>14 0.12047276 <a title="63-tfidf-14" href="../high_scalability-2008/high_scalability-2008-09-23-How_to_Scale_with_Ruby_on_Rails.html">389 high scalability-2008-09-23-How to Scale with Ruby on Rails</a></p>
<p>15 0.11254696 <a title="63-tfidf-15" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<p>16 0.11145343 <a title="63-tfidf-16" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>17 0.11127412 <a title="63-tfidf-17" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>18 0.11093984 <a title="63-tfidf-18" href="../high_scalability-2011/high_scalability-2011-07-25-Is_NoSQL_a_Premature_Optimization_that%27s_Worse_than_Death%3F_Or_the_Lady_Gaga_of_the_Database_World%3F.html">1085 high scalability-2011-07-25-Is NoSQL a Premature Optimization that's Worse than Death? Or the Lady Gaga of the Database World?</a></p>
<p>19 0.11010431 <a title="63-tfidf-19" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>20 0.10931008 <a title="63-tfidf-20" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, 0.066), (2, -0.003), (3, -0.143), (4, -0.005), (5, 0.015), (6, -0.027), (7, -0.075), (8, 0.029), (9, -0.032), (10, -0.035), (11, -0.034), (12, -0.014), (13, 0.02), (14, 0.09), (15, 0.033), (16, 0.017), (17, 0.051), (18, -0.117), (19, 0.045), (20, 0.035), (21, -0.029), (22, -0.038), (23, -0.065), (24, 0.09), (25, 0.029), (26, 0.083), (27, -0.032), (28, -0.026), (29, 0.002), (30, -0.016), (31, 0.011), (32, -0.048), (33, 0.012), (34, 0.005), (35, 0.008), (36, 0.039), (37, 0.009), (38, 0.03), (39, 0.05), (40, 0.075), (41, -0.073), (42, -0.07), (43, -0.073), (44, 0.011), (45, -0.02), (46, -0.023), (47, -0.02), (48, 0.061), (49, -0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97609252 <a title="63-lsi-1" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>Introduction: Hey,I do have a website that I would like to scale. Right now we have 10
servers but this does not scale well.I know how to deal with my apache web
servers but have problems with sql servers.I would like to use the "scale out"
system and add servers when we need. We have over 100Gb of data for mysql and
we tried to have around 20G per server. It works well except that if a server
goes down then 1/5 of the user can't access the website. We could use
replication but we would need to at least double sql servers to replicate each
server. And maybe in the future it's not gonna be enough we would need maybe 3
slaves per master ... well I don't really like this idea.I would prefer to
have 8 servers that all deal with data from the 5 servers we have right now
and then we could add new servers when we need. I looked at NFS but that does
not seem to be a good idea for SQL servers ? Can you confirm?</p><p>2 0.80377984 <a title="63-lsi-2" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>Introduction: I've been trying to find a high availability file storage solution without
success. I tried GlusterFS which looks very promising but experienced problems
with stability and don't want something I can't easily control and rely on.
Other solutions are too complicated or have a SPOF.So I'm thinking of the
following setup:Two NFS servers, a primary and a warm backup. The primary
server will be rsynced with the warm backup every minute or two. I can do it
so frequently as a PHP script will know which directories have changed
recently from a database and only rsync those. Both servers will be NFS
mounted on a cluster of web servers as /mnt/nfs-primary (sym linked as
/home/websites) and /mnt/nfs-backup.I'll then use Ucarp
(http://www.ucarp.org/project/ucarp) to monitor both NFS servers availability
every couple of seconds and when one goes down, the Ucarp up script will be
set to change the symbolic link on all web servers for the /home/websites dir
from /mnt/nfs-primary to /mnt/nfs-backupThe</p><p>3 0.78059214 <a title="63-lsi-3" href="../high_scalability-2010/high_scalability-2010-08-23-6_Ways_to_Kill_Your_Servers_-__Learning_How_to_Scale_the_Hard_Way.html">884 high scalability-2010-08-23-6 Ways to Kill Your Servers -  Learning How to Scale the Hard Way</a></p>
<p>Introduction: This is a guest post by Steffen Konerow, author of theHigh Performance
Blog.Learning how to scale isn't easy without any prior experience. Nowadays
you have plenty of websites likehighscalability.comto get some inspiration,
but unfortunately there is no solution that fits all websites and needs. You
still have to think on your own to find a concept that works for your
requirements. So did I.A few years ago, my bosses came to me and said "We've
got a new project for you. It's the relaunch of a website that has already 1
million users a month. You have to build the website and make sure we'll be
able to grow afterwards". I was already an experienced coder, but not in these
dimensions, so I had to start learning how to scale - the hard way.The
software behind the website was a PHP content management system, based on
Smarty and MySQL. The first task was finding a proper hosting company who had
the experience and would also manage the servers for us. After some research
we found one, told t</p><p>4 0.75301236 <a title="63-lsi-4" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>Introduction: HackerEarth is a coding skill practice and testing service that in a series of
well written articles describes the trials and tribulations of building their
site and how they overcame them: Scaling Python/Django application with Apache
and mod_wsgi,Programming challenges, uptime, and mistakes in 2013, Post-
mortem: The big outage on January 25, 2014,The Robust Realtime Server,100,000
strong - CodeFactory server,Scaling database with Django and
HAProxy,Continuous Deployment System,HackerEarth Technology Stack.What
characterizes these articles and makes them especially helpful is a drive for
improvement and an openness towards reporting what didn't work and how they
figured out what would work.As they say, mistakes happen when you are building
a complex product with a team of just 3-4 engineers, but investing in
infrastructure allowed them to take more breaks, roam the streets of Bangalore
while their servers are happily serving thousands of requests every minute,
while reaching a 50,000</p><p>5 0.75019658 <a title="63-lsi-5" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>Introduction: Our company offers a web service that is provided to users from several
different hosting centers across the globe.The content and functionality at
each of the servers is almost exactly the same, and we could have based them
all in a single location. However, we chose to distribute the servers
geographically to offer our users the best performance, regardless where they
might be.Up until now, the only content on the servers that has had to be
synchronized is the server software itself. The features and functionality of
our service are being updated regularly, so every week or two we push updates
out to all the servers at basically the same time. We use a relatively manual
approach to do the updating, but it works fine.Sometime soon, however, our
synchronization needs are going to get a bit more complex.In particular, we'll
soon start offering a feature at our site that will involve a database with
content that will change on an almost second-by-second basis, based on user
input and act</p><p>6 0.74941778 <a title="63-lsi-6" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>7 0.74664193 <a title="63-lsi-7" href="../high_scalability-2009/high_scalability-2009-08-05-Stack_Overflow_Architecture.html">671 high scalability-2009-08-05-Stack Overflow Architecture</a></p>
<p>8 0.73698986 <a title="63-lsi-8" href="../high_scalability-2009/high_scalability-2009-08-31-Scaling_MySQL_on_Amazon_Web_Services.html">690 high scalability-2009-08-31-Scaling MySQL on Amazon Web Services</a></p>
<p>9 0.7354396 <a title="63-lsi-9" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>10 0.73236686 <a title="63-lsi-10" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>11 0.72391164 <a title="63-lsi-11" href="../high_scalability-2007/high_scalability-2007-09-06-Scaling_IMAP_and_POP3.html">81 high scalability-2007-09-06-Scaling IMAP and POP3</a></p>
<p>12 0.72168934 <a title="63-lsi-12" href="../high_scalability-2007/high_scalability-2007-11-02-How_WordPress.com_Tracks_300_Servers_Handling_10_Million_Pageviews.html">140 high scalability-2007-11-02-How WordPress.com Tracks 300 Servers Handling 10 Million Pageviews</a></p>
<p>13 0.71661204 <a title="63-lsi-13" href="../high_scalability-2013/high_scalability-2013-11-19-We_Finally_Cracked_the_10K_Problem_-_This_Time_for_Managing_Servers_with_2000x_Servers_Managed_Per_Sysadmin.html">1550 high scalability-2013-11-19-We Finally Cracked the 10K Problem - This Time for Managing Servers with 2000x Servers Managed Per Sysadmin</a></p>
<p>14 0.71488625 <a title="63-lsi-14" href="../high_scalability-2007/high_scalability-2007-08-03-Scaling_IMAP_and_POP3.html">57 high scalability-2007-08-03-Scaling IMAP and POP3</a></p>
<p>15 0.71203905 <a title="63-lsi-15" href="../high_scalability-2008/high_scalability-2008-04-10-Mysql_scalability_and_failover....html">302 high scalability-2008-04-10-Mysql scalability and failover...</a></p>
<p>16 0.70158499 <a title="63-lsi-16" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>17 0.69600695 <a title="63-lsi-17" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>18 0.68780565 <a title="63-lsi-18" href="../high_scalability-2008/high_scalability-2008-10-13-SQL_Server_2008_Database_Performance_and_Scalability.html">410 high scalability-2008-10-13-SQL Server 2008 Database Performance and Scalability</a></p>
<p>19 0.6841048 <a title="63-lsi-19" href="../high_scalability-2014/high_scalability-2014-01-14-SharePoint_VPS_solution.html">1579 high scalability-2014-01-14-SharePoint VPS solution</a></p>
<p>20 0.67843139 <a title="63-lsi-20" href="../high_scalability-2008/high_scalability-2008-09-10-Shard_servers_--_go_big_or_small%3F.html">383 high scalability-2008-09-10-Shard servers -- go big or small?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.108), (2, 0.252), (10, 0.093), (51, 0.229), (79, 0.183)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9107706 <a title="63-lda-1" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to
Mike Swift, in Facebook gets ready for New Year's Eve, we get a little insight
as to their method for the madness, nothing really detailed, but still
interesting.Problem SetupFacebook expects tha one billion+ photos will be
shared on New Year's eve.Facebook's 800 million users are scattered around the
world. Three quarters live outside the US. Each user is linked to an average
of 130 friends.Photos and posts must appear in less than a second. Opening a
homepage requires executing requests on a 100 different servers, and those
requests have to be ranked, sorted, and privacy-checked, and then
rendered.Different events put different stresses on different parts of
Facebook. Photo and Video Uploads - Holidays require hundreds of terabytes of
capacity News Feed - News events like big sports events and the death of Steve
Jobs drive user status updatesCoping StrategiesTry to predictthe surge in
traffic. Run checkson h</p><p>2 0.89808071 <a title="63-lda-2" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>Introduction: Think of building websites as engineeringcomposite materials. A composite
material is when two or more materials are combined to create a third material
that does something useful that the components couldn't do on their own.
Composites like reinforced concrete have revolutionized design and
construction. When building websites we usually bring different component
materials together, like creating a composite, to get the features we need
rather than building a completely new thing from scratch that does everything
we want.This approach has been seen as a hack because it leads to inelegancies
like data duplication; great gobs of component glue; consistency issues; and
messy operations. But what if the the composite approach is really a strength,
not a hack, but a messy part of the world that needs to be embraced rather
than belittled?They key is tosee data as a material. Right now we are arguing
which is the best single material to build with. Is itNoSQL, relational,
massively parallel,</p><p>3 0.88994557 <a title="63-lda-3" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data
( arround 44 gigs right now ).For some time I have been using rsync to keep
all the content equal on all servers, but the amount of data has been growing,
and rsync takes a few too much time to "compare" all data from source to
destination, and create a lot of I/O.I have been taking a look at MogileFS, it
seems a good and reliable option, but as the fuse module is not finished, we
should have to rewrite all our apps, and its not an option atm.Any ideas?I
just want a "real time, non resource-hungry" solution alternative for rsync.
If I get more features on the way, then they are welcome :)Why I prefer to use
a Distributed File System instead of using NAS + NFS?- I need 2 NAS, if I dont
want a point of failure, and NAS hard is expensive.- Non-shared hardware, all
server has their own local disks.- As files are replicated, I can save a lot
of money, RAID is not a MUST.Thnx in advance for your help and sorry for</p><p>4 0.88009453 <a title="63-lda-4" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>Introduction: Our last article on Disqus: How Disqus Went Realtime With 165K Messages Per
Second And Less Than .2 Seconds Latency, was a little out of date, but the
folks at Disqus have been busy implementing, not talking, so we don't know a
lot about what they are doing now, but we do have a short update in C1MM and
NGINXby John Watson and an article Trying out this Go thing.So Disqus has
grown a bit:1.3 billion unique visitors10 billion page views500 million users
engaged in discussions3 million communities25 million commentsThey are still
all about realtime, but Go replaced Python in their Realtime system:Original
Realtime backend was written in a pretty lightweight Python + gevent.The
realtime service is a hybrid of CPU intensive tasks + lots of network IO.
Gevent was handling the network IO without an issue, but at higher contention,
the CPU was choking everything. Switching over to Go removed that contention,
which was the primary issue that was being seen.Still runs on 5 machines Nginx
machin</p><p>same-blog 5 0.87699056 <a title="63-lda-5" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>Introduction: Hey,I do have a website that I would like to scale. Right now we have 10
servers but this does not scale well.I know how to deal with my apache web
servers but have problems with sql servers.I would like to use the "scale out"
system and add servers when we need. We have over 100Gb of data for mysql and
we tried to have around 20G per server. It works well except that if a server
goes down then 1/5 of the user can't access the website. We could use
replication but we would need to at least double sql servers to replicate each
server. And maybe in the future it's not gonna be enough we would need maybe 3
slaves per master ... well I don't really like this idea.I would prefer to
have 8 servers that all deal with data from the 5 servers we have right now
and then we could add new servers when we need. I looked at NFS but that does
not seem to be a good idea for SQL servers ? Can you confirm?</p><p>6 0.8671993 <a title="63-lda-6" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>7 0.8646102 <a title="63-lda-7" href="../high_scalability-2011/high_scalability-2011-10-28-Stuff_The_Internet_Says_On_Scalability_For_October_28%2C_2011.html">1134 high scalability-2011-10-28-Stuff The Internet Says On Scalability For October 28, 2011</a></p>
<p>8 0.85235298 <a title="63-lda-8" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>9 0.84998173 <a title="63-lda-9" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>10 0.84986937 <a title="63-lda-10" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>11 0.84053391 <a title="63-lda-11" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>12 0.83210057 <a title="63-lda-12" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>13 0.82549614 <a title="63-lda-13" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>14 0.81419384 <a title="63-lda-14" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>15 0.80863279 <a title="63-lda-15" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>16 0.80606842 <a title="63-lda-16" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>17 0.803765 <a title="63-lda-17" href="../high_scalability-2011/high_scalability-2011-11-16-Google%2B_Infrastructure_Update_-_the_JavaScript_Story.html">1143 high scalability-2011-11-16-Google+ Infrastructure Update - the JavaScript Story</a></p>
<p>18 0.80365938 <a title="63-lda-18" href="../high_scalability-2013/high_scalability-2013-11-08-Stuff_The_Internet_Says_On_Scalability_For_November_8th%2C_2013.html">1545 high scalability-2013-11-08-Stuff The Internet Says On Scalability For November 8th, 2013</a></p>
<p>19 0.80125958 <a title="63-lda-19" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>20 0.80101144 <a title="63-lda-20" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
