<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-112" href="#">iccv2013-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</h1>
<br/><p>Source: <a title="iccv-2013-112-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Turetken_Detecting_Irregular_Curvilinear_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Engin Türetken, Carlos Becker, Przemyslaw Glowacki, Fethallah Benmansour, Pascal Fua</p><p>Abstract: We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.</p><p>Reference: <a title="iccv-2013-112-reference" href="../iccv2013_reference/iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch  Abstract We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. [sent-7, score-0.605]
</p><p>2 In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. [sent-8, score-0.087]
</p><p>3 This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. [sent-9, score-0.744]
</p><p>4 We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures. [sent-11, score-0.172]
</p><p>5 Introduction Enhancing curvilinear structures is a crucial preprocessing step in a broad array of biomedical delineation tasks, ranging from finding blood vessels [4, 15, 3] to dendritic  arbors [23, 24, 21]. [sent-13, score-0.595]
</p><p>6 The most popular techniques rely on filters that are crafted to respond to locally linear features [ 13, 7, 20, 18], optimized for specific profiles [ 1 1, 22], or learnt [ 19, 10]. [sent-14, score-0.088]
</p><p>7 They return a measure that, ideally, should be maximal on the centerline of tubular structures. [sent-15, score-0.401]
</p><p>8 In recent years, Optimally Oriented Flux (OOF) [ 13] has emerged as one of the best such tubularity measures, in part because it can easily be normalized for scale and is relatively insensitive to the presence of adjacent structures. [sent-16, score-0.501]
</p><p>9 Thus, it provides a higher discriminative power in detecting curvilinear structures in scale-space than Hessian based features. [sent-17, score-0.476]
</p><p>10 Each one of the four examples shown here includes at the top a maximal or minimal intensity projection and below two cross sections of tubular structures. [sent-22, score-0.225]
</p><p>11 (a) Confocal stack of blood vessels with non-uniform staining. [sent-24, score-0.125]
</p><p>12 (c,d) Brightfield image stacks with irregular staining and point-spread function blur. [sent-26, score-0.294]
</p><p>13 However, performance is adversely affected when cross sections become very irregular and deviate strongly from  being circular. [sent-28, score-0.196]
</p><p>14 1, this is an issue because curvilinear structures in biological imagery often appear as jagged filaments whose cross sections are extremely irregular, due among others to imaging noise, artifacts, nonuniform staining, occlusion and point-spread function. [sent-30, score-0.656]
</p><p>15 In this paper, we introduce a new tubularity measure that we refer to as Multi-Directional Oriented Flux (MDOF), 11555533  designed to handle this kind of irregularities. [sent-31, score-0.478]
</p><p>16 This is achieved by maximizing the image gradient flux along multiple directions and radii, rather than only along two orthogonal directions with the same radii as in the OOF framework. [sent-33, score-0.668]
</p><p>17 Our contribution is both this new MDOF measure for detecting curvilinear structures in gray scale and color images and an approach to computing it efficiently, which makes it practical to solve real-world problems. [sent-36, score-0.567]
</p><p>18 Related Work  Hand-designed curvilinear structure detectors fall into two main categories: the very popular Hessian-based detectors and model-based optimal filters. [sent-39, score-0.391]
</p><p>19 The main assumption is that the desired structure is characterized by a local principal direction and a cross-sectional plane with Gaussian intensity profile. [sent-46, score-0.128]
</p><p>20 Hessian-based approaches [7, 20, 6, 18] consist in computing the eigenvalues of the Hessian and combining them to obtain a tubularity measure that quantifies the likelihood that a specific pixel lies on a ceterline of a filament-like structure. [sent-48, score-0.53]
</p><p>21 If the cross-section does not present a circular profile the assumptions justifying using the Hessian matrix no longer hold, which often leads to false negative responses. [sent-51, score-0.087]
</p><p>22 The lack of robustness in the presence of an irregular profile is a gen-  eral drawback common to all existing hand designed detectors. [sent-52, score-0.14]
</p><p>23 Therefore the response of such filters at a thick branch may be biased by the presence of adjacent structures in the immediate neighborhood. [sent-55, score-0.194]
</p><p>24 Last but not least, normalization over scales of Hessian-based tubularity measures is not straightforward. [sent-57, score-0.452]
</p><p>25 Unfortunately, curvilinear structures in real data do not necessarily conform to such an appearance model. [sent-59, score-0.451]
</p><p>26 A related approach, the Optimally Oriented Flux filtering [13], consists of convolving the second order derivatives of the image with the indicator of a sphere, which is a steerable filter  designed for detecting curvilinear structures. [sent-67, score-0.536]
</p><p>27 The OOF computation is confined to a local spherical region which results in less sensitivity to the presence of adjacent structures as compared to Hessian-based methods that involve convolution with the second derivative of Gaussian. [sent-69, score-0.111]
</p><p>28 This approach has been extended to take into account antisymmetry of the projected gradients at the boundaries of tubular structures [14], as explained in detail in Section 3. [sent-70, score-0.399]
</p><p>29 All these approaches assume an ideal circular appearence of the curvilinear structure cross-sections, which is often not true in biomedical images, as shown in Fig. [sent-72, score-0.473]
</p><p>30 Approach Our objective is to devise an algorithm able both to reliably detect the centerlines of irregularly shaped curvilinear structures and to estimate their radius. [sent-77, score-0.551]
</p><p>31 To this end, we assign to each spatial location and each radius value a tubularity score and then extract the centerline points and their radii as local maxima in scale-space. [sent-78, score-1.164]
</p><p>32 Formally, we compute a tubularity value f(x, r) for each 3D image location x and radius r. [sent-79, score-0.758]
</p><p>33 It quantifies the likelihood of x being on  the centerline of a curvilinear structure of radius r within a range [rmin rmax] . [sent-80, score-0.916]
</p><p>34 In practice, we sample the radius range 11555544  Figure 2. [sent-81, score-0.26]
</p><p>35 (b) Maximum intensity projection of the 4-D scale-space tubularity volumes obtained with our approach along the scale dimension. [sent-86, score-0.54]
</p><p>36 (c) Maximum intensity projection of the non-maxima suppressed tubularity volumes obtained by running the NMST algorithm of Section 4. [sent-88, score-0.548]
</p><p>37 We therefore produce 4D tubularity images, such as those depicted by Fig. [sent-94, score-0.452]
</p><p>38 Without loss of generality, from now on, we will assume that curvilinear structures are brighter than the background. [sent-96, score-0.451]
</p><p>39 Furthermore, we will use the terms radius and scale interchangeably. [sent-98, score-0.26]
</p><p>40 Our tubularity measure extends the well-known Optimally Oriented Flux (OOF) [13] filter, which we first review briefly for completeness. [sent-99, score-0.478]
</p><p>41 We then introduce our approach to handling the irregularity of many curvilinear structures, which deviates from the idealized models used by the OOF techniques. [sent-100, score-0.404]
</p><p>42 Optimally Oriented Flux As discussed in Section 2, many existing approaches to enhancing curvilinear structures [16, 7, 20] rely on a Hessian based measure obtained by convolving the image with second order Gaussian derivatives. [sent-104, score-0.539]
</p><p>43 As a result, they take into account image intensities in the vicinity of the structures, which can adversely affect their accuracy in the presence of adjacent structures. [sent-106, score-0.087]
</p><p>44 Furthermore, in order to estimate the structure radius from the Gaussian standard deviation, they use an idealized intensity profile model, which is not trivial to obtain and is not applicable to all imaging modalities. [sent-107, score-0.417]
</p><p>45 The OOF filter addresses these issues by considering  intensity values only within a spherical volume of certain diameter, which provides a good estimate of the structure width [13]. [sent-108, score-0.1]
</p><p>46 The filter is computed by convolving the second derivatives of the image with the indicator function of the sphere. [sent-109, score-0.114]
</p><p>47 More formally, its value f(x, p, r) for image location x, radius r, and orientation p is obtained by integrating the projected image gradients in the p direction over a sphere ∂Sr of radius r centered at x. [sent-110, score-0.646]
</p><p>48 The smaller the value of f(x, p, r), the more likely it is that x is the center of a tube with radius r and orientation p. [sent-113, score-0.26]
</p><p>49 It can be shown that f(x, p, r) can be rewritten as the quadratic form pTQx,rp, where Qx,r is known as the oriented flux matrix [13]. [sent-115, score-0.433]
</p><p>50 Using the divergence theorem, its entries can be expressed as Qix,,jr =  4π1r2(∂i,jGσ0(x) ∗ 1r∗ I(x)),  (2)  where 1r is the indicator of the sphere of radius r. [sent-116, score-0.303]
</p><p>51 Their associated eigenvectors ex1,r and ex2,r provide the optimal directions for the above maximization. [sent-118, score-0.079]
</p><p>52 Due to its intrinsic symmetry, the OOF filter yields high responses along centerlines of curvilinear structures at their associated scales. [sent-119, score-0.676]
</p><p>53 To prevent this, a gradient antisymmetry function g(x, p, r) was introduced in [14]: g(x,p,r) =  4π1r2? [sent-121, score-0.227]
</p><p>54 Using this principle, the OOF tubularity measure can be refined to take into account structure symmetricity as follows [14]:  fOF(x,r)−? [sent-124, score-0.502]
</p><p>55 The two antisymmetry terms above act complementary to the OOF filter as they annihilate the OOF response away from the structure centerlines. [sent-127, score-0.31]
</p><p>56 However, since both OOF and OFA are evaluated only at two directions and a single radius value, the measure of Eq. [sent-128, score-0.336]
</p><p>57 6 tends to favor circular and symmetric cross sections. [sent-129, score-0.105]
</p><p>58 MDOF: Multi-Directional Oriented Flux Curvilinear structures such as the ones shown in Fig. [sent-132, score-0.084]
</p><p>59 1, often appear as irregular filaments in medical imagery due to reasons such as imaging noise, non-uniform staining and point-spread function. [sent-133, score-0.293]
</p><p>60 We address this issue by finding multiple directions and radii of maximal response of the joint 11555566  oriented flux and antisymmetry operators. [sent-134, score-0.855]
</p><p>61 More specifically, given a set ofpredetermined radius levels Sr, we compute a number of direction and radius pairs (pi ∈ R3, ri ∈ Sr) that maximize  ? [sent-135, score-0.584]
</p><p>62 tnri3ecx-s, into account non-circular cross-sections by allowing multiple directions pi and radii ri at every possible location x. [sent-147, score-0.259]
</p><p>63 Note that, in contrast to [14] which first finds the two dominant eigenvalues of Qx,r and then plugs their eigenvectors into the antisymmetry term, we jointly optimize for the oriented flux and the antisym-  metry terms. [sent-152, score-0.694]
</p><p>64 To this end, we employ a greedy approach, which first finds the dominant direction p1∗ and the associated radius r1∗ that maximizes fMS (x) for d = 1and then (p∗2 , r2∗) that maximizes the same function subject to the constraint that p2∗ is perpendicular to p1∗ . [sent-155, score-0.32]
</p><p>65 The first step involves computing p1∗ as the eigenvector associated with the smallest eigenvalue among those of Fx,ri , for all ri ∈ Sr. [sent-156, score-0.104]
</p><p>66 The − wvehcetorer eth∗e can rbixe computed as athse P eigenvector associated with the smallest eigenvalue among those of P Fx,ri P, for all ri ∈ Sr. [sent-158, score-0.104]
</p><p>67 We then find the additional d −2 directions by sampling themW ea t equiangular aidntdeirtvioanlsa lt od span π e ractdiioannss biny tsahem plane defined by p1∗ and p2∗. [sent-160, score-0.082]
</p><p>68 The corresponding radius values are found by maximizing the same cost function of Eq. [sent-161, score-0.288]
</p><p>69 This produces a 3-D score image, which we then extend to 4-D by adding the strongest response of piTFx,ri pi for each radius level. [sent-163, score-0.373]
</p><p>70 We take the final score for radius r at  location x to be  fMDOF(x,r) = −λx1,r+d1  {pmi,rai}xid=1fMS(x),  (9)  where λx1,r is the smallest eigenvalue of Fx,r. [sent-164, score-0.383]
</p><p>71 At each radius level, we use only a single eigenvalue, which gives the strongest response, because, in most datasets, the gradient information is strong and provides a reliable estimate only along one direction in the irregular cross-sectional profiles. [sent-165, score-0.497]
</p><p>72 9 measures the likelihood of x being on the centerline of a curvilinear structure, while the first one measures the contribution of the radius level r. [sent-170, score-0.868]
</p><p>73 As a result, at each image point x, the radius level r that yields the strongest gradient flux on the associated sphere Sr will be assigned the highest score. [sent-171, score-0.744]
</p><p>74 As we will show in Section 4, this measure is more effective in suppressing spurious responses away from the centerlines when the structures are irregular. [sent-172, score-0.239]
</p><p>75 Handling Color Images With the advent of new imaging techniques, color information has gained increasing importance in the analysis of complex curvilinear structures. [sent-175, score-0.431]
</p><p>76 In fact, the color constancy property also widely holds for other curvilinear structures such as road networks. [sent-178, score-0.487]
</p><p>77 We use this property to compute, for each image location x, a similarity image, which signifies how likely both x and another location y in its vicinity belong to the same curvilinear structure based on their color. [sent-179, score-0.51]
</p><p>78 We then use the resulting similarity image to compute the tubularity measure of Eq. [sent-180, score-0.478]
</p><p>79 This is done because the aforementioned color imaging techniques aim to maximize the color contrast among curvilinear structures for visual inspection by human experts. [sent-183, score-0.551]
</p><p>80 11requires sequentially computing only |C| similarity images, which serve as input to the tubularity computation dmeascgreisb,e wdh iinc hth see previous uste tcoti tohne. [sent-194, score-0.452]
</p><p>81 In our experiments, the radius levels are measured in the units of image spacing along the XY axes. [sent-202, score-0.329]
</p><p>82 •  Retina Two confocal micrographs of direction selectRiveet rneati nTawl ganglion lc mellisc aongrda loopy v daisreccutlaiotunre se lneect-works. [sent-203, score-0.164]
</p><p>83 In our computations, we sampled 9 radius val-  •  •  •  ues ranging from 3 to 16. [sent-204, score-0.302]
</p><p>84 Brainbow Four micrographs of mice primary visual cBorrateixnb acquired using rthapeh s br oafin mbiocwe staining tiesucha-l nique [17]. [sent-205, score-0.16]
</p><p>85 Brightfield Four brightfield micrographs of biocytinsBtraiingehdtf riealtd d bFr aoinusr. [sent-207, score-0.248]
</p><p>86 VC6 Four brightfield micrographs of biocytin-labeled cVaCt primary vrigisuhatfil ecldort mexic layer p6h stak oefn b iforcomyti nth-lea bDeIlAedDEM challenge data [2]. [sent-209, score-0.248]
</p><p>87 2 for all the four datasets: the number of direction and radius pairs d = 10, color similarity sigma σ = 50 in the the CIELAB space and the number of color clusters |C| = 50. [sent-212, score-0.369]
</p><p>88 To this end we begin by computing the 4-D tubularity image stacks, where the fourth dimension represents radius, using all five methods. [sent-216, score-0.452]
</p><p>89 These stacks can be turned into ordinary 3-D stacks by choosing the radius that yields the highest tubularity value at each location. [sent-217, score-0.971]
</p><p>90 All five methods compute a matrix whose eigenvectors can be used to compute a local orientation, which we use in conjunction with the radii estimates to implement a Canny-style approach to non-maxima suppression. [sent-220, score-0.133]
</p><p>91 More specifically, we suppress voxels that are not local maxima in the plane that is perpendicular to the local orientation and within the circular neighborhood defined by their radius. [sent-221, score-0.247]
</p><p>92 Unfortunately, because the estimated orientations are not always accurate, this often removes some of the legitimate centerline points and produces gaps along the structures, which would skew our accuracy estimates. [sent-222, score-0.271]
</p><p>93 We solve this problem by first finding the Minimum Spanning Tree (MST) that connects all the voxels in the original 3-D tubularity stack. [sent-223, score-0.529]
</p><p>94 We use 26-connectivity and take the weight of each edge to be the negative exponential of the mean tubularity score of the voxels it connects. [sent-224, score-0.529]
</p><p>95 We then find connected components in the non-maxima suppressed stack and identify the MST paths that link pairs of these. [sent-225, score-0.089]
</p><p>96 For each component, we retain only the two MST paths that have the highest average tubularity scores. [sent-226, score-0.452]
</p><p>97 Note that locally maximum voxels in the NMST are still attached tubularity values. [sent-234, score-0.529]
</p><p>98 To also estimate the quality of our radius estimates, we therefore threshold it at different values. [sent-235, score-0.26]
</p><p>99 Spheres are centered at NMST image voxels that above an established threshold, generating the segmentation on the right. [sent-310, score-0.077]
</p><p>100 each, we then construct a tubular structure by using our radius estimates to place spheres at each non-empty voxel of the thresholded NMST as shown in Fig. [sent-311, score-0.463]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tubularity', 0.452), ('curvilinear', 0.367), ('flux', 0.353), ('radius', 0.26), ('centerline', 0.241), ('antisymmetry', 0.204), ('oof', 0.191), ('brightfield', 0.158), ('nmst', 0.158), ('stacks', 0.117), ('brainbow', 0.113), ('tubular', 0.111), ('irregular', 0.107), ('radii', 0.104), ('centerlines', 0.1), ('micrographs', 0.09), ('structures', 0.084), ('oriented', 0.08), ('voxels', 0.077), ('staining', 0.07), ('sr', 0.069), ('mdof', 0.068), ('ofa', 0.064), ('maxima', 0.061), ('hessian', 0.058), ('jx', 0.056), ('jaccard', 0.056), ('circular', 0.054), ('fof', 0.052), ('stack', 0.051), ('optimally', 0.051), ('directions', 0.05), ('retina', 0.048), ('eigenvalue', 0.047), ('mst', 0.046), ('profiles', 0.046), ('location', 0.046), ('engin', 0.045), ('filaments', 0.045), ('fms', 0.045), ('spheres', 0.043), ('sphere', 0.043), ('imagery', 0.043), ('filters', 0.042), ('ranging', 0.042), ('response', 0.041), ('filter', 0.041), ('strongest', 0.04), ('convolving', 0.039), ('spacing', 0.039), ('suppressed', 0.038), ('idealized', 0.037), ('blood', 0.037), ('confocal', 0.037), ('vessels', 0.037), ('direction', 0.037), ('color', 0.036), ('intensity', 0.035), ('cielab', 0.035), ('derivatives', 0.034), ('adversely', 0.033), ('epfl', 0.033), ('neuron', 0.033), ('biological', 0.033), ('profile', 0.033), ('pi', 0.032), ('plane', 0.032), ('steerable', 0.03), ('along', 0.03), ('smallest', 0.03), ('responses', 0.029), ('gray', 0.029), ('cross', 0.029), ('eigenvectors', 0.029), ('eigenvalues', 0.028), ('maximizing', 0.028), ('biomedical', 0.028), ('imaging', 0.028), ('ri', 0.027), ('adjacent', 0.027), ('sections', 0.027), ('vicinity', 0.027), ('measure', 0.026), ('voxel', 0.025), ('detecting', 0.025), ('yields', 0.025), ('gaussian', 0.024), ('tolerance', 0.024), ('quantifies', 0.024), ('structure', 0.024), ('fua', 0.023), ('volumes', 0.023), ('maximal', 0.023), ('pr', 0.023), ('gradient', 0.023), ('perpendicular', 0.023), ('enhancing', 0.023), ('insensitive', 0.022), ('ch', 0.022), ('favor', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="112-tfidf-1" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>Author: Engin Türetken, Carlos Becker, Przemyslaw Glowacki, Fethallah Benmansour, Pascal Fua</p><p>Abstract: We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.</p><p>2 0.080420621 <a title="112-tfidf-2" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>Author: Byung-Soo Kim, Pushmeet Kohli, Silvio Savarese</p><p>Abstract: Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of par- tial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images.</p><p>3 0.06008146 <a title="112-tfidf-3" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>4 0.050505862 <a title="112-tfidf-4" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>Author: Jan Stühmer, Peter Schröder, Daniel Cremers</p><p>Abstract: We propose a novel method to include a connectivity prior into image segmentation that is based on a binary labeling of a directed graph, in this case a geodesic shortest path tree. Specifically we make two contributions: First, we construct a geodesic shortest path tree with a distance measure that is related to the image data and the bending energy of each path in the tree. Second, we include a connectivity prior in our segmentation model, that allows to segment not only a single elongated structure, but instead a whole connected branching tree. Because both our segmentation model and the connectivity constraint are convex, a global optimal solution can be found. To this end, we generalize a recent primal-dual algorithm for continuous convex optimization to an arbitrary graph structure. To validate our method we present results on data from medical imaging in angiography and retinal blood vessel segmentation.</p><p>5 0.050190613 <a title="112-tfidf-5" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>Author: Manjunath Narayana, Allen Hanson, Erik Learned-Miller</p><p>Abstract: In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths.</p><p>6 0.049601328 <a title="112-tfidf-6" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>7 0.047496598 <a title="112-tfidf-7" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>8 0.045443092 <a title="112-tfidf-8" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<p>9 0.044166617 <a title="112-tfidf-9" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>10 0.043923482 <a title="112-tfidf-10" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>11 0.043147311 <a title="112-tfidf-11" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>12 0.042917203 <a title="112-tfidf-12" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>13 0.042453192 <a title="112-tfidf-13" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>14 0.040933765 <a title="112-tfidf-14" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>15 0.037456565 <a title="112-tfidf-15" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>16 0.036434963 <a title="112-tfidf-16" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>17 0.03622257 <a title="112-tfidf-17" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>18 0.036178507 <a title="112-tfidf-18" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>19 0.035973232 <a title="112-tfidf-19" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>20 0.035176486 <a title="112-tfidf-20" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, -0.038), (2, -0.004), (3, -0.014), (4, 0.002), (5, 0.023), (6, -0.012), (7, -0.017), (8, -0.006), (9, -0.028), (10, 0.002), (11, -0.003), (12, -0.012), (13, -0.013), (14, 0.015), (15, 0.024), (16, -0.013), (17, 0.003), (18, -0.022), (19, 0.004), (20, -0.001), (21, 0.033), (22, 0.02), (23, 0.003), (24, -0.026), (25, 0.044), (26, 0.007), (27, -0.005), (28, -0.042), (29, 0.025), (30, -0.01), (31, 0.002), (32, 0.047), (33, 0.034), (34, -0.052), (35, 0.031), (36, -0.02), (37, -0.038), (38, -0.014), (39, -0.004), (40, 0.007), (41, -0.015), (42, -0.047), (43, 0.001), (44, 0.025), (45, -0.009), (46, -0.092), (47, -0.014), (48, -0.092), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87408996 <a title="112-lsi-1" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>Author: Engin Türetken, Carlos Becker, Przemyslaw Glowacki, Fethallah Benmansour, Pascal Fua</p><p>Abstract: We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.</p><p>2 0.64264923 <a title="112-lsi-2" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>Author: Dengxin Dai, Hayko Riemenschneider, Gerhard Schmitt, Luc Van_Gool</p><p>Abstract: There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets in particular for the different building styles they contain demonstrate the potential of the method. – –</p><p>3 0.63636446 <a title="112-lsi-3" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>Author: Jonathan T. Barron, Mark D. Biggin, Pablo Arbeláez, David W. Knowles, Soile V.E. Keranen, Jitendra Malik</p><p>Abstract: We present an algorithm for the per-voxel semantic segmentation of a three-dimensional volume. At the core of our algorithm is a novel “pyramid context” feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient. This feature not only allows for efficient semantic segmentation but enables other aspects of our algorithm, such as novel learned features and a stacked architecture that can reason about self-consistency. We demonstrate our technique on 3Dfluorescence microscopy data ofDrosophila embryosfor which we are able to produce extremely accurate semantic segmentations in a matter of minutes, and for which other algorithms fail due to the size and high-dimensionality of the data, or due to the difficulty of the task.</p><p>4 0.62428486 <a title="112-lsi-4" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>Author: Kim Steenstrup Pedersen, Kristoffer Stensbo-Smidt, Andrew Zirm, Christian Igel</p><p>Abstract: A texture descriptor based on the shape index and the accompanying curvedness measure is proposed, and it is evaluated for the automated analysis of astronomical image data. A representative sample of images of low-redshift galaxies from the Sloan Digital Sky Survey (SDSS) serves as a testbed. The goal of applying texture descriptors to these data is to extract novel information about galaxies; information which is often lost in more traditional analysis. In this study, we build a regression model for predicting a spectroscopic quantity, the specific star-formation rate (sSFR). As texture features we consider multi-scale gradient orientation histograms as well as multi-scale shape index histograms, which lead to a new descriptor. Our results show that we can successfully predict spectroscopic quantities from the texture in optical multi-band images. We successfully recover the observed bi-modal distribution of galaxies into quiescent and star-forming. The state-ofthe-art for predicting the sSFR is a color-based physical model. We significantly improve its accuracy by augmenting the model with texture information. This study is thefirst step towards enabling the quantification of physical galaxy properties from imaging data alone.</p><p>5 0.56787664 <a title="112-lsi-5" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>Author: Chih-Yuan Yang, Ming-Hsuan Yang</p><p>Abstract: The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functionsfor super-resolution, and efficiency ofgenerating highresolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that theproposed algorithmperforms efficiently and effectively over state-of-the-art methods.</p><p>6 0.56238943 <a title="112-lsi-6" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>7 0.5542897 <a title="112-lsi-7" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>8 0.5496909 <a title="112-lsi-8" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>9 0.54357445 <a title="112-lsi-9" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>10 0.521447 <a title="112-lsi-10" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>11 0.50906485 <a title="112-lsi-11" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>12 0.50435728 <a title="112-lsi-12" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>13 0.49167129 <a title="112-lsi-13" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>14 0.49106422 <a title="112-lsi-14" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>15 0.4861947 <a title="112-lsi-15" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>16 0.48393643 <a title="112-lsi-16" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>17 0.48272255 <a title="112-lsi-17" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>18 0.47204143 <a title="112-lsi-18" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>19 0.46509865 <a title="112-lsi-19" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>20 0.46397266 <a title="112-lsi-20" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.044), (7, 0.038), (12, 0.013), (13, 0.018), (26, 0.055), (31, 0.036), (42, 0.077), (46, 0.293), (64, 0.039), (73, 0.05), (89, 0.18), (95, 0.015), (98, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75311041 <a title="112-lda-1" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>Author: Engin Türetken, Carlos Becker, Przemyslaw Glowacki, Fethallah Benmansour, Pascal Fua</p><p>Abstract: We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.</p><p>2 0.68669027 <a title="112-lda-2" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>3 0.59529907 <a title="112-lda-3" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>Author: Naiyan Wang, Dit-Yan Yeung</p><p>Abstract: Matrix factorization is a fundamental problem that is often encountered in many computer vision and machine learning tasks. In recent years, enhancing the robustness of matrix factorization methods has attracted much attention in the research community. To benefit from the strengths of full Bayesian treatment over point estimation, we propose here a full Bayesian approach to robust matrix factorization. For the generative process, the model parameters have conjugate priors and the likelihood (or noise model) takes the form of a Laplace mixture. For Bayesian inference, we devise an efficient sampling algorithm by exploiting a hierarchical view of the Laplace distribution. Besides the basic model, we also propose an extension which assumes that the outliers exhibit spatial or temporal proximity as encountered in many computer vision applications. The proposed methods give competitive experimental results when compared with several state-of-the-art methods on some benchmark image and video processing tasks.</p><p>4 0.59385085 <a title="112-lda-4" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>Author: Yuandong Tian, Srinivasa G. Narasimhan</p><p>Abstract: Real-world surfaces such as clothing, water and human body deform in complex ways. The image distortions observed are high-dimensional and non-linear, making it hard to estimate these deformations accurately. The recent datadriven descent approach [17] applies Nearest Neighbor estimators iteratively on a particular distribution of training samples to obtain a globally optimal and dense deformation field between a template and a distorted image. In this work, we develop a hierarchical structure for the Nearest Neighbor estimators, each of which can have only a local image support. We demonstrate in both theory and practice that this algorithm has several advantages over the nonhierarchical version: it guarantees global optimality with significantly fewer training samples, is several orders faster, provides a metric to decide whether a given image is “hard” (or “easy ”) requiring more (or less) samples, and can handle more complex scenes that include both global motion and local deformation. The proposed algorithm successfully tracks a broad range of non-rigid scenes including water, clothing, and medical images, and compares favorably against several other deformation estimation and tracking approaches that do not provide optimality guarantees.</p><p>5 0.59208834 <a title="112-lda-5" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>Author: Hongyi Zhang, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper, we are interested in understanding the semantics of outdoor scenes in the context of autonomous driving. Towards this goal, we propose a generative model of 3D urban scenes which is able to reason not only about the geometry and objects present in the scene, but also about the high-level semantics in the form of traffic patterns. We found that a small number of patterns is sufficient to model the vast majority of traffic scenes and show how these patterns can be learned. As evidenced by our experiments, this high-level reasoning significantly improves the overall scene estimation as well as the vehicle-to-lane association when compared to state-of-the-art approaches [10].</p><p>6 0.59191942 <a title="112-lda-6" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>7 0.59168762 <a title="112-lda-7" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>8 0.59107757 <a title="112-lda-8" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>9 0.59102964 <a title="112-lda-9" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>10 0.59049648 <a title="112-lda-10" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>11 0.58985639 <a title="112-lda-11" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>12 0.58960748 <a title="112-lda-12" href="./iccv-2013-Shortest_Paths_with_Curvature_and_Torsion.html">389 iccv-2013-Shortest Paths with Curvature and Torsion</a></p>
<p>13 0.58950305 <a title="112-lda-13" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>14 0.58922046 <a title="112-lda-14" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>15 0.58885479 <a title="112-lda-15" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>16 0.58882636 <a title="112-lda-16" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>17 0.58875209 <a title="112-lda-17" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>18 0.58825481 <a title="112-lda-18" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>19 0.58819026 <a title="112-lda-19" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>20 0.58817071 <a title="112-lda-20" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
