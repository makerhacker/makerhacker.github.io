<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1622" href="#">high_scalability-2014-1622</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1622-html" href="http://highscalability.com//blog/2014/3/31/how-whatsapp-grew-to-nearly-500-million-users-11000-cores-an.html">html</a></p><p>Introduction: When we lastvisited WhatsAppthey'd just been acquired by Facebook for $19
billion. We learned about their early architecture, which centered around a
maniacal focus on optimizing Erlang into handling 2 million connections a
server, working on All The Phones, and making users happy through
simplicity.Two years later traffic has grown 10x. How did WhatsApp make that
jump to the next level of scalability?Rick Reedtells us in a talk he gave at
the Erlang Factory:That's 'Billion' with a 'B': Scaling to the next level at
WhatsApp(slides), which revealed some eye popping WhatsApp stats:What has
hundreds of nodes, thousands of cores, hundreds of terabytes of RAM, and hopes
to serve the billions of smartphones that will soon be a reality around the
globe? The Erlang/FreeBSD-based server infrastructure at WhatsApp. We've faced
many challenges in meeting the ever-growing demand for our messaging services,
but as we continue to push the envelope on size (>8000 cores) and speed (>70M
Erlang message</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 19B messages in & 40B out per day600M pics, 200M voice, 100M videos147M peak concurrent connections - phones connected to the systems230K peak logins/sec - phones connecting and disconnecting342K peak msgs in/sec, 712K out~10 team member works on Erlang and they handle both development and ops. [sent-33, score-0.861]
</p><p>2 146Gb/s out (Christmas Eve), quite a bit of bandwidth going out to phones360M videos downloaded (Christmas Even)2B pics downloaded (46k/s) (New Years Eve)1 pic downloaded 32M times (New Years Eve)StackErlang R16B01 (plus their own patches)FreeBSD 9. [sent-35, score-0.278]
</p><p>3 Run at 1 million connections per server instead of the two million connections per server they did two years ago, generally because the servers are a lot busier:With more users they want to run with more head room on each server to soak up peak loads. [sent-56, score-0.742]
</p><p>4 If a node or network connecting nodes runs into trouble, it can block work in an application. [sent-71, score-0.294]
</p><p>5 So when sending to different nodes the messages are given to differentprocs(lightweight concurrency in Erlang) so only messages destined for a problem node are backed up. [sent-72, score-0.685]
</p><p>6 This allows messages to healthy nodes to flow freely. [sent-73, score-0.307]
</p><p>7 Patched mnesia to do this well at async_dirty replication time. [sent-75, score-0.435]
</p><p>8 Example: two main clusters in two datacenters, two multimedia clusters in two different datacenters, and a shared global cluster between the two datacenters. [sent-82, score-0.565]
</p><p>9 In Erlang, handle_call blocks for a response and messages are queued up, handle_cast doesn't block because the result of the operation isn't of interest. [sent-87, score-0.317]
</p><p>10 At a certain load the dispatch process itself became a bottleneck and not just because of the execute time. [sent-96, score-0.359]
</p><p>11 There's a high fan-in with a lot of nodes feeding into the dispatch process for a box, the locks on the process become a bottleneck with the distribution ports coming in and the process itself. [sent-97, score-0.607]
</p><p>12 So created a gen_industry, a layer above gen_factory, so that there are multiple dispatch procs which allows for the parallelization of all the input coming into the box as well as the dispatch to the workers themselves. [sent-98, score-0.508]
</p><p>13 Generally try to limit the number of procs that access a singleets(built-in term storage) or single mnesia fragment to 8. [sent-107, score-0.651]
</p><p>14 Each mnesia fragment is only being written to or read from at the application level on one node, which allows replication streams that only go in one direction. [sent-112, score-0.515]
</p><p>15 Patch to allow the mnesia library directory to be split over multiple libraries, which means it could be written to multiple drives, which increases throughput to the disk. [sent-115, score-0.599]
</p><p>16 Gives better opportunity to support schema operations under load because there's only two nodes that have to complete the schema operation. [sent-121, score-0.397]
</p><p>17 Slow access to mnesia table with lots of fragmentsThe account table as 512 fragments which are partitioned into the islands, which means there's a sparse mapping of users to these 512 fragments. [sent-138, score-0.518]
</p><p>18 With a few million connections into a single host and each of those is setting and resetting a timer whenever something happens with a particular phone, the results is hundreds of thousands of timer sets and resets per second. [sent-145, score-0.495]
</p><p>19 Some clusters span a continent, so mnesia should load from a nearby node rather than across the country. [sent-152, score-0.616]
</p><p>20 Can't complete a schema operation with dumps pending so if a lot of dumps are queued it wasn't possible to do schema ops. [sent-157, score-0.312]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mnesia', 0.435), ('erlang', 0.225), ('multimedia', 0.2), ('messages', 0.186), ('dispatch', 0.159), ('procs', 0.136), ('timer', 0.135), ('phones', 0.134), ('partitions', 0.131), ('nodes', 0.121), ('connections', 0.114), ('node', 0.11), ('eve', 0.108), ('islands', 0.091), ('fragments', 0.083), ('whatsapp', 0.083), ('sending', 0.082), ('wandist', 0.08), ('fragment', 0.08), ('peak', 0.079), ('rick', 0.078), ('coupling', 0.076), ('two', 0.073), ('ets', 0.072), ('flapping', 0.072), ('blocking', 0.072), ('downloaded', 0.071), ('load', 0.071), ('io', 0.069), ('queued', 0.068), ('years', 0.067), ('distribution', 0.066), ('process', 0.066), ('schema', 0.066), ('pics', 0.065), ('bottleneck', 0.063), ('block', 0.063), ('async', 0.062), ('fifo', 0.061), ('beam', 0.059), ('patched', 0.058), ('throughput', 0.056), ('dumps', 0.056), ('per', 0.056), ('smp', 0.055), ('million', 0.055), ('problems', 0.055), ('multiple', 0.054), ('boxes', 0.054), ('christmas', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="1622-tfidf-1" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>Introduction: When we lastvisited WhatsAppthey'd just been acquired by Facebook for $19
billion. We learned about their early architecture, which centered around a
maniacal focus on optimizing Erlang into handling 2 million connections a
server, working on All The Phones, and making users happy through
simplicity.Two years later traffic has grown 10x. How did WhatsApp make that
jump to the next level of scalability?Rick Reedtells us in a talk he gave at
the Erlang Factory:That's 'Billion' with a 'B': Scaling to the next level at
WhatsApp(slides), which revealed some eye popping WhatsApp stats:What has
hundreds of nodes, thousands of cores, hundreds of terabytes of RAM, and hopes
to serve the billions of smartphones that will soon be a reality around the
globe? The Erlang/FreeBSD-based server infrastructure at WhatsApp. We've faced
many challenges in meeting the ever-growing demand for our messaging services,
but as we continue to push the envelope on size (>8000 cores) and speed (>70M
Erlang message</p><p>2 0.28476787 <a title="1622-tfidf-2" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>3 0.17373231 <a title="1622-tfidf-3" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>Introduction: This is a guest a post by Alvaro Videla describing their architecture
forPoppen.de, a popular German dating site. This site is very much NSFW, so be
careful before clicking on the link. What I found most interesting is how they
manage to sucessfully blend a little of the old with a little of the new,
using technologies like Nginx, MySQL, CouchDB, and Erlang, Memcached,
RabbitMQ, PHP, Graphite, Red5, and Tsung.What is Poppen.de?Poppen.de (NSFW) is
the top dating website in Germany, and while it may be a small site compared
to giants like Flickr or Facebook, we believe it's a nice architecture to
learn from if you are starting to get some scaling problems.The Stats2.000.000
users20.000 concurrent users300.000 private messages per day250.000 logins per
dayWe have a team of eleven developers, two designers and two sysadmins for
this project.Business ModelThe site works with a freemium model, where users
can do for free things like: Search for other users.Write private messages to
each othe</p><p>4 0.16993813 <a title="1622-tfidf-4" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update: Erlang at Facebook by Eugene Letuchy. How Facebook uses Erlang to
implement Chat, AIM Presence, and Chat Jabber support. I've done
someXMPPdevelopment so when I readFacebook was making a Jabber chat clientI
was really curious how they would make it work. While core XMPP is
straightforward, a number of protocol extensions like discovery, forms, chat
states, pubsub, multi user chat, and privacy lists really up the
implementation complexity. Some real engineering challenges were involved to
make this puppy scale and perform. It's not clear what extensions they've
implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the
architectural challenges they faced and how they overcame them.A web based
Jabber client poses a few problems because XMPP, like most IM protocols, is an
asynchronous event driven system that pretty much assumes you have a full time
open connection. After logging in the server sends a client roster information
and presence information. Your client</p><p>5 0.16841698 <a title="1622-tfidf-5" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>Introduction: For solutions take a look at:7 Life Saving Scalability Defenses Against Load
Monster Attacks.This is a look at all the bad things that can happen to your
carefully crafted program as loads increase: all hell breaks lose. Sure, you
can scale out or scale up, but you can also choose to program better. Make
your system handle larger loads. This saves money because fewer boxes are
needed and it will make the entire application more reliable and have better
response times. And it can be quite satisfying as a programmer.Large Number Of
ObjectsWe usually get into scaling problems when the number of objects gets
larger. Clearly resource usage of all types is stressed as the number of
objects grow.Continuous Failures Makes An Infinite Event StreamDuring large
network failure scenarios there is never time for the system recover. We are
in a continual state of stress.Lots of High Priority WorkFor example,
rerouting is a high priority activity. If there is a large amount of rerouting
work that can</p><p>6 0.16770764 <a title="1622-tfidf-6" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>7 0.16608842 <a title="1622-tfidf-7" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>8 0.16522278 <a title="1622-tfidf-8" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>9 0.15955719 <a title="1622-tfidf-9" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>10 0.15949394 <a title="1622-tfidf-10" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>11 0.15695427 <a title="1622-tfidf-11" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>12 0.15038706 <a title="1622-tfidf-12" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>13 0.1499878 <a title="1622-tfidf-13" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<p>14 0.14473891 <a title="1622-tfidf-14" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>15 0.14467502 <a title="1622-tfidf-15" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>16 0.14418922 <a title="1622-tfidf-16" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>17 0.14418688 <a title="1622-tfidf-17" href="../high_scalability-2008/high_scalability-2008-05-10-Hitting_300_SimbleDB_Requests_Per_Second_on_a_Small_EC2_Instance.html">317 high scalability-2008-05-10-Hitting 300 SimbleDB Requests Per Second on a Small EC2 Instance</a></p>
<p>18 0.14312546 <a title="1622-tfidf-18" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>19 0.14059466 <a title="1622-tfidf-19" href="../high_scalability-2011/high_scalability-2011-05-17-Facebook%3A_An_Example_Canonical_Architecture_for_Scaling_Billions_of_Messages.html">1042 high scalability-2011-05-17-Facebook: An Example Canonical Architecture for Scaling Billions of Messages</a></p>
<p>20 0.1392277 <a title="1622-tfidf-20" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.275), (1, 0.165), (2, -0.046), (3, -0.052), (4, -0.016), (5, 0.021), (6, 0.098), (7, 0.065), (8, -0.083), (9, -0.047), (10, 0.029), (11, 0.072), (12, 0.018), (13, -0.025), (14, -0.005), (15, 0.081), (16, 0.024), (17, -0.003), (18, -0.032), (19, 0.03), (20, 0.063), (21, 0.035), (22, -0.013), (23, -0.033), (24, 0.012), (25, 0.005), (26, 0.058), (27, 0.056), (28, 0.068), (29, -0.024), (30, 0.053), (31, -0.047), (32, -0.044), (33, -0.029), (34, 0.062), (35, 0.018), (36, -0.006), (37, -0.033), (38, -0.004), (39, 0.01), (40, 0.039), (41, -0.056), (42, 0.023), (43, 0.009), (44, -0.061), (45, 0.006), (46, 0.022), (47, 0.057), (48, -0.016), (49, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97023231 <a title="1622-lsi-1" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>Introduction: When we lastvisited WhatsAppthey'd just been acquired by Facebook for $19
billion. We learned about their early architecture, which centered around a
maniacal focus on optimizing Erlang into handling 2 million connections a
server, working on All The Phones, and making users happy through
simplicity.Two years later traffic has grown 10x. How did WhatsApp make that
jump to the next level of scalability?Rick Reedtells us in a talk he gave at
the Erlang Factory:That's 'Billion' with a 'B': Scaling to the next level at
WhatsApp(slides), which revealed some eye popping WhatsApp stats:What has
hundreds of nodes, thousands of cores, hundreds of terabytes of RAM, and hopes
to serve the billions of smartphones that will soon be a reality around the
globe? The Erlang/FreeBSD-based server infrastructure at WhatsApp. We've faced
many challenges in meeting the ever-growing demand for our messaging services,
but as we continue to push the envelope on size (>8000 cores) and speed (>70M
Erlang message</p><p>2 0.8216002 <a title="1622-lsi-2" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>Introduction: We talked about 42 Monster Problems That Attack As Loads Increase. And inThe
Aggregation Collection we talked about the value of prioritizing work and
making smart queues as a way of absorbing and not reflecting traffic
spikes.Now we move on to our next batch of strategies where the theme
isconditioning, which is the idea of shaping and controlling flows of work
within your application...Use Resources Proportional To a Fixed LimitThis is
probably the most important rule for achieving scalability within an
application. What it means:Find the resource that has a fixed limit that you
know you can support. For example, a guarantee to handle a certain number of
objects in memory. So if we always use resources proportional to the number of
objects it is likely we can prevent resource exhaustion.Devise ways of tying
what you need to do to the individual resources.Some examples:Keep a list of
purchase orders with line items over $20 (or whatever). Do not keep a list of
the line items because t</p><p>3 0.80237347 <a title="1622-lsi-3" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>4 0.80092847 <a title="1622-lsi-4" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>Introduction: For solutions take a look at:7 Life Saving Scalability Defenses Against Load
Monster Attacks.This is a look at all the bad things that can happen to your
carefully crafted program as loads increase: all hell breaks lose. Sure, you
can scale out or scale up, but you can also choose to program better. Make
your system handle larger loads. This saves money because fewer boxes are
needed and it will make the entire application more reliable and have better
response times. And it can be quite satisfying as a programmer.Large Number Of
ObjectsWe usually get into scaling problems when the number of objects gets
larger. Clearly resource usage of all types is stressed as the number of
objects grow.Continuous Failures Makes An Infinite Event StreamDuring large
network failure scenarios there is never time for the system recover. We are
in a continual state of stress.Lots of High Priority WorkFor example,
rerouting is a high priority activity. If there is a large amount of rerouting
work that can</p><p>5 0.78572929 <a title="1622-lsi-5" href="../high_scalability-2012/high_scalability-2012-12-17-11_Uses_For_the_Humble_Presents_Queue%2C_er%2C_Message_Queue.html">1373 high scalability-2012-12-17-11 Uses For the Humble Presents Queue, er, Message Queue</a></p>
<p>Introduction: It's a little known fact that Santa Clause was an early queue innovator. Faced
with the problem of delivering a planet full of presents in one night, Santa,
in his hacker's workshop, created a Present Distribution System using
thousands of region based priority present queues for continuous delivery by
the Rudolphs. Rudolphs? You didn't think there was only one Rudolph did you?
Presents are delivered in parallel by a cluster of sleighs, each with
redundant reindeer in a master-master configuration. Each Rudolph is a cluster
leader and they coordinate work using an early and more magical version of the
ZooKeeper protocol.Programmers have followed Santa's lead and you can find
amessage queue in nearly every major architecture profile onHighScalability.
Historically they may have been introduced after a first generation
architecture needed to scale up from their two tier system into something a
little more capable (asynchronicity, work dispatch, load buffering, database
offloading, etc).</p><p>6 0.77569497 <a title="1622-lsi-6" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>7 0.77562982 <a title="1622-lsi-7" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>8 0.76758993 <a title="1622-lsi-8" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>9 0.7647649 <a title="1622-lsi-9" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>10 0.75426745 <a title="1622-lsi-10" href="../high_scalability-2007/high_scalability-2007-07-23-GoogleTalk_Architecture.html">21 high scalability-2007-07-23-GoogleTalk Architecture</a></p>
<p>11 0.75416648 <a title="1622-lsi-11" href="../high_scalability-2008/high_scalability-2008-12-29-Paper%3A_Spamalytics%3A_An_Empirical_Analysisof_Spam_Marketing_Conversion.html">478 high scalability-2008-12-29-Paper: Spamalytics: An Empirical Analysisof Spam Marketing Conversion</a></p>
<p>12 0.74987042 <a title="1622-lsi-12" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>13 0.74401307 <a title="1622-lsi-13" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>14 0.74251419 <a title="1622-lsi-14" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Littleâ€™s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>15 0.74109954 <a title="1622-lsi-15" href="../high_scalability-2014/high_scalability-2014-01-13-NYTimes_Architecture%3A_No_Head%2C_No_Master%2C_No_Single_Point_of_Failure.html">1577 high scalability-2014-01-13-NYTimes Architecture: No Head, No Master, No Single Point of Failure</a></p>
<p>16 0.74060154 <a title="1622-lsi-16" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>17 0.7350632 <a title="1622-lsi-17" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<p>18 0.73108917 <a title="1622-lsi-18" href="../high_scalability-2011/high_scalability-2011-12-30-Stuff_The_Internet_Says_On_Scalability_For_December_30%2C_2011.html">1166 high scalability-2011-12-30-Stuff The Internet Says On Scalability For December 30, 2011</a></p>
<p>19 0.73082924 <a title="1622-lsi-19" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>20 0.72733444 <a title="1622-lsi-20" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.114), (2, 0.275), (10, 0.054), (27, 0.12), (30, 0.017), (40, 0.021), (56, 0.013), (61, 0.108), (77, 0.023), (79, 0.081), (85, 0.056), (94, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9814201 <a title="1622-lda-1" href="../high_scalability-2011/high_scalability-2011-08-12-Stuff_The_Internet_Says_On_Scalability_For_August_12%2C_2011.html">1097 high scalability-2011-08-12-Stuff The Internet Says On Scalability For August 12, 2011</a></p>
<p>Introduction: Submitted for your scaling pleasure, you may not  scale often, but when you
scale, please drink us:Quotably quotable quotes:@mardix: There is no single
point of truth in #NoSQL . #Consistency is no longer global, it's relative to
the one accessing it. #Scalability@kekline: RT @CurtMonash: "...from industry
figures, Basho/Riak is our third-biggest competitor." How often do you
encounter them? "Never have" #nosql@dave_jacobs: Love being in a city where I
can overhear a convo about Heroku scalability while doing deadlifts.
#ahsanfrancisco@satheeshilu: Doctor at #hospital in india says #ge #healthcare
software is slow to handle 100K X-rays an year.Scalability is critical 4
Indian #software@sufw: How can it be possible that Tagged has 80m users and I
have *never* heard of it!?!@EventCloudPro: One of my vacation realizations?
Whole #bigdata thing has turned into a lotta #bighype - many distinct issues &
nothing to do w/ #bigdataNoSQL as dynamic duos. NoSQL combinations - what
works best? A c</p><p>2 0.97168821 <a title="1622-lda-2" href="../high_scalability-2011/high_scalability-2011-11-11-Stuff_The_Internet_Says_On_Scalability_For_November_11%2C_2011.html">1141 high scalability-2011-11-11-Stuff The Internet Says On Scalability For November 11, 2011</a></p>
<p>Introduction: You got performance in my scalability! You got scalability in my performance!
Two great tastes that taste great together:Quotable quotes:@jasoncbooth: Tired
of the term #nosql. I would like to coin NRDS (pronounced "nerds"), standing
for Non Relational Data Store. @zenfeed: One lesson I learn about scalability,
is that it has a LOT to do with simplicity and consistency.Ray Walters: Quad-
core chips in mobile phones is nothing but a marketing snow jobFlickr:  Real-
time Updates on the Cheap for Fun and Profit. How Flickr added real-time push
feed on the cheap. Events happen all over Flickr, uploads and updates (around
100/s depending on the time of day), all of them inserting tasks.Implemented
with Cache, Tasks, & Queues: PubSubHubbub; Async task system Gearman; use
async EVERYWHERE; use Redis Lists for queues; cron to consume events off the
queue; Cloud Event Processing - Big Data, Low Latency Use Cases at LinkedIn by
Colin Clark. It talks about some big data, low latency use cases and</p><p>3 0.97112137 <a title="1622-lda-3" href="../high_scalability-2007/high_scalability-2007-07-25-Product%3A_NetApp_MetroCluster_Software.html">28 high scalability-2007-07-25-Product: NetApp MetroCluster Software</a></p>
<p>Introduction: NetApp MetroCluster SoftwareCost-effective is an integrated high-availability
storage cluster and site failover capability.NetApp MetroCluster is an
integrated high-availability and disaster recovery solution that can reduce
system complexity and simplify management while ensuring greater return on
investment. MetroCluster uses clustered server technology to replicate data
synchronously between sites located miles apart, eliminating data loss in case
of a disruption. Simple and powerful recovery process minimizes downtime, with
little or no user action required.At one company I worked at they used the
NetApp snap mirror feature to replicate data across long distances to multiple
datacenters. They had a very fast backbone and it worked well. The issue with
NetApp is always one of cost, but if you can afford it, it's a good option.</p><p>4 0.96330911 <a title="1622-lda-4" href="../high_scalability-2013/high_scalability-2013-06-27-Paper%3A_XORing_Elephants%3A_Novel_Erasure_Codes_for_Big_Data.html">1483 high scalability-2013-06-27-Paper: XORing Elephants: Novel Erasure Codes for Big Data</a></p>
<p>Introduction: Erasure codesare one of those seemingly magical mathematical creations that
with the developments described in the paper XORing Elephants: Novel Erasure
Codes for Big Data, are set to replace triple replication as the data storage
protection mechanism of choice.The result says Robin Harris (StorageMojo) in
an excellent article, Facebook's advanced erasure codes: "WebCos will be able
to store massive amounts of data more efficiently than ever before. Bad news:
so will anyone else."Robin says with cheap disks triple replication made sense
and was economical. With ever bigger BigData the overhead has become costly.
But erasure codes have always suffered from unacceptably long time to repair
times. This paper describes new Locally Repairable Codes (LRCs) that are
efficiently repairable in disk I/O and bandwidth requirements:These systems
are now designed to survive the loss of up to four storage elements - disks,
servers, nodes or even entire data centers - without losing any data. What is</p><p>same-blog 5 0.96159804 <a title="1622-lda-5" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>Introduction: When we lastvisited WhatsAppthey'd just been acquired by Facebook for $19
billion. We learned about their early architecture, which centered around a
maniacal focus on optimizing Erlang into handling 2 million connections a
server, working on All The Phones, and making users happy through
simplicity.Two years later traffic has grown 10x. How did WhatsApp make that
jump to the next level of scalability?Rick Reedtells us in a talk he gave at
the Erlang Factory:That's 'Billion' with a 'B': Scaling to the next level at
WhatsApp(slides), which revealed some eye popping WhatsApp stats:What has
hundreds of nodes, thousands of cores, hundreds of terabytes of RAM, and hopes
to serve the billions of smartphones that will soon be a reality around the
globe? The Erlang/FreeBSD-based server infrastructure at WhatsApp. We've faced
many challenges in meeting the ever-growing demand for our messaging services,
but as we continue to push the envelope on size (>8000 cores) and speed (>70M
Erlang message</p><p>6 0.95408773 <a title="1622-lda-6" href="../high_scalability-2010/high_scalability-2010-08-20-Hot_Scalability_Links_For_Aug_20%2C_2010.html">883 high scalability-2010-08-20-Hot Scalability Links For Aug 20, 2010</a></p>
<p>7 0.94841522 <a title="1622-lda-7" href="../high_scalability-2009/high_scalability-2009-10-07-How_to_Avoid_the_Top_5_Scale-Out_Pitfalls.html">717 high scalability-2009-10-07-How to Avoid the Top 5 Scale-Out Pitfalls</a></p>
<p>8 0.94138861 <a title="1622-lda-8" href="../high_scalability-2012/high_scalability-2012-04-18-Ansible_-__A_Simple_Model-Driven_Configuration_Management_and_Command_Execution_Framework.html">1230 high scalability-2012-04-18-Ansible -  A Simple Model-Driven Configuration Management and Command Execution Framework</a></p>
<p>9 0.9367885 <a title="1622-lda-9" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>10 0.93382335 <a title="1622-lda-10" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>11 0.93216234 <a title="1622-lda-11" href="../high_scalability-2012/high_scalability-2012-03-26-7_Years_of_YouTube_Scalability_Lessons_in_30_Minutes.html">1215 high scalability-2012-03-26-7 Years of YouTube Scalability Lessons in 30 Minutes</a></p>
<p>12 0.93087745 <a title="1622-lda-12" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>13 0.93050671 <a title="1622-lda-13" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>14 0.93004566 <a title="1622-lda-14" href="../high_scalability-2009/high_scalability-2009-03-18-QCon_London_2009%3A_Upgrading_Twitter_without_service_disruptions.html">544 high scalability-2009-03-18-QCon London 2009: Upgrading Twitter without service disruptions</a></p>
<p>15 0.92993397 <a title="1622-lda-15" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>16 0.92897421 <a title="1622-lda-16" href="../high_scalability-2011/high_scalability-2011-12-05-Stuff_The_Internet_Says_On_Scalability_For_December_5%2C_2011.html">1151 high scalability-2011-12-05-Stuff The Internet Says On Scalability For December 5, 2011</a></p>
<p>17 0.92884624 <a title="1622-lda-17" href="../high_scalability-2009/high_scalability-2009-06-13-Neo4j_-_a_Graph_Database_that_Kicks_Buttox.html">628 high scalability-2009-06-13-Neo4j - a Graph Database that Kicks Buttox</a></p>
<p>18 0.92880547 <a title="1622-lda-18" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>19 0.92838573 <a title="1622-lda-19" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>20 0.9281801 <a title="1622-lda-20" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
