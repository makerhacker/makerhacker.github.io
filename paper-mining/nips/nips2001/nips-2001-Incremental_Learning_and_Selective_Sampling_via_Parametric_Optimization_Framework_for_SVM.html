<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-94" href="#">nips2001-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</h1>
<br/><p>Source: <a title="nips-2001-94-pdf" href="http://papers.nips.cc/paper/1978-incremental-learning-and-selective-sampling-via-parametric-optimization-framework-for-svm.pdf">pdf</a></p><p>Author: Shai Fine, Katya Scheinberg</p><p>Abstract: We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence. 1</p><p>Reference: <a title="nips-2001-94-reference" href="../nips2001_reference/nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. [sent-9, score-0.28]
</p><p>2 This framework, can be specialized to obtain two SVM optimization methods. [sent-10, score-0.088]
</p><p>3 The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. [sent-11, score-1.701]
</p><p>4 The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. [sent-12, score-0.507]
</p><p>5 Moreover, the second method can also be used independently to solve the complete SVM training problem. [sent-13, score-0.193]
</p><p>6 A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. [sent-14, score-0.529]
</p><p>7 1  Introduction  SVM training is a convex optimization problem which scales with the training set size rather than the input dimension. [sent-16, score-0.232]
</p><p>8 While this is usually considered to be a desired The quality, in large scale problems it may cause training to be impractical. [sent-17, score-0.051]
</p><p>9 common way to handle massive data applications is to turn to active set methods, which gradually build the set of active constraints by feeding a generic optimizer with small scale sub-problems. [sent-18, score-0.415]
</p><p>10 Active set methods guarantee to converge to the global solut ion, however, convergence may be very slow, it may require too many passes over the data set, and at each iteration there's an implicit computational overhead of the actual active set selection. [sent-19, score-0.424]
</p><p>11 Another common practice is to modify the SVM optimization problem such that it wont handle the bias term directly. [sent-21, score-0.467]
</p><p>12 [6]) or added as another dimension to the feature space (e. [sent-25, score-0.07]
</p><p>13 The advantage is that the resulting dual optimization problem does not contain the linear constraint, in which case one can suggest a procedure which updates only IThroughout this sequel we will refer to such solut ion as the fixed bias solut ion. [sent-28, score-0.778]
</p><p>14 Thus, an incremental approach, which efficiently updates an existing solution given a new training point, can be devised. [sent-30, score-0.412]
</p><p>15 Though widely used, the solution resulting from this practice has inferior generalization performances and the number of SY tends to be much higher [4]. [sent-31, score-0.191]
</p><p>16 To the best of our knowledge, the only incremental algorithm suggested so far to exactly solve the 1-Norm Soft Margin 2 optimization problem, have been described by Cauwenberghs and Poggio at [3]. [sent-32, score-0.444]
</p><p>17 This algorithm, handles Adiabatic increments by solving a system of linear equations resulted from a parametric transcription of the KKT conditions. [sent-33, score-0.254]
</p><p>18 In this paper 3 we introduce two new methods derived from parametric QP techniques. [sent-35, score-0.126]
</p><p>19 The first method solves the fixed bias problem, while the second one starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. [sent-37, score-1.747]
</p><p>20 Each of these methods can be used independently to solve the SYM training problem. [sent-38, score-0.147]
</p><p>21 The most interesting application, however, is alternating between the two methods to obtain a unique incremental algorithm. [sent-39, score-0.249]
</p><p>22 We will show how by using this approach we can adjust the optimal solution as more data becomes available, and by applying Selective Sampling techniques we may further boost convergence rate. [sent-40, score-0.408]
</p><p>23 In principle, this number may be exponential in the training set size, n. [sent-42, score-0.051]
</p><p>24 However, since parametric QP methods are based on the well-known Simplex method for linear programming, a similar behavior is expected: Though in theory the Simplex method is known to have exponential complexity, in practice it hardly ever displays exponential behavior. [sent-43, score-0.261]
</p><p>25 The per-iteration complexity is expected to be O(nl), where l is the number of active points at that iteration, with the exception of some rare cases in which the complexity is expected to be bounded by O(nl2). [sent-44, score-0.279]
</p><p>26 2  Parametric QP for SVM  Any optimal solution to the 1-Norm Soft Margin SYM optimization problem must satisfy the Karush-Kuhn-Tucker (KKT) necessary and sufficient conditions: 1 2  exiSi = 0, i = 1, . [sent-45, score-0.322]
</p><p>27 °  2 A different incremental approach stems from a geometric interpretation of the primal problem: Keerthi et al. [sent-52, score-0.249]
</p><p>28 [7] were the first to suggest a nearest point batch algorithm and Kowalczyk [8] provided the on-line version. [sent-53, score-0.201]
</p><p>29 ;c~) and b ~ b, which establish the equivalence between the Hard Margin and the 2-Norm Soft Margin optimization problems. [sent-55, score-0.088]
</p><p>30 It is obvious by the above transformation that the i-Norm Soft Margin is the most general SVM optimization problem. [sent-58, score-0.088]
</p><p>31 where a E Rn is the vector of Lagrange multipliers, b is the bias (scalar) and sand ~ are the n-dimensional vectors of slack and surplus variables, respectively. [sent-60, score-0.497]
</p><p>32 If we assume that the value of the bias is fixed to some predefined value b, then condition 3 disappears from the system (1) and condition 4 becomes  -Qa + S  -  ~  = -e - by  (2)  Consider the following modified parametric system of KKT conditions  aiSi = 0, i = 1, . [sent-65, score-0.678]
</p><p>33 For u = 1 the systems (3) reduces to the fixed bias system. [sent-74, score-0.461]
</p><p>34 Our fixed bias method starts at a solution to (3) for u = 0 and by increasing u while updating a, s and ~ so that they satisfy (3), obtains the optimal solution for u = 1. [sent-75, score-0.936]
</p><p>35 Similarly we can obtain solution to (1) by starting at a fixed bias solution and update b, while maintaining a, s and ~ feasible for (2) , until the optimal value for b is reached. [sent-76, score-0.807]
</p><p>36 The optimal value of the bias is recognized when the corresponding solution satisfy (1), namely aT y = O. [sent-77, score-0.532]
</p><p>37 Both these methods are based on the same framework of adjusting a scalar parameter in the right hand side of a KKT system. [sent-78, score-0.104]
</p><p>38 In the next section we will present the method for adjusting the bias (adjusting u in (3) is very similar, save for a few technical differences). [sent-79, score-0.45]
</p><p>39 An advantage of this special case is that it solves the original problem and can, in principal, be applied "from scratch" . [sent-80, score-0.05]
</p><p>40 3  Correcting a "Fixed Bias" Solution  Let (a(b), s(b), ~(b)) be a fixed bias solution for a given b. [sent-81, score-0.573]
</p><p>41 The algorithm that we present here is based on increasing (or decreasing) b monotonically, until the optimal b* is found, while updating and maintaining (a(b),s(b),~(b)). [sent-82, score-0.224]
</p><p>42 For a given b and and a fixed bias solution, (a(b), s(b), ~(b)), we partition the index set I = {I, . [sent-84, score-0.618]
</p><p>43 We will call the partition (Io(b),Ie(b),Is(b)) - the optimal partition for a given b. [sent-89, score-0.436]
</p><p>44 Based on partition (Io,Ie,Is) we define Qss (Qes Qse Qee, Qos, Qoo) as the submatrix of Q whose columns are the columns of Q indexed by the set Is (Ie, Is, Ie, 10 , 10 ) and whose rows are the rows of Q indexed by Is (Is, Ie, Ie, Is , 10). [sent-91, score-0.257]
</p><p>45 To initiate the algorithm we  4Whether bO < b* can be determined by evaluating -y T a(bO): if -y T a(bO) > 0 then bO < b*, otherwise bO > b*, in which case the algorithm is essentially the same, save for obvious changes. [sent-95, score-0.151]
</p><p>46 assume that we know the optimal partition (Ioo'! [sent-96, score-0.279]
</p><p>47 We can write the set of active constraints as  (4) If Qss is nonsingular (the nondegenerate case), then as depends linearly on scalar b. [sent-102, score-0.271]
</p><p>48 If Q ss is singular (the degenerate case), then, the set of all possible solutions as changes linearly with b as long as the partition remains optimal. [sent-104, score-0.252]
</p><p>49 At each iteration b can increase until one of the four types of inequality constraints becomes active. [sent-106, score-0.115]
</p><p>50 Then, the optimal partition is updated, new linear expressions of the active variables through b are computed, and the algorithm iterates. [sent-107, score-0.501]
</p><p>51 The final iteration gives us the correct optimal active set and optimal partition; from that we can easily compute b* and a*. [sent-109, score-0.491]
</p><p>52 A geometric interpretation of the algorithmic steps suggest that we are trying to move the separating hyperplane by increasing its bias and at the same time adjusting its orientation so it stays optimal for the current bias. [sent-110, score-0.532]
</p><p>53 At each iteration we move the hyperplane until either a support vector is dropped from the support set, a support vector becomes violated, a violated point becomes a support vector or an inactive point joins the support vector set. [sent-111, score-0.639]
</p><p>54 The algorithm is guaranteed to terminate after finitely many iterations. [sent-112, score-0.163]
</p><p>55 At each iteration the algorithm covers an interval that corresponds to an optimal partition. [sent-113, score-0.257]
</p><p>56 The same partition cannot correspond to two different intervals and the number of partitions is finite, hence so is the number of iterations (d. [sent-114, score-0.201]
</p><p>57 Per-iteration complexity depends on whether an iteration is degenerate or not. [sent-116, score-0.213]
</p><p>58 A nondegenerate iteration takes O(niIs I) + O(IIs 3 ) arithmetic operations, while a degenerate iteration should in theory take 0(n21Is 12) operations, but in practice it only takes 5 0(nIIsI2). [sent-117, score-0.428]
</p><p>59 Note that the degeneracy occurs when the active support vectors are linearly dependent. [sent-118, score-0.218]
</p><p>60 The storage requirement of the algorithm is O(n) + 0(IIsI2). [sent-120, score-0.055]
</p><p>61 1  4  Incremental Algorithm  Incremental and on-line algorithms are aimed at training problems for which the data becomes available in the course of training. [sent-121, score-0.086]
</p><p>62 Such an algorithm, when given an optimal solution for a training set of size n, and additional m training points, has to efficiently find the optimal solution to the extended n + m training set. [sent-122, score-0.71]
</p><p>63 Assume we have an optimal solution (a, b, s,~) for a given data set X of size n. [sent-123, score-0.276]
</p><p>64 For each new point that is added, we take the following actions: a new Lagrange multiplier a n +l = 0 is added to the set of multipliers, then the distance to the margin is evaluated for this point. [sent-124, score-0.212]
</p><p>65 If the point is not violated, that is if Sn+l = W T xn+l_yn+1b_1 > 0, then the new positive slack Sn+l is added to the set of slack variables. [sent-125, score-0.314]
</p><p>66 If the point is violated then sn+1 = 1 is added to the set of slack variables. [sent-126, score-0.366]
</p><p>67 ) A surplus variable ~n+l = 0 is also added to the set of surplus variables. [sent-128, score-0.264]
</p><p>68 The process is repeated for all the points that have to be added at the given step. [sent-130, score-0.106]
</p><p>69 Figure 1: Outline of the incremental algorithm (AltPOKER)  then no further action is necessary. [sent-144, score-0.304]
</p><p>70 The current solution is optimal and the bias is unchanged. [sent-145, score-0.532]
</p><p>71 However, it is easy to find p such that (Q, b, s, ~) is optimal for (3). [sent-147, score-0.169]
</p><p>72 Thus we can first apply the fixed bias algorithm to find a new solution and then apply the adjustable bias algorithm to find the optimal solution to the new extended problem (see Figure 1). [sent-148, score-1.391]
</p><p>73 In theory adding even one point may force the algorithm to work as hard as if it were solving the problem "from scratch". [sent-149, score-0.095]
</p><p>74 In our experiments, just a few iterations of the fixed bias and adjustable bias algorithms were sufficient to find the solution to the extended problem. [sent-151, score-1.044]
</p><p>75 Overall, the computational complexity ofthe incremental algorithm is expected to be O(n 2 ) . [sent-152, score-0.342]
</p><p>76 5  Experiments  Convergence in Batch Mode: The most straight-forward way to activate POKER in a batch mode is to construct the trivial partition6 and then apply the adjustable bias algorithm to get the optimal solution. [sent-153, score-0.749]
</p><p>77 Note that the initial value of the bias is most likely far away from the global solution, and as such, the results presented here should be regarded as a lower bound. [sent-155, score-0.298]
</p><p>78 We examined performances on a moderate size problem, the Abalone data set from the VCI Repository [2]. [sent-156, score-0.078]
</p><p>79 We fed the training algorithm with increasing subsets up to the whole set (of size 4177). [sent-157, score-0.195]
</p><p>80 We demonstrate convergence for polynomial kernel with increasing degree, which in this setting corresponds to level of difficulty. [sent-160, score-0.169]
</p><p>81 However naive our implementation is, one can observe (see Figure 2) a linear convergence rate in the batch mode. [sent-161, score-0.186]
</p><p>82 Convergence in Incremental Mode: AltPOKER is the incremental algorithm described in section 4. [sent-162, score-0.304]
</p><p>83 We demonstrate convergence for the RBF kernel with increasing penalty ("C" ). [sent-164, score-0.169]
</p><p>84 Figure 3 demonstrates the advantage of the more flexible approach 6Fixing the bias term to be large enough (positive or negative) and the Lagrange multipliers to 0 or C based on their class (negative/positive) membership. [sent-165, score-0.386]
</p><p>85 Selective Sampling: We can use the incremental algorithm even in case when all the data is available in advance to improve the overall efficiency. [sent-179, score-0.304]
</p><p>86 We applied selective sampling as a preprocess in incremental mode: At each meta-iteration, we ranked the points according to a predefined selection criterion, and then picked just the top ones for the increment. [sent-181, score-0.621]
</p><p>87 The following selection criteria have been used in our experiments: CIs2W picks the closest point to the current hyperplane. [sent-182, score-0.165]
</p><p>88 This approach is inspired by active learning schemes which strive to halve the version space. [sent-183, score-0.167]
</p><p>89 Thus, it is reasonable to adapt a greedy approach which selects the point that will cause the larger change in the value of the objective function. [sent-185, score-0.088]
</p><p>90 While solving the optimization problem for all possible increments is impracticable, it may still worthwhile to approximate the potential change: MaxSlk picks the most violating point. [sent-186, score-0.261]
</p><p>91 This corresponds to an upper bound estimate of the change in the objective, since the value of the slack (times c) is an upper bound to the feasibility gap. [sent-187, score-0.102]
</p><p>92 dObj perform only few iterations of the adjustable bias algorithm and examine the change in the objective value. [sent-188, score-0.527]
</p><p>93 Although performing only few iterations is much cheaper than converging to the optimal solution, this technique is still more demanding then previous selection methods. [sent-191, score-0.206]
</p><p>94 Hence we first ranked the points using CIs2W (MaxSlk) and then applied dObj only to the top few . [sent-192, score-0.075]
</p><p>95 )  6  Conclusions and Discussion  We propose a new finitely convergent method that can be applied in both batch and incremental modes to solve the 1-Norm Soft Margin SVM problem. [sent-198, score-0.518]
</p><p>96 Assuming that the number of support vectors is small compared to the size of the data, the method is expected to perform O(n 2 ) arithmetic operations, where n is the problem size. [sent-199, score-0.204]
</p><p>97 Applying Selective Sampling techniques may further boost convergence and reduce computation load. [sent-200, score-0.139]
</p><p>98 Our method is independently developed, but somewhat similar to that in [3]. [sent-201, score-0.09]
</p><p>99 This re-introduces some very important applications of the on-line technology, such as active learning, and various forms of adaptation. [sent-204, score-0.167]
</p><p>100 A fast iterative nearest point algorithm for SVM classifier design . [sent-257, score-0.095]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bias', 0.298), ('incremental', 0.249), ('bo', 0.224), ('active', 0.167), ('fixed', 0.163), ('dobj', 0.162), ('poker', 0.162), ('ie', 0.158), ('partition', 0.157), ('violated', 0.154), ('io', 0.143), ('kkt', 0.14), ('sn', 0.132), ('selective', 0.129), ('altpoker', 0.129), ('maxslk', 0.129), ('increments', 0.128), ('parametric', 0.126), ('svm', 0.124), ('optimal', 0.122), ('solution', 0.112), ('batch', 0.106), ('margin', 0.102), ('slack', 0.102), ('pojykemel', 0.097), ('solut', 0.097), ('surplus', 0.097), ('degenerate', 0.095), ('cauwenberghs', 0.09), ('optimization', 0.088), ('mode', 0.086), ('adjustable', 0.082), ('iteration', 0.08), ('convergence', 0.08), ('soft', 0.079), ('qp', 0.078), ('sampling', 0.072), ('vi', 0.072), ('watson', 0.071), ('added', 0.07), ('adjusting', 0.065), ('arithmetic', 0.065), ('clsw', 0.065), ('diabetes', 0.065), ('finitely', 0.065), ('nis', 0.065), ('nondegenerate', 0.065), ('qss', 0.065), ('selfinit', 0.065), ('sym', 0.065), ('poggio', 0.064), ('ai', 0.062), ('ibm', 0.061), ('lagrange', 0.06), ('boost', 0.059), ('predefined', 0.056), ('chunk', 0.056), ('algorithm', 0.055), ('solve', 0.052), ('training', 0.051), ('support', 0.051), ('indexed', 0.05), ('solves', 0.05), ('xn', 0.05), ('multipliers', 0.049), ('objective', 0.048), ('keerthi', 0.048), ('yn', 0.048), ('increasing', 0.047), ('find', 0.047), ('method', 0.046), ('fine', 0.045), ('picks', 0.045), ('iterations', 0.044), ('independently', 0.044), ('practice', 0.043), ('terminate', 0.043), ('repository', 0.043), ('massive', 0.043), ('scratch', 0.043), ('kernel', 0.042), ('size', 0.042), ('operations', 0.041), ('save', 0.041), ('criteria', 0.04), ('selection', 0.04), ('point', 0.04), ('scalar', 0.039), ('flexible', 0.039), ('adjusts', 0.039), ('ranked', 0.039), ('simplex', 0.039), ('complexity', 0.038), ('handle', 0.038), ('ao', 0.038), ('starts', 0.036), ('performances', 0.036), ('points', 0.036), ('becomes', 0.035), ('ion', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="94-tfidf-1" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>Author: Shai Fine, Katya Scheinberg</p><p>Abstract: We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence. 1</p><p>2 0.13386045 <a title="94-tfidf-2" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>Author: Martijn Leisink, Bert Kappen</p><p>Abstract: The partition function for a Boltzmann machine can be bounded from above and below. We can use this to bound the means and the correlations. For networks with small weights, the values of these statistics can be restricted to non-trivial regions (i.e. a subset of [-1 , 1]). Experimental results show that reasonable bounding occurs for weight sizes where mean field expansions generally give good results. 1</p><p>3 0.11705813 <a title="94-tfidf-3" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>Author: Patrick Haffner</p><p>Abstract: Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 1</p><p>4 0.11567052 <a title="94-tfidf-4" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>5 0.11400812 <a title="94-tfidf-5" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>Author: Eyal Even-dar, Yishay Mansour</p><p>Abstract: Vie sho,v the convergence of tV/O deterministic variants of Qlearning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an Eoptimal policy. The second is a new and novel algorithm incremental Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algorithm can be viewed as derandomization of the E-greedy Q-learning. 1</p><p>6 0.1099907 <a title="94-tfidf-6" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>7 0.10770886 <a title="94-tfidf-7" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>8 0.10278042 <a title="94-tfidf-8" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>9 0.091682546 <a title="94-tfidf-9" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>10 0.090164416 <a title="94-tfidf-10" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>11 0.089235209 <a title="94-tfidf-11" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>12 0.087837674 <a title="94-tfidf-12" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>13 0.086437188 <a title="94-tfidf-13" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>14 0.085646883 <a title="94-tfidf-14" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>15 0.083339095 <a title="94-tfidf-15" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>16 0.078768037 <a title="94-tfidf-16" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>17 0.07840392 <a title="94-tfidf-17" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>18 0.076440215 <a title="94-tfidf-18" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>19 0.075886518 <a title="94-tfidf-19" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>20 0.075397626 <a title="94-tfidf-20" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.249), (1, 0.059), (2, 0.062), (3, 0.03), (4, 0.108), (5, 0.022), (6, 0.004), (7, -0.045), (8, -0.026), (9, 0.005), (10, 0.067), (11, 0.051), (12, 0.08), (13, 0.07), (14, 0.187), (15, 0.101), (16, 0.01), (17, -0.037), (18, -0.008), (19, -0.066), (20, -0.053), (21, -0.026), (22, 0.018), (23, -0.112), (24, 0.063), (25, 0.079), (26, 0.04), (27, -0.069), (28, -0.002), (29, -0.102), (30, -0.009), (31, -0.005), (32, 0.132), (33, 0.051), (34, 0.114), (35, 0.059), (36, 0.093), (37, 0.079), (38, 0.052), (39, 0.015), (40, 0.046), (41, -0.069), (42, -0.062), (43, 0.028), (44, 0.085), (45, -0.035), (46, -0.043), (47, -0.114), (48, 0.212), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95775223 <a title="94-lsi-1" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>Author: Shai Fine, Katya Scheinberg</p><p>Abstract: We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence. 1</p><p>2 0.62598962 <a title="94-lsi-2" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann</p><p>Abstract: We consider noisy Euclidean traveling salesman problems in the plane, which are random combinatorial problems with underlying structure. Gibbs sampling is used to compute average trajectories, which estimate the underlying structure common to all instances. This procedure requires identifying the exact relationship between permutations and tours. In a learning setting, the average trajectory is used as a model to construct solutions to new instances sampled from the same source. Experimental results show that the average trajectory can in fact estimate the underlying structure and that overfitting effects occur if the trajectory adapts too closely to a single instance. 1</p><p>3 0.5699594 <a title="94-lsi-3" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>Author: Manfred K. Warmuth, Gunnar Rätsch, Michael Mathieson, Jun Liao, Christian Lemmen</p><p>Abstract: We investigate the following data mining problem from Computational Chemistry: From a large data set of compounds, ﬁnd those that bind to a target molecule in as few iterations of biological testing as possible. In each iteration a comparatively small batch of compounds is screened for binding to the target. We apply active learning techniques for selecting the successive batches. One selection strategy picks unlabeled examples closest to the maximum margin hyperplane. Another produces many weight vectors by running perceptrons over multiple permutations of the data. Each weight vector votes with its prediction and we pick the unlabeled examples for which the prediction is most evenly split between and . For a third selection strategy note that each unlabeled example bisects the version space of consistent weight vectors. We estimate the volume on both sides of the split by bouncing a billiard through the version space and select unlabeled examples that cause the most even split of the version space. We demonstrate that on two data sets provided by DuPont Pharmaceuticals that all three selection strategies perform comparably well and are much better than selecting random batches for testing. § © ¨</p><p>4 0.56204629 <a title="94-lsi-4" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>Author: Patrick Haffner</p><p>Abstract: Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 1</p><p>5 0.48590708 <a title="94-lsi-5" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>Author: Thomas G. Dietterich, Xin Wang</p><p>Abstract: We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formulations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradient methods, the kernel methods described here can easily 'adjust the complexity of the function approximator to fit the complexity of the value function. 1</p><p>6 0.4808867 <a title="94-lsi-6" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>7 0.4725894 <a title="94-lsi-7" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>8 0.46645838 <a title="94-lsi-8" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>9 0.45849589 <a title="94-lsi-9" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>10 0.43274781 <a title="94-lsi-10" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>11 0.43031949 <a title="94-lsi-11" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>12 0.42759141 <a title="94-lsi-12" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>13 0.4210811 <a title="94-lsi-13" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>14 0.41918987 <a title="94-lsi-14" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>15 0.39235163 <a title="94-lsi-15" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>16 0.39036065 <a title="94-lsi-16" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>17 0.38681778 <a title="94-lsi-17" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>18 0.38649678 <a title="94-lsi-18" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>19 0.36859876 <a title="94-lsi-19" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>20 0.36798105 <a title="94-lsi-20" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.038), (17, 0.456), (19, 0.032), (27, 0.099), (30, 0.05), (59, 0.027), (72, 0.077), (79, 0.035), (83, 0.016), (88, 0.011), (91, 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92740482 <a title="94-lda-1" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>Author: Rémi Munos</p><p>Abstract: It is desirable that a complex decision-making problem in an uncertain world be adequately modeled by a Markov Decision Process (MDP) whose structural representation is adaptively designed by a parsimonious resources allocation process. Resources include time and cost of exploration, amount of memory and computational time allowed for the policy or value function representation. Concerned about making the best use of the available resources, we address the problem of efficiently estimating where adding extra resources is highly needed in order to improve the expected performance of the resulting policy. Possible application in reinforcement learning (RL) , when real-world exploration is highly costly, concerns the detection of those areas of the state-space that need primarily to be explored in order to improve the policy. Another application concerns approximation of continuous state-space stochastic control problems using adaptive discretization techniques for which highly efficient grid points allocation is mandatory to survive high dimensionality. Maybe surprisingly these two problems can be formulated under a common framework: for a given resource allocation, which defines a belief state over possible MDPs, find where adding new resources (thus decreasing the uncertainty of some parameters -transition probabilities or rewards) will most likely increase the expected performance of the new policy. To do so, we use sampling techniques for estimating the contribution of each parameter's probability distribution function (Pdf) to the expected loss of using an approximate policy (such as the optimal policy of the most probable MDP) instead of the true (but unknown) policy.</p><p>2 0.92691463 <a title="94-lda-2" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>Author: John Shawe-Taylor, Nello Cristianini, Jaz S. Kandola</p><p>Abstract: We consider the problem of measuring the eigenvalues of a randomly drawn sample of points. We show that these values can be reliably estimated as can the sum of the tail of eigenvalues. Furthermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample. Experiments are presented that confirm the theoretical results. 1</p><p>same-paper 3 0.90582836 <a title="94-lda-3" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>Author: Shai Fine, Katya Scheinberg</p><p>Abstract: We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence. 1</p><p>4 0.54591388 <a title="94-lda-4" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>Author: Dale Schuurmans, Relu Patrascu</p><p>Abstract: We present a simple approach for computing reasonable policies for factored Markov decision processes (MDPs), when the optimal value function can be approximated by a compact linear form. Our method is based on solving a single linear program that approximates the best linear fit to the optimal value function. By applying an efficient constraint generation procedure we obtain an iterative solution method that tackles concise linear programs. This direct linear programming approach experimentally yields a significant reduction in computation time over approximate value- and policy-iteration methods (sometimes reducing several hours to a few seconds). However, the quality of the solutions produced by linear programming is weaker-usually about twice the approximation error for the same approximating class. Nevertheless, the speed advantage allows one to use larger approximation classes to achieve similar error in reasonable time. 1</p><p>5 0.543715 <a title="94-lda-5" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efﬁcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inﬂuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efﬁciently by merely observing a relatively small number of completely random trials.</p><p>6 0.53453785 <a title="94-lda-6" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>7 0.52927613 <a title="94-lda-7" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>8 0.52809793 <a title="94-lda-8" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>9 0.5101136 <a title="94-lda-9" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>10 0.50512075 <a title="94-lda-10" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>11 0.50463217 <a title="94-lda-11" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>12 0.50099558 <a title="94-lda-12" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>13 0.49264711 <a title="94-lda-13" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>14 0.47447705 <a title="94-lda-14" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>15 0.47406596 <a title="94-lda-15" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>16 0.47402042 <a title="94-lda-16" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>17 0.47086817 <a title="94-lda-17" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>18 0.46886861 <a title="94-lda-18" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>19 0.46666127 <a title="94-lda-19" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>20 0.46402293 <a title="94-lda-20" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
