<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-510" href="#">high_scalability-2009-510</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-510-html" href="http://highscalability.com//blog/2009/2/9/paper-consensus-protocols-two-phase-commit.html">html</a></p><p>Introduction: Henry Robinson has created an excellent series of articles on consensus
protocols. Henry starts with a very useful discussion of what all this talk
about consensus really means:The consensus problem is the problem of getting a
set of nodes in a distributed system to agree on something - it might be a
value, a course of action or a decision. Achieving consensus allows a
distributed system to act as a single entity, with every individual node aware
of and in agreement with the actions of the whole of the network.In this
article Henry tackles Two-Phase Commit, the protocol most databases use to
arrive at a consensus for database writes. The article is very well written
with lots of pretty and informative pictures. He did a really good job.In
conclusion we learn 2PC is very efficient, a minimal number of messages are
exchanged and latency is low. The problem is when a co-ordinator fails
availability is dramatically reduced. This is why 2PC isn't generally used on
highly distributed systems</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('consensus', 0.601), ('henry', 0.466), ('robinson', 0.171), ('exchanged', 0.165), ('articles', 0.138), ('agreement', 0.138), ('tackles', 0.136), ('problem', 0.135), ('arrive', 0.128), ('achieving', 0.11), ('article', 0.105), ('entity', 0.105), ('commit', 0.104), ('dramatically', 0.104), ('actions', 0.102), ('conclusion', 0.102), ('agree', 0.102), ('minimal', 0.099), ('aware', 0.097), ('fails', 0.096)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="510-tfidf-1" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus
protocols. Henry starts with a very useful discussion of what all this talk
about consensus really means:The consensus problem is the problem of getting a
set of nodes in a distributed system to agree on something - it might be a
value, a course of action or a decision. Achieving consensus allows a
distributed system to act as a single entity, with every individual node aware
of and in agreement with the actions of the whole of the network.In this
article Henry tackles Two-Phase Commit, the protocol most databases use to
arrive at a consensus for database writes. The article is very well written
with lots of pretty and informative pictures. He did a really good job.In
conclusion we learn 2PC is very efficient, a minimal number of messages are
exchanged and latency is low. The problem is when a co-ordinator fails
availability is dramatically reduced. This is why 2PC isn't generally used on
highly distributed systems</p><p>2 0.28863993 <a title="510-tfidf-2" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>Introduction: Update: Barbara Liskov's Turing Award, and Byzantine Fault Tolerance.Henry
Robinson has created an excellent series of articles on consensus protocols.
We already covered his2 Phase Commitarticle and he also has a3 Phase
Commitarticle showing how to handle 2PC under single node failures.But that is
not enough! 3PC works well under node failures, but fails for network
failures. So another consensus mechanism is needed that handles both network
and node failures. And that'sPaxos.Paxos correctly handles both types of
failures, but it does this by becoming inaccessible if too many components
fail. This is the "liveness" property of protocols. Paxos waits until the
faults are fixed. Read queries can be handled, but updates will be blocked
until the protocol thinks it can make forward progress.The liveness of Paxos
is primarily dependent on network stability. In a distributed heterogeneous
environment you are at risk of losing the ability to make updates. Users hate
that.breakSo when compani</p><p>3 0.19214293 <a title="510-tfidf-3" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>Introduction: Update:Streamy Explains CAP and HBase's Approach to CAP.We plan to employ
inter-cluster replication, with each cluster located in a single DC. Remote
replication will introduce some eventual consistency into the system, but each
cluster will continue to be strongly consistent.Ryan Barrett, Google App
Engine datastore lead, gave this talkTransactions Across Datacenters (and
Other Weekend Projects)at the Google I/O 2009 conference.While the talk
doesn't necessarily break new technical ground, Ryan does an excellent job
explaining and evaluating the different options you have when architecting a
system to work across multiple datacenters. This is calledmultihoming,
operating from multiple datacenters simultaneously.As multihoming is one of
the most challenging tasks in all computing, Ryan's clear and thoughtful style
comfortably leads you through the various options. On the trip you learn:The
differentmulti-homing optionsare: Backups, Master-Slave, Multi-Master, 2PC,
and Paxos. You'll als</p><p>4 0.17992176 <a title="510-tfidf-4" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written anduseful paper. It talks in detail about
experiences implementing a complex project, something we don't see very often.
They shockingly even admit that creating a working implementation of Paxos was
more difficult than just translating the pseudo code. Imagine that,
programmers aren't merely typists! I particularly like the explanation of the
Paxos algorithm and why anyone would care about it, working with disk
corruption, using leases to support simultaneous reads, using epoch numbers to
indicate a new master election, using snapshots to prevent unbounded logs,
using MultiOp to implement database transactions, how they tested the system,
and their openness with the various problems they had. A lot to learn
here.From the paper:We describe our experience building a fault-tolerant data-
base using the Paxos consensus algorithm. Despite the existing literature in
the field, building such a database proved to be non-trivial. We describe
selected algorithm</p><p>5 0.12300745 <a title="510-tfidf-5" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>Introduction: If like many humans you've found evenPaxos Made Simplea bit difficult to
understand, you might enjoy RAFT as described inIn Search of an Understandable
Consensus Algorithm by Stanford'sDiego Ongaroand John Ousterhout. The video
presentation of the paper is given byJohn Ousterhout. Both the paper and the
video are delightfully accessible.mcherm has a good summary of the paper:A
consensus algorithm is: a cluster of servers should record a series of records
("log entries") in response to requests from clients of the cluster. (It may
also take action based on those entries.) It does so in a way that guarantees
that the responses seen by clients of the cluster will be consistent EVEN in
the face of servers crashing in unpredictable ways (but not loosing data that
was synched to disk), and networks introducing unpredictable delays or
communication blockages.Here's what Raft does. First, it elects a leader, then
the leader records the master version of the log, telling other cluster
servers w</p><p>6 0.10083438 <a title="510-tfidf-6" href="../high_scalability-2013/high_scalability-2013-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_4th%2C_2013.html">1527 high scalability-2013-10-04-Stuff The Internet Says On Scalability For October 4th, 2013</a></p>
<p>7 0.096718252 <a title="510-tfidf-7" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>8 0.083237499 <a title="510-tfidf-8" href="../high_scalability-2014/high_scalability-2014-05-09-Stuff_The_Internet_Says_On_Scalability_For_May_9th%2C_2014.html">1645 high scalability-2014-05-09-Stuff The Internet Says On Scalability For May 9th, 2014</a></p>
<p>9 0.081948407 <a title="510-tfidf-9" href="../high_scalability-2009/high_scalability-2009-11-24-Hot_Scalability_Links_for_Nov_24_2009.html">744 high scalability-2009-11-24-Hot Scalability Links for Nov 24 2009</a></p>
<p>10 0.077659316 <a title="510-tfidf-10" href="../high_scalability-2010/high_scalability-2010-10-14-I%2C_Cloud.html">919 high scalability-2010-10-14-I, Cloud</a></p>
<p>11 0.076708183 <a title="510-tfidf-11" href="../high_scalability-2010/high_scalability-2010-10-28-Notes_from_A_NOSQL_Evening_in_Palo_Alto_.html">931 high scalability-2010-10-28-Notes from A NOSQL Evening in Palo Alto </a></p>
<p>12 0.075871788 <a title="510-tfidf-12" href="../high_scalability-2014/high_scalability-2014-03-07-Stuff_The_Internet_Says_On_Scalability_For_March_7th%2C_2014.html">1607 high scalability-2014-03-07-Stuff The Internet Says On Scalability For March 7th, 2014</a></p>
<p>13 0.075271666 <a title="510-tfidf-13" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>14 0.07228744 <a title="510-tfidf-14" href="../high_scalability-2012/high_scalability-2012-06-08-Stuff_The_Internet_Says_On_Scalability_For_June_8%2C_2012.html">1261 high scalability-2012-06-08-Stuff The Internet Says On Scalability For June 8, 2012</a></p>
<p>15 0.070906304 <a title="510-tfidf-15" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>16 0.069985829 <a title="510-tfidf-16" href="../high_scalability-2010/high_scalability-2010-12-16-7_Design_Patterns_for_Almost-infinite_Scalability.html">958 high scalability-2010-12-16-7 Design Patterns for Almost-infinite Scalability</a></p>
<p>17 0.069194309 <a title="510-tfidf-17" href="../high_scalability-2012/high_scalability-2012-01-30-37signals_Still_Happily_Scaling_on_Moore_RAM_and_SSDs.html">1183 high scalability-2012-01-30-37signals Still Happily Scaling on Moore RAM and SSDs</a></p>
<p>18 0.068101414 <a title="510-tfidf-18" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>19 0.067295291 <a title="510-tfidf-19" href="../high_scalability-2013/high_scalability-2013-06-21-Stuff_The_Internet_Says_On_Scalability_For_June_21%2C_2013.html">1479 high scalability-2013-06-21-Stuff The Internet Says On Scalability For June 21, 2013</a></p>
<p>20 0.066910423 <a title="510-tfidf-20" href="../high_scalability-2013/high_scalability-2013-09-06-Stuff_The_Internet_Says_On_Scalability_For_September_6%2C_2013.html">1513 high scalability-2013-09-06-Stuff The Internet Says On Scalability For September 6, 2013</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.094), (1, 0.067), (2, -0.005), (3, 0.033), (4, 0.016), (5, 0.049), (6, 0.008), (7, 0.018), (8, -0.027), (9, -0.015), (10, 0.01), (11, 0.023), (12, -0.038), (13, -0.03), (14, 0.041), (15, 0.002), (16, 0.037), (17, -0.016), (18, -0.01), (19, -0.01), (20, 0.051), (21, 0.012), (22, -0.025), (23, 0.012), (24, -0.064), (25, 0.006), (26, 0.067), (27, 0.039), (28, -0.021), (29, -0.025), (30, -0.012), (31, -0.016), (32, -0.02), (33, 0.006), (34, -0.018), (35, -0.049), (36, 0.021), (37, 0.013), (38, 0.004), (39, -0.02), (40, -0.023), (41, -0.005), (42, -0.024), (43, 0.02), (44, -0.031), (45, 0.003), (46, 0.031), (47, 0.055), (48, -0.04), (49, -0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95273292 <a title="510-lsi-1" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus
protocols. Henry starts with a very useful discussion of what all this talk
about consensus really means:The consensus problem is the problem of getting a
set of nodes in a distributed system to agree on something - it might be a
value, a course of action or a decision. Achieving consensus allows a
distributed system to act as a single entity, with every individual node aware
of and in agreement with the actions of the whole of the network.In this
article Henry tackles Two-Phase Commit, the protocol most databases use to
arrive at a consensus for database writes. The article is very well written
with lots of pretty and informative pictures. He did a really good job.In
conclusion we learn 2PC is very efficient, a minimal number of messages are
exchanged and latency is low. The problem is when a co-ordinator fails
availability is dramatically reduced. This is why 2PC isn't generally used on
highly distributed systems</p><p>2 0.86687905 <a title="510-lsi-2" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>Introduction: Update: Barbara Liskov's Turing Award, and Byzantine Fault Tolerance.Henry
Robinson has created an excellent series of articles on consensus protocols.
We already covered his2 Phase Commitarticle and he also has a3 Phase
Commitarticle showing how to handle 2PC under single node failures.But that is
not enough! 3PC works well under node failures, but fails for network
failures. So another consensus mechanism is needed that handles both network
and node failures. And that'sPaxos.Paxos correctly handles both types of
failures, but it does this by becoming inaccessible if too many components
fail. This is the "liveness" property of protocols. Paxos waits until the
faults are fixed. Read queries can be handled, but updates will be blocked
until the protocol thinks it can make forward progress.The liveness of Paxos
is primarily dependent on network stability. In a distributed heterogeneous
environment you are at risk of losing the ability to make updates. Users hate
that.breakSo when compani</p><p>3 0.74262482 <a title="510-lsi-3" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>Introduction: Can you have your ACID cake and eat your distributed database too? Yes
explains Daniel Abadi, Assistant Professor of Computer Science at Yale
University, in an epic post,The problems with ACID, and how to fix them
without going NoSQL, coauthored with Alexander Thomson, on their paperThe Case
for Determinism in Database Systems. We've already seenVoltDBoffer the best of
both worlds, this sounds like a completely different approach.The solution,
they propose, is: ...an architecture and execution model that avoids deadlock,
copes with failures without aborting transactions, and achieves high
concurrency. The paper contains full details, but the basic idea is to use
ordered locking coupled with optimistic lock location prediction, while
exploiting deterministic systems' nice replication properties in the case of
failures.The problem they are trying to solve is:In our opinion, the NoSQL
decision to give up on ACID is the lazy solution to these scalability and
replication issues. Responsibil</p><p>4 0.71047574 <a title="510-lsi-4" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>Introduction: When building a system on top of a set of wildly uncooperative and unruly
computers you have knowledge problems: knowing when other nodes are dead;
knowing when nodes become alive; getting information about other nodes so you
can make local decisions, like knowing which node should handle a request
based on a scheme for assigning nodes to a certain range of users; learning
about new configuration data; agreeing on data values; and so on.How do you
solve these problems? A common centralized approach is to use a database and
all nodes query it for information. Obvious availability and performance
issues for large distributed clusters. Another approach is to use Paxos, a
protocol for solving consensus in a network to maintain strict consistency
requirements for small groups of unreliable processes. Not practical when
larger number of nodes are involved.So what's the super cool decentralized way
to bring order to large clusters?Gossip protocols, which maintain relaxed
consistency requireme</p><p>5 0.71045274 <a title="510-lsi-5" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>Introduction: Teams fromPrincetonand CMU areworking togetherto solve one of the most
difficult problems in the repertoire: scalable geo-distributed data stores.
Major companies like Google and Facebook have been working on multiple
datacenter database functionality for some time, but there's still a general
lack of available systems that work for complex data scenarios.The ideas in
this paper--Don't Settle for Eventual: Scalable Causal Consistency for Wide-
Area Storage with COPS--are different. It's not another eventually consistent
system, or a traditional transaction oriented system, or a replication based
system, or a system that punts on the issue. It's something new, a causally
consistent system that achievesALPSsystem properties. Move over CAP, NoSQL,
etc, we have another acronym: ALPS - Available (operations always complete
successfully), Low-latency (operations complete quickly (single digit
milliseconds)), Partition-tolerant (operates with a partition), and Scalable
(just add more servers</p><p>6 0.7049576 <a title="510-lsi-6" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>7 0.70222694 <a title="510-lsi-7" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>8 0.69950449 <a title="510-lsi-8" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>9 0.6951316 <a title="510-lsi-9" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>10 0.68143493 <a title="510-lsi-10" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>11 0.66406494 <a title="510-lsi-11" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>12 0.66266328 <a title="510-lsi-12" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>13 0.66168392 <a title="510-lsi-13" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>14 0.66094017 <a title="510-lsi-14" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>15 0.6548149 <a title="510-lsi-15" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>16 0.6544528 <a title="510-lsi-16" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>17 0.6531502 <a title="510-lsi-17" href="../high_scalability-2010/high_scalability-2010-12-16-7_Design_Patterns_for_Almost-infinite_Scalability.html">958 high scalability-2010-12-16-7 Design Patterns for Almost-infinite Scalability</a></p>
<p>18 0.65000391 <a title="510-lsi-18" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>19 0.64828396 <a title="510-lsi-19" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>20 0.64462996 <a title="510-lsi-20" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.075), (2, 0.31), (10, 0.064), (40, 0.016), (51, 0.278), (61, 0.103), (79, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91616136 <a title="510-lda-1" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus
protocols. Henry starts with a very useful discussion of what all this talk
about consensus really means:The consensus problem is the problem of getting a
set of nodes in a distributed system to agree on something - it might be a
value, a course of action or a decision. Achieving consensus allows a
distributed system to act as a single entity, with every individual node aware
of and in agreement with the actions of the whole of the network.In this
article Henry tackles Two-Phase Commit, the protocol most databases use to
arrive at a consensus for database writes. The article is very well written
with lots of pretty and informative pictures. He did a really good job.In
conclusion we learn 2PC is very efficient, a minimal number of messages are
exchanged and latency is low. The problem is when a co-ordinator fails
availability is dramatically reduced. This is why 2PC isn't generally used on
highly distributed systems</p><p>2 0.89660227 <a title="510-lda-2" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data
( arround 44 gigs right now ).For some time I have been using rsync to keep
all the content equal on all servers, but the amount of data has been growing,
and rsync takes a few too much time to "compare" all data from source to
destination, and create a lot of I/O.I have been taking a look at MogileFS, it
seems a good and reliable option, but as the fuse module is not finished, we
should have to rewrite all our apps, and its not an option atm.Any ideas?I
just want a "real time, non resource-hungry" solution alternative for rsync.
If I get more features on the way, then they are welcome :)Why I prefer to use
a Distributed File System instead of using NAS + NFS?- I need 2 NAS, if I dont
want a point of failure, and NAS hard is expensive.- Non-shared hardware, all
server has their own local disks.- As files are replicated, I can save a lot
of money, RAID is not a MUST.Thnx in advance for your help and sorry for</p><p>3 0.88650513 <a title="510-lda-3" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>Introduction: Our last article on Disqus: How Disqus Went Realtime With 165K Messages Per
Second And Less Than .2 Seconds Latency, was a little out of date, but the
folks at Disqus have been busy implementing, not talking, so we don't know a
lot about what they are doing now, but we do have a short update in C1MM and
NGINXby John Watson and an article Trying out this Go thing.So Disqus has
grown a bit:1.3 billion unique visitors10 billion page views500 million users
engaged in discussions3 million communities25 million commentsThey are still
all about realtime, but Go replaced Python in their Realtime system:Original
Realtime backend was written in a pretty lightweight Python + gevent.The
realtime service is a hybrid of CPU intensive tasks + lots of network IO.
Gevent was handling the network IO without an issue, but at higher contention,
the CPU was choking everything. Switching over to Go removed that contention,
which was the primary issue that was being seen.Still runs on 5 machines Nginx
machin</p><p>4 0.8734532 <a title="510-lda-4" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>Introduction: In articleBuilding Super-Scalable Web Systems with RESTUdi Dahan tells an
interesting story of how they made a weather reporting system scale for over
10 million users. So many users hitting their weather database didn't scale.
Caching in a straightforward way wouldn't work because weather is obviously
local. Caching all local reports would bring the entire database into memory,
which would work for some companies, but wasn't cost efficient for them.So in
typical REST fashion they turned locations into URIs. For example:
http://weather.myclient.com/UK/London. This allows the weather information to
be cached by intermediaries instead of hitting their servers. Hopefully for
each location their servers will be hit a few times and then the caches will
be hit until expiry.breakIn order to send users directly to the correct
location an IP location check is performed on login and stored in a cookie.
The lookup is done once and from then on out a GET is performed directly on
the resource. Ther</p><p>5 0.87017506 <a title="510-lda-5" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to
Mike Swift, in Facebook gets ready for New Year's Eve, we get a little insight
as to their method for the madness, nothing really detailed, but still
interesting.Problem SetupFacebook expects tha one billion+ photos will be
shared on New Year's eve.Facebook's 800 million users are scattered around the
world. Three quarters live outside the US. Each user is linked to an average
of 130 friends.Photos and posts must appear in less than a second. Opening a
homepage requires executing requests on a 100 different servers, and those
requests have to be ranked, sorted, and privacy-checked, and then
rendered.Different events put different stresses on different parts of
Facebook. Photo and Video Uploads - Holidays require hundreds of terabytes of
capacity News Feed - News events like big sports events and the death of Steve
Jobs drive user status updatesCoping StrategiesTry to predictthe surge in
traffic. Run checkson h</p><p>6 0.84675127 <a title="510-lda-6" href="../high_scalability-2011/high_scalability-2011-10-28-Stuff_The_Internet_Says_On_Scalability_For_October_28%2C_2011.html">1134 high scalability-2011-10-28-Stuff The Internet Says On Scalability For October 28, 2011</a></p>
<p>7 0.83724821 <a title="510-lda-7" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>8 0.83412087 <a title="510-lda-8" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>9 0.82302243 <a title="510-lda-9" href="../high_scalability-2010/high_scalability-2010-04-30-Behind_the_scenes_of_an_online_marketplace.html">818 high scalability-2010-04-30-Behind the scenes of an online marketplace</a></p>
<p>10 0.81871516 <a title="510-lda-10" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>11 0.79914963 <a title="510-lda-11" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>12 0.77210677 <a title="510-lda-12" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>13 0.76371437 <a title="510-lda-13" href="../high_scalability-2009/high_scalability-2009-06-10-Paper%3A_Graph_Databases_and_the_Future_of_Large-Scale_Knowledge_Management.html">626 high scalability-2009-06-10-Paper: Graph Databases and the Future of Large-Scale Knowledge Management</a></p>
<p>14 0.76162881 <a title="510-lda-14" href="../high_scalability-2007/high_scalability-2007-10-03-Save_on_a_Load_Balancer_By_Using_Client_Side_Load_Balancing.html">109 high scalability-2007-10-03-Save on a Load Balancer By Using Client Side Load Balancing</a></p>
<p>15 0.76039779 <a title="510-lda-15" href="../high_scalability-2008/high_scalability-2008-05-28-Job_queue_and_search_engine.html">332 high scalability-2008-05-28-Job queue and search engine</a></p>
<p>16 0.75934529 <a title="510-lda-16" href="../high_scalability-2010/high_scalability-2010-02-05-High_Availability_Principle_%3A_Concurrency_Control.html">772 high scalability-2010-02-05-High Availability Principle : Concurrency Control</a></p>
<p>17 0.75902784 <a title="510-lda-17" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<p>18 0.75635386 <a title="510-lda-18" href="../high_scalability-2013/high_scalability-2013-01-30-Better_Browser_Caching_is_More_Important_than_No_Javascript_or_Fast_Networks_for_HTTP_Performance.html">1396 high scalability-2013-01-30-Better Browser Caching is More Important than No Javascript or Fast Networks for HTTP Performance</a></p>
<p>19 0.75432152 <a title="510-lda-19" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>20 0.75368643 <a title="510-lda-20" href="../high_scalability-2011/high_scalability-2011-09-27-Use_Instance_Caches_to_Save_Money%3A_Latency_%3D%3D_%24%24%24.html">1126 high scalability-2011-09-27-Use Instance Caches to Save Money: Latency == $$$</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
