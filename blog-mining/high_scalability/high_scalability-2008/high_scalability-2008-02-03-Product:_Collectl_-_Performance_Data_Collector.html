<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-237" href="#">high_scalability-2008-237</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-237-html" href="http://highscalability.com//blog/2008/2/3/product-collectl-performance-data-collector.html">html</a></p><p>Introduction: From theirwebsite:There are a number of times in which you find yourself
needing performance data. These can include benchmarking, monitoring a
system's general heath or trying to determine what your system was doing at
some time in the past. Sometimes you just want to know what the system is
doing right now. Depending on what you're doing, you often end up using
different tools, each designed to for that specific situation. Features
include:You are be able to run with non-integral sampling
intervals.Collectluses very little CPU. In fact it has been measured to use
<0.1% when run as a daemon using the default sampling interval of 60 seconds
for process and slab data and 10 seconds for everything else.Brief, verbose,
and plot formats are supported.You can report aggregated performance numbers
on many devices such as CPUs, Disks, interconnects such as Infiniband or
Quadrics, Networks or even Lustre file systems.Collectl will align its
sampling on integral second boundaries.Supports proce</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sampling', 0.316), ('quadrics', 0.265), ('interactively', 0.229), ('collectl', 0.222), ('lustre', 0.211), ('slab', 0.202), ('formats', 0.181), ('infiniband', 0.173), ('daemon', 0.161), ('tools', 0.137), ('slabs', 0.133), ('theirwebsite', 0.133), ('default', 0.13), ('include', 0.129), ('output', 0.129), ('format', 0.126), ('inodes', 0.119), ('inter', 0.119), ('verbose', 0.119), ('external', 0.113)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="237-tfidf-1" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>Introduction: From theirwebsite:There are a number of times in which you find yourself
needing performance data. These can include benchmarking, monitoring a
system's general heath or trying to determine what your system was doing at
some time in the past. Sometimes you just want to know what the system is
doing right now. Depending on what you're doing, you often end up using
different tools, each designed to for that specific situation. Features
include:You are be able to run with non-integral sampling
intervals.Collectluses very little CPU. In fact it has been measured to use
<0.1% when run as a daemon using the default sampling interval of 60 seconds
for process and slab data and 10 seconds for everything else.Brief, verbose,
and plot formats are supported.You can report aggregated performance numbers
on many devices such as CPUs, Disks, interconnects such as Infiniband or
Quadrics, Networks or even Lustre file systems.Collectl will align its
sampling on integral second boundaries.Supports proce</p><p>2 0.36258495 <a title="237-tfidf-2" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>Introduction: Todd had originally posted an entry oncollectlhere atCollectl - Performance
Data Collector. Collectl collects real-time data from a large number of
subsystems like buddyinfo, cpu, disk, inodes, infiniband, lustre, memory,
network, nfs, processes, quadrics, slabs, sockets and tcp, all using one tool
and in one consistent format.Since then a lot has happened.  It's now part of
both Fedora and Debian distros, not to mention several others. There has also
been a pretty good summary written up byJoe Brockmeier. It's also pretty well
documented (I like to think) onsourceforge. There have also been a few blog
postings by Martin Bachon his blog.Anyhow, awhile back I released a new
version of collectl-utils and gave a complete face-lift to one of the
utilities, colmux, which is a collectl multiplexor.  This tool has the ability
to run collectl on multiple systems, which in turn send all their output back
to colmux.  Colmux then sorts the output on a user-specified column and
reports the 'top-n'</p><p>3 0.23210011 <a title="237-tfidf-3" href="../high_scalability-2009/high_scalability-2009-04-06-How_do_you_monitor_the_performance_of_your_cluster%3F.html">558 high scalability-2009-04-06-How do you monitor the performance of your cluster?</a></p>
<p>Introduction: I had posted a note the other day about collectl and its ganglia interface but
perhaps I wasn't provocative enough to get any responses so let me ask it a
different way, specifically how do people monitor their clusters and more
importantly how often? Do you monitor to get a general sense of what the
system is doing OR do you monitor with the expectation that when something
goes wrong you'll have enough data to diagnose the problem? Or both? I suspect
both...Many cluster-based monitoring tools tend to have a data collection
daemon running on each target node which periodically sends data to some
central management station. That machine typically writes the data to some
database from which it can then extract historical plots. Some even put up
graphics in real-time.From my experience working with large clusters - and I'm
talking either many hundreds or even 1000s of nodes, most have to limit both
the amount of data they manage centrally as well as the frequency that they
collect it, oth</p><p>4 0.18584603 <a title="237-tfidf-4" href="../high_scalability-2008/high_scalability-2008-04-29-Strategy%3A_Sample_to_Reduce_Data_Set.html">311 high scalability-2008-04-29-Strategy: Sample to Reduce Data Set</a></p>
<p>Introduction: Update:Arjenlinks to videoSupporting Scalable Online Statistical
Processingwhich shows"rather than doing complete aggregates, use statistical
sampling to provide a reasonable estimate (unbiased guess) of the result."When
you have a lot of data,samplingallows you to draw conclusions from a much
smaller amount of data. That's why sampling is a scalability solution. If you
don't have to process all your data to get the information you need then
you've made the problem smaller and you'll need fewer resources and you'll get
more timely results.breakSampling is not useful when you need a complete list
that matches a specific criteria. If you need to know the exact set of people
who bought a car in the last week then sampling won't help.But, if you want to
know many people bought a car then you could take a sample and then create
estimate of the full data-set. The difference is you won't really know the
exact car count. You'll have a confidence interval saying how confident you
are in your es</p><p>5 0.15688354 <a title="237-tfidf-5" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>Introduction: Lustre速is a scalable, secure, robust, highly-available cluster file system. It
is designed, developed and maintained by Cluster File Systems, Inc.The central
goal is the development of a next-generation cluster file system which can
serve clusters with 10,000's of nodes, provide petabytes of storage, and move
100's of GB/sec with state-of-the-art security and management
infrastructure.Lustre runs on many of the largest Linux clusters in the world,
and is included by CFS's partners as a core component of their cluster
offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1
supercomputers). Today's users have also demonstrated that Lustre scales down
as well as it scales up, and runs in production on clusters as small as 4 and
as large as 25,000 nodes.The latest version of Lustre is always available from
Cluster File Systems, Inc. Public Open Source releases of Lustre are available
under the GNU General Public License. These releases are found here, and are
used in produ</p><p>6 0.15198992 <a title="237-tfidf-6" href="../high_scalability-2008/high_scalability-2008-10-14-Implementing_the_Lustre_File_System_with_Sun_Storage%3A_High_Performance_Storage_for_High_Performance_Computing.html">411 high scalability-2008-10-14-Implementing the Lustre File System with Sun Storage: High Performance Storage for High Performance Computing</a></p>
<p>7 0.12232403 <a title="237-tfidf-7" href="../high_scalability-2010/high_scalability-2010-04-27-Paper%3A__Dapper%2C_Google%27s_Large-Scale_Distributed_Systems_Tracing_Infrastructure.html">815 high scalability-2010-04-27-Paper:  Dapper, Google's Large-Scale Distributed Systems Tracing Infrastructure</a></p>
<p>8 0.11759448 <a title="237-tfidf-8" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>9 0.11182044 <a title="237-tfidf-9" href="../high_scalability-2009/high_scalability-2009-10-09-Have_you_collectl%27d_yet%3F__If_not%2C_maybe_collectl-utils_will_make_it_easier_to_do_so.html">719 high scalability-2009-10-09-Have you collectl'd yet?  If not, maybe collectl-utils will make it easier to do so</a></p>
<p>10 0.11125783 <a title="237-tfidf-10" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>11 0.10310745 <a title="237-tfidf-11" href="../high_scalability-2008/high_scalability-2008-04-29-High_performance_file_server.html">310 high scalability-2008-04-29-High performance file server</a></p>
<p>12 0.096037902 <a title="237-tfidf-12" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>13 0.08750996 <a title="237-tfidf-13" href="../high_scalability-2010/high_scalability-2010-08-16-Scaling_an_AWS_infrastructure_-_Tools_and_Patterns.html">881 high scalability-2010-08-16-Scaling an AWS infrastructure - Tools and Patterns</a></p>
<p>14 0.087404042 <a title="237-tfidf-14" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>15 0.078183755 <a title="237-tfidf-15" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>16 0.077712692 <a title="237-tfidf-16" href="../high_scalability-2007/high_scalability-2007-07-26-Product%3A_AWStats_a_Log_Analyzer.html">30 high scalability-2007-07-26-Product: AWStats a Log Analyzer</a></p>
<p>17 0.077266566 <a title="237-tfidf-17" href="../high_scalability-2012/high_scalability-2012-07-25-Sponsored_Post%3A_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_ground%28ctrl%29%2C_New_Relic%2C_NetDNA%2C_Torbit%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1290 high scalability-2012-07-25-Sponsored Post: ElasticHosts, Atlantic.Net, ScaleOut, ground(ctrl), New Relic, NetDNA, Torbit, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>18 0.075781785 <a title="237-tfidf-18" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>19 0.075646654 <a title="237-tfidf-19" href="../high_scalability-2012/high_scalability-2012-07-10-Sponsored_Post%3A_New_Relic%2C_NetDNA%2C_Torbit%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1280 high scalability-2012-07-10-Sponsored Post: New Relic, NetDNA, Torbit, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>20 0.075332403 <a title="237-tfidf-20" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, 0.034), (2, -0.019), (3, -0.038), (4, -0.021), (5, 0.028), (6, 0.1), (7, 0.038), (8, -0.01), (9, 0.006), (10, 0.005), (11, -0.006), (12, 0.049), (13, 0.0), (14, 0.041), (15, 0.001), (16, -0.006), (17, 0.013), (18, -0.05), (19, 0.033), (20, 0.024), (21, -0.043), (22, -0.008), (23, 0.08), (24, 0.019), (25, -0.005), (26, -0.015), (27, -0.003), (28, -0.08), (29, -0.025), (30, -0.034), (31, -0.046), (32, 0.05), (33, 0.007), (34, -0.022), (35, 0.037), (36, -0.016), (37, -0.066), (38, -0.043), (39, 0.032), (40, -0.072), (41, -0.032), (42, 0.01), (43, 0.02), (44, 0.001), (45, 0.104), (46, -0.032), (47, -0.023), (48, 0.029), (49, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94444543 <a title="237-lsi-1" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>Introduction: From theirwebsite:There are a number of times in which you find yourself
needing performance data. These can include benchmarking, monitoring a
system's general heath or trying to determine what your system was doing at
some time in the past. Sometimes you just want to know what the system is
doing right now. Depending on what you're doing, you often end up using
different tools, each designed to for that specific situation. Features
include:You are be able to run with non-integral sampling
intervals.Collectluses very little CPU. In fact it has been measured to use
<0.1% when run as a daemon using the default sampling interval of 60 seconds
for process and slab data and 10 seconds for everything else.Brief, verbose,
and plot formats are supported.You can report aggregated performance numbers
on many devices such as CPUs, Disks, interconnects such as Infiniband or
Quadrics, Networks or even Lustre file systems.Collectl will align its
sampling on integral second boundaries.Supports proce</p><p>2 0.84932435 <a title="237-lsi-2" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>Introduction: Todd had originally posted an entry oncollectlhere atCollectl - Performance
Data Collector. Collectl collects real-time data from a large number of
subsystems like buddyinfo, cpu, disk, inodes, infiniband, lustre, memory,
network, nfs, processes, quadrics, slabs, sockets and tcp, all using one tool
and in one consistent format.Since then a lot has happened.  It's now part of
both Fedora and Debian distros, not to mention several others. There has also
been a pretty good summary written up byJoe Brockmeier. It's also pretty well
documented (I like to think) onsourceforge. There have also been a few blog
postings by Martin Bachon his blog.Anyhow, awhile back I released a new
version of collectl-utils and gave a complete face-lift to one of the
utilities, colmux, which is a collectl multiplexor.  This tool has the ability
to run collectl on multiple systems, which in turn send all their output back
to colmux.  Colmux then sorts the output on a user-specified column and
reports the 'top-n'</p><p>3 0.70166045 <a title="237-lsi-3" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>Introduction: Lustre速is a scalable, secure, robust, highly-available cluster file system. It
is designed, developed and maintained by Cluster File Systems, Inc.The central
goal is the development of a next-generation cluster file system which can
serve clusters with 10,000's of nodes, provide petabytes of storage, and move
100's of GB/sec with state-of-the-art security and management
infrastructure.Lustre runs on many of the largest Linux clusters in the world,
and is included by CFS's partners as a core component of their cluster
offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1
supercomputers). Today's users have also demonstrated that Lustre scales down
as well as it scales up, and runs in production on clusters as small as 4 and
as large as 25,000 nodes.The latest version of Lustre is always available from
Cluster File Systems, Inc. Public Open Source releases of Lustre are available
under the GNU General Public License. These releases are found here, and are
used in produ</p><p>4 0.69122934 <a title="237-lsi-4" href="../high_scalability-2009/high_scalability-2009-04-06-How_do_you_monitor_the_performance_of_your_cluster%3F.html">558 high scalability-2009-04-06-How do you monitor the performance of your cluster?</a></p>
<p>Introduction: I had posted a note the other day about collectl and its ganglia interface but
perhaps I wasn't provocative enough to get any responses so let me ask it a
different way, specifically how do people monitor their clusters and more
importantly how often? Do you monitor to get a general sense of what the
system is doing OR do you monitor with the expectation that when something
goes wrong you'll have enough data to diagnose the problem? Or both? I suspect
both...Many cluster-based monitoring tools tend to have a data collection
daemon running on each target node which periodically sends data to some
central management station. That machine typically writes the data to some
database from which it can then extract historical plots. Some even put up
graphics in real-time.From my experience working with large clusters - and I'm
talking either many hundreds or even 1000s of nodes, most have to limit both
the amount of data they manage centrally as well as the frequency that they
collect it, oth</p><p>5 0.67168266 <a title="237-lsi-5" href="../high_scalability-2013/high_scalability-2013-02-19-Puppet_monitoring%3A_how_to_monitor_the_success_or_failure_of_Puppet_runs__.html">1408 high scalability-2013-02-19-Puppet monitoring: how to monitor the success or failure of Puppet runs  </a></p>
<p>Introduction: This is a guest post by LogicMonitor's Director of Tech Ops,Jesse Aukeman,
about the different ways they're monitoring the success or failure of Puppet
runs.If you are like us, you are running some type of linux configuration
management tool. The value of centralized configuration and deployment is well
known and hard to overstate. Puppetis our tool of choice. It is powerful and
works well for us, except when things don't go as planned. Failures of puppet
can be innocuous and cosmetic, or they can cause production issues, for
example when crucial updates do not get properly propagated.Why?In the most
innocuous cases, the puppet agent craps out (we run puppet agent via cron). As
nice as puppet is, we still need to goose it from time to time to get past
some sort of network or host resource issue. A more dangerous case is when an
administrator temporarily disables puppet runs on a host in order to perform
some test or administrative task and then forgets to reenable it. In either
case it</p><p>6 0.65920401 <a title="237-lsi-6" href="../high_scalability-2007/high_scalability-2007-12-31-Product%3A_collectd.html">197 high scalability-2007-12-31-Product: collectd</a></p>
<p>7 0.64486045 <a title="237-lsi-7" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_FAI_-_Fully_Automatic_Installation.html">272 high scalability-2008-03-08-Product: FAI - Fully Automatic Installation</a></p>
<p>8 0.64429247 <a title="237-lsi-8" href="../high_scalability-2012/high_scalability-2012-04-18-Ansible_-__A_Simple_Model-Driven_Configuration_Management_and_Command_Execution_Framework.html">1230 high scalability-2012-04-18-Ansible -  A Simple Model-Driven Configuration Management and Command Execution Framework</a></p>
<p>9 0.64197063 <a title="237-lsi-9" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>10 0.6300838 <a title="237-lsi-10" href="../high_scalability-2009/high_scalability-2009-10-09-Have_you_collectl%27d_yet%3F__If_not%2C_maybe_collectl-utils_will_make_it_easier_to_do_so.html">719 high scalability-2009-10-09-Have you collectl'd yet?  If not, maybe collectl-utils will make it easier to do so</a></p>
<p>11 0.62751114 <a title="237-lsi-11" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>12 0.61427969 <a title="237-lsi-12" href="../high_scalability-2014/high_scalability-2014-03-05-10_Things_You_Should_Know_About_Running_MongoDB_at_Scale.html">1606 high scalability-2014-03-05-10 Things You Should Know About Running MongoDB at Scale</a></p>
<p>13 0.61133313 <a title="237-lsi-13" href="../high_scalability-2009/high_scalability-2009-01-08-file_synchronization_solutions.html">488 high scalability-2009-01-08-file synchronization solutions</a></p>
<p>14 0.60672742 <a title="237-lsi-14" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>15 0.59859413 <a title="237-lsi-15" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>16 0.59729862 <a title="237-lsi-16" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>17 0.59453547 <a title="237-lsi-17" href="../high_scalability-2013/high_scalability-2013-01-14-MongoDB_and_GridFS_for_Inter_and_Intra_Datacenter_Data_Replication_.html">1386 high scalability-2013-01-14-MongoDB and GridFS for Inter and Intra Datacenter Data Replication </a></p>
<p>18 0.59332538 <a title="237-lsi-18" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>19 0.59242755 <a title="237-lsi-19" href="../high_scalability-2008/high_scalability-2008-05-25-Product%3A_Condor__-_Compute_Intensive_Workload_Management.html">326 high scalability-2008-05-25-Product: Condor  - Compute Intensive Workload Management</a></p>
<p>20 0.59069419 <a title="237-lsi-20" href="../high_scalability-2008/high_scalability-2008-04-02-Product%3A_Supervisor_-__Monitor_and_Control_Your_Processes.html">295 high scalability-2008-04-02-Product: Supervisor -  Monitor and Control Your Processes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.133), (2, 0.246), (30, 0.078), (50, 0.195), (56, 0.013), (61, 0.052), (84, 0.073), (85, 0.061), (94, 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91984022 <a title="237-lda-1" href="../high_scalability-2010/high_scalability-2010-04-13-Strategy%3A_Saving_Your_Butt_With_Deferred_Deletes.html">809 high scalability-2010-04-13-Strategy: Saving Your Butt With Deferred Deletes</a></p>
<p>Introduction: Deferred Deletesis a technique where deleted items are marked as deleted but
not garbage collected until some days or preferably weeks later. James
Hamilton talks describes this strategy in his classicOn Designing and
Deploying Internet-Scale Services:Never delete anything. Just mark it deleted.
When new data comes in, record the requests on the way. Keep a rolling two
week (or more) history of all changes to help recover from software or
administrative errors. If someone makes a mistake and forgets the where clause
on a delete statement (it has happened before and it will again), all logical
copies of the data are deleted. Neither RAID nor mirroring can protect against
this form of error. The ability to recover the data can make the difference
between a highly embarrassing issue or a minor, barely noticeable glitch. For
those systems already doing off-line backups, this additional record of data
coming into the service only needs to be since the last backup. But, being
cautious, we re</p><p>same-blog 2 0.89461654 <a title="237-lda-2" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>Introduction: From theirwebsite:There are a number of times in which you find yourself
needing performance data. These can include benchmarking, monitoring a
system's general heath or trying to determine what your system was doing at
some time in the past. Sometimes you just want to know what the system is
doing right now. Depending on what you're doing, you often end up using
different tools, each designed to for that specific situation. Features
include:You are be able to run with non-integral sampling
intervals.Collectluses very little CPU. In fact it has been measured to use
<0.1% when run as a daemon using the default sampling interval of 60 seconds
for process and slab data and 10 seconds for everything else.Brief, verbose,
and plot formats are supported.You can report aggregated performance numbers
on many devices such as CPUs, Disks, interconnects such as Infiniband or
Quadrics, Networks or even Lustre file systems.Collectl will align its
sampling on integral second boundaries.Supports proce</p><p>3 0.87376332 <a title="237-lda-3" href="../high_scalability-2009/high_scalability-2009-11-26-What_I%27m_Thankful_For_on_Thanksgiving.html">747 high scalability-2009-11-26-What I'm Thankful For on Thanksgiving</a></p>
<p>Introduction: I try to keep this blog targeted and on topic. So even though I may be
thankful for thesongof the tinniestsparrowatsunrise, I'll save you from all
that. It's hard to tie scalability and thegiving of thankstogether, especially
as it sometimes occurs to me that this blog may be a self-indulgent waste of
time. But I think I found a sentiment in A New THEORY of AWESOMENESS and
MIRACLES byJames Bridlethat manages to marry the topic of this blog and giving
thanks meaningfully together:I distrust commercial definitions of innovation,
and particularly of awesomeness. It's an overused term. When I think of
awesomeness, I want something awe-inspiring, vast and mind-expanding.So I
started thinking about things that I think are awesome, or miraculous, and for
me, it kept coming back to scale and complexity.We're not actually very good
about thinking about scale and complexity in real terms, so we have to use
metaphors and examples. Douglas Adams writes somewhere about how big the
Hitchhiker's Guid</p><p>4 0.8508541 <a title="237-lda-4" href="../high_scalability-2009/high_scalability-2009-04-29-Presentations%3A_MySQL_Conference_%26_Expo_2009.html">586 high scalability-2009-04-29-Presentations: MySQL Conference & Expo 2009</a></p>
<p>Introduction: The Presentations of theMySQL Conference & Expo 2009held April 20-23 in Santa
Clara is available on the above link.They include:Beginner's Guide to Website
Performance with MySQL and memcached by Adam DonnisonCalpont: Open Source
Columnar Storage Engine for Scalable MySQL DW by Jim TommaneyCreating Quick
and Powerful Web Applications with MySQL, GlassFish, and NetBeans by Arun
GuptaDeep-inspecting MySQL with DTrace by Domas MituzasDistributed Innodb
Caching with memcached by Matthew Yonkovit and Yves TrudeauImproving
Performance by Running MySQL Multiple Times by MC BrownIntroduction to Using
DTrace with MySQL by Vince CarboneMySQL Cluster 7.0 - New Features by Johan
AnderssonOptimizing MySQL Performance with ZFS by Allan PackerSAN Performance
on a Internal Disk Budget: The Coming Solid State Disk Revolution by Matthew
YonkovitThis is Not a Web App: The Evolution of a MySQL Deployment at Google
by Mark Callaghan</p><p>5 0.83236682 <a title="237-lda-5" href="../high_scalability-2013/high_scalability-2013-06-28-Stuff_The_Internet_Says_On_Scalability_For_June_28%2C_2013.html">1484 high scalability-2013-06-28-Stuff The Internet Says On Scalability For June 28, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:(Leandro Erlich's super cool scaling
illusion)Who am I? I have 50 petabytes of data stored in Hadoop and Teradata,
400 million items for sale, 250 million queries a day, 100,000 pages served
per second, 112 million active users, $75 billions sold in 2012...If
youguessed eBay then you've won the auction.Quotable Quotes:Controlled
Experiments at Large Scale: Bing found that every 100ms faster they deliver
search result pages yields 0.6% more in revenueLuis Bettencourt: A city is
first and foremost a social reactor. It works like a star, attracting people
and accelerating social interaction and social outputs in a way that is
analogous to how stars compress matter and burn brighter and faster the bigger
they are.@nntaleb: unless you understand that fat tails come from
concentration of errors, you should not discuss probability & risk Need to
make Hadoop faster? Hadoop + GPU: Boost performance of your big data project
by 50x-200x? Or there's Spark, which uses</p><p>6 0.82360572 <a title="237-lda-6" href="../high_scalability-2012/high_scalability-2012-11-12-Gone_Fishin%27%3A_Hilarious_Video%3A_Relational_Database_Vs_NoSQL_Fanbois.html">1357 high scalability-2012-11-12-Gone Fishin': Hilarious Video: Relational Database Vs NoSQL Fanbois</a></p>
<p>7 0.8036229 <a title="237-lda-7" href="../high_scalability-2009/high_scalability-2009-07-27-Handle_700_Percent_More_Requests_Using_Squid_and_APC_Cache.html">662 high scalability-2009-07-27-Handle 700 Percent More Requests Using Squid and APC Cache</a></p>
<p>8 0.8003695 <a title="237-lda-8" href="../high_scalability-2009/high_scalability-2009-07-16-Scaling_Traffic%3A_People_Pod_Pool_of_On_Demand_Self_Driving_Robotic_Cars_who_Automatically_Refuel_from_Cheap_Solar.html">657 high scalability-2009-07-16-Scaling Traffic: People Pod Pool of On Demand Self Driving Robotic Cars who Automatically Refuel from Cheap Solar</a></p>
<p>9 0.79947412 <a title="237-lda-9" href="../high_scalability-2010/high_scalability-2010-11-15-Strategy%3A_Biggest_Performance_Impact_is_to_Reduce_the_Number_of_HTTP_Requests.html">942 high scalability-2010-11-15-Strategy: Biggest Performance Impact is to Reduce the Number of HTTP Requests</a></p>
<p>10 0.79383636 <a title="237-lda-10" href="../high_scalability-2010/high_scalability-2010-09-28-6_Strategies_for_Scaling_BBC_iPlayer.html">908 high scalability-2010-09-28-6 Strategies for Scaling BBC iPlayer</a></p>
<p>11 0.7938143 <a title="237-lda-11" href="../high_scalability-2014/high_scalability-2014-04-03-Leslie_Lamport_to_Programmers%3A_You%27re_Doing_it_Wrong.html">1625 high scalability-2014-04-03-Leslie Lamport to Programmers: You're Doing it Wrong</a></p>
<p>12 0.78255129 <a title="237-lda-12" href="../high_scalability-2011/high_scalability-2011-07-18-Building_your_own_Facebook_Realtime_Analytics_System__.html">1081 high scalability-2011-07-18-Building your own Facebook Realtime Analytics System  </a></p>
<p>13 0.78191209 <a title="237-lda-13" href="../high_scalability-2011/high_scalability-2011-09-13-Must_see%3A_5_Steps_to_Scaling_MongoDB_%28Or_Any_DB%29_in_8_Minutes.html">1114 high scalability-2011-09-13-Must see: 5 Steps to Scaling MongoDB (Or Any DB) in 8 Minutes</a></p>
<p>14 0.7818625 <a title="237-lda-14" href="../high_scalability-2009/high_scalability-2009-09-10-How_to_handle_so_many_socket_connection.html">699 high scalability-2009-09-10-How to handle so many socket connection</a></p>
<p>15 0.78036541 <a title="237-lda-15" href="../high_scalability-2007/high_scalability-2007-07-10-mixi.jp__Architecture.html">5 high scalability-2007-07-10-mixi.jp  Architecture</a></p>
<p>16 0.78015089 <a title="237-lda-16" href="../high_scalability-2012/high_scalability-2012-02-16-A_Super_Short_on_the_Youporn_Stack_-_300K_QPS_and_100_Million_Page_Views_Per_Day.html">1194 high scalability-2012-02-16-A Super Short on the Youporn Stack - 300K QPS and 100 Million Page Views Per Day</a></p>
<p>17 0.77808744 <a title="237-lda-17" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>18 0.77711326 <a title="237-lda-18" href="../high_scalability-2009/high_scalability-2009-04-06-How_do_you_monitor_the_performance_of_your_cluster%3F.html">558 high scalability-2009-04-06-How do you monitor the performance of your cluster?</a></p>
<p>19 0.77538192 <a title="237-lda-19" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>20 0.77519196 <a title="237-lda-20" href="../high_scalability-2012/high_scalability-2012-11-29-Performance_data_for_LevelDB%2C_Berkley_DB_and_BangDB_for_Random_Operations.html">1364 high scalability-2012-11-29-Performance data for LevelDB, Berkley DB and BangDB for Random Operations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
