<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 hunch net-2006-07-26-Two more UAI papers of interest</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-199" href="#">hunch_net-2006-199</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>199 hunch net-2006-07-26-Two more UAI papers of interest</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-199-html" href="http://hunch.net/?p=217">html</a></p><p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('manfred', 0.395), ('intriguing', 0.198), ('gaussians', 0.198), ('paperby', 0.198), ('ed', 0.198), ('moments', 0.198), ('eye', 0.183), ('sanjoy', 0.183), ('projections', 0.183), ('look', 0.179), ('dasgupta', 0.173), ('jointly', 0.173), ('daniel', 0.173), ('beautiful', 0.165), ('hsu', 0.165), ('caught', 0.158), ('mixture', 0.148), ('surprisingly', 0.14), ('brain', 0.13), ('distributed', 0.128)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="199-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><p>2 0.16769655 <a title="199-tfidf-2" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><p>3 0.11738728 <a title="199-tfidf-3" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><p>4 0.086505905 <a title="199-tfidf-4" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>Introduction: I learned a number of things atNIPS.The financial people were there in greater
force than previously.Two Sigmasponsored NIPS whileDRW Tradinghad a
booth.Theadversarial machine learning workshophad a number of talks about
interesting applications where an adversary really is out to try and mess up
your learning algorithm. This is very different from the situation we often
think of where the world is oblivious to our learning. This may present new
and convincing applications for the learning-against-an-adversary work common
atCOLT.There were several interesing papers.Sanjoy Dasgupta,Daniel Hsu,
andClaire Monteleonihad a paper onGeneral Agnostic Active Learning. The basic
idea is that active learning can be done via reduction to a form of supervised
learning problem. This is great, because we have many supervised learning
algorithms from which the benefits of active learning may be derived.Joseph
BradleyandRobert Schapirehad aPaper on Filterboost. Filterboost is an online
boosting algorit</p><p>5 0.079018556 <a title="199-tfidf-5" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>6 0.07497339 <a title="199-tfidf-6" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>7 0.074807651 <a title="199-tfidf-7" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>8 0.073766813 <a title="199-tfidf-8" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>9 0.072694376 <a title="199-tfidf-9" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>10 0.066585422 <a title="199-tfidf-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.065971248 <a title="199-tfidf-11" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>12 0.065833949 <a title="199-tfidf-12" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>13 0.064096957 <a title="199-tfidf-13" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>14 0.063815787 <a title="199-tfidf-14" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>15 0.061642747 <a title="199-tfidf-15" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>16 0.061585605 <a title="199-tfidf-16" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>17 0.060936123 <a title="199-tfidf-17" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>18 0.060388058 <a title="199-tfidf-18" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>19 0.059368264 <a title="199-tfidf-19" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>20 0.059130654 <a title="199-tfidf-20" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, 0.037), (2, -0.025), (3, 0.054), (4, -0.089), (5, 0.071), (6, 0.03), (7, -0.051), (8, 0.017), (9, -0.018), (10, 0.103), (11, 0.077), (12, -0.048), (13, 0.078), (14, -0.0), (15, 0.044), (16, 0.045), (17, 0.028), (18, 0.019), (19, 0.016), (20, -0.01), (21, 0.032), (22, -0.062), (23, -0.017), (24, 0.1), (25, 0.021), (26, -0.065), (27, -0.015), (28, -0.023), (29, 0.016), (30, 0.031), (31, -0.044), (32, 0.064), (33, -0.016), (34, 0.051), (35, -0.026), (36, 0.012), (37, 0.085), (38, 0.016), (39, 0.02), (40, -0.006), (41, -0.043), (42, 0.064), (43, -0.006), (44, 0.001), (45, -0.015), (46, 0.175), (47, 0.009), (48, -0.011), (49, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97438169 <a title="199-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><p>2 0.55904281 <a title="199-lsi-2" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>Introduction: Here are a few other papers I enjoyed from ICML06.Topic Models:Dynamic Topic
ModelsDavid Blei, John LaffertyA nice model for how topics in LDA type models
can evolve over time,using a linear dynamical system on the natural parameters
and a veryclever structured variational approximation (in which the mean
fieldparameters are pseudo-observations of a virtual LDS). Like all
Bleipapers, he makes it look easy, but it is extremely impressive.Pachinko
AllocationWei Li, Andrew McCallumA very elegant (but computationally
challenging) model which inducescorrelation amongst topics using a multi-level
DAG whose interior nodesare "super-topics" and "sub-topics" and whose leaves
are thevocabulary words. Makes the slumbering monster of structure learning
stir.Sequence Analysis (I missed these talks since I was chairing another
session)Online Decoding of Markov Models with Latency ConstraintsMukund
Narasimhan, Paul Viola, Michael ShilmanAn "ah-ha!" paper showing how to trade
off latency and decodinga</p><p>3 0.55121106 <a title="199-lsi-3" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><p>4 0.54450005 <a title="199-lsi-4" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>5 0.52729356 <a title="199-lsi-5" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><p>6 0.52226973 <a title="199-lsi-6" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>7 0.50225151 <a title="199-lsi-7" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>8 0.50214893 <a title="199-lsi-8" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>9 0.50191861 <a title="199-lsi-9" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>10 0.45201677 <a title="199-lsi-10" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>11 0.43308163 <a title="199-lsi-11" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>12 0.4326942 <a title="199-lsi-12" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>13 0.43251541 <a title="199-lsi-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.43108863 <a title="199-lsi-14" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>15 0.42072919 <a title="199-lsi-15" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>16 0.41537473 <a title="199-lsi-16" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>17 0.41307825 <a title="199-lsi-17" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>18 0.41212532 <a title="199-lsi-18" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>19 0.41171953 <a title="199-lsi-19" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>20 0.40854141 <a title="199-lsi-20" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.334), (2, 0.023), (29, 0.047), (42, 0.289), (45, 0.052), (68, 0.012), (74, 0.098), (92, 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96506083 <a title="199-lda-1" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>2 0.9625963 <a title="199-lda-2" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>Introduction: 2012 was a tumultuous year for me, but it was undeniably a great year for deep
learning efforts. Signs of this include:Winning aKaggle competition.Wide
adoption ofdeep learning for speech recognition.Significantindustry
support.Gains inimagerecognition.This is a rare event in research: a
significant capability breakout. Congratulations are definitely in order for
those who managed to achieve it. At this point, deep learning algorithms seem
like a choice undeniably worth investigating for real applications with
significant data.</p><p>3 0.9461326 <a title="199-lda-3" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is anothermachine learning meetupwhere I'll be speaking.
Although the topic is contextual bandits, I think of it as "the future of
machine learning". In particular, it's all about how to learn in an
interactive environment, such as for ad display, trading, news recommendation,
etcâ&euro;ŚOn Sept 24, abstracts for theNew York Machine Learning Symposiumare due.
This is the largest Machine Learning event in the area, so it's a great way to
have a conversation with other people.On Oct 22, the NY ML Symposium actually
happens. This year, we are expanding the spotlights, and trying to have more
time for posters. In addition, we have a strong set of invited speakers:David
Blei,Sanjoy Dasgupta,Tommi Jaakkola, andYann LeCun. After the meeting, a
latehackNYrelated event is planned where students and startups can meet.I'd
also like to point out the relatedCS/Econ symposiumas I have interests there
as well.</p><p>same-blog 4 0.91134006 <a title="199-lda-4" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><p>5 0.85851514 <a title="199-lda-5" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!'sKey Scientific ChallengesforMachine Learninggrant applications are due
March 11. If you are a student working on relevant research, please consider
applying. It's for $5K of unrestricted funding.</p><p>6 0.78888106 <a title="199-lda-6" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>7 0.73261416 <a title="199-lda-7" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>8 0.66767418 <a title="199-lda-8" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>9 0.66123635 <a title="199-lda-9" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>10 0.66059268 <a title="199-lda-10" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>11 0.66035759 <a title="199-lda-11" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>12 0.66020203 <a title="199-lda-12" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>13 0.66019613 <a title="199-lda-13" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>14 0.65820712 <a title="199-lda-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.65801883 <a title="199-lda-15" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>16 0.65799809 <a title="199-lda-16" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>17 0.65784359 <a title="199-lda-17" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>18 0.6571306 <a title="199-lda-18" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>19 0.65708172 <a title="199-lda-19" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>20 0.65614617 <a title="199-lda-20" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
