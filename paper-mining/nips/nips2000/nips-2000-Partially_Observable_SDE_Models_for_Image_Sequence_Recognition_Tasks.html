<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-98" href="#">nips2000-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</h1>
<br/><p>Source: <a title="nips-2000-98-pdf" href="http://papers.nips.cc/paper/1836-partially-observable-sde-models-for-image-sequence-recognition-tasks.pdf">pdf</a></p><p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>Reference: <a title="nips-2000-98-reference" href="../nips2000_reference/nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('diffus', 0.556), ('sde', 0.521), ('path', 0.258), ('luettin', 0.214), ('hmms', 0.159), ('wo', 0.139), ('stochast', 0.126), ('conto', 0.125), ('network', 0.123), ('hid', 0.109), ('drift', 0.097), ('hmm', 0.087), ('recognit', 0.087), ('movel', 0.084), ('wh', 0.08), ('sequ', 0.078), ('shap', 0.076), ('sampl', 0.071), ('continu', 0.07), ('lip', 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="98-tfidf-1" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>2 0.13178848 <a title="98-tfidf-2" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>3 0.12971133 <a title="98-tfidf-3" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>Author: Arno Schödl, Irfan A. Essa</p><p>Abstract: We present techniques for rendering and animation of realistic scenes by analyzing and training on short video sequences. This work extends the new paradigm for computer animation, video textures, which uses recorded video to generate novel animations by replaying the video samples in a new order. Here we concentrate on video sprites, which are a special type of video texture. In video sprites, instead of storing whole images, the object of interest is separated from the background and the video samples are stored as a sequence of alpha-matted sprites with associated velocity information. They can be rendered anywhere on the screen to create a novel animation of the object. We present methods to create such animations by finding a sequence of sprite samples that is both visually smooth and follows a desired path. To estimate visual smoothness, we train a linear classifier to estimate visual similarity between video samples. If the motion path is known in advance, we use beam search to find a good sample sequence. We can specify the motion interactively by precomputing the sequence cost function using Q-Iearning.</p><p>4 0.11088526 <a title="98-tfidf-4" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><p>5 0.099121645 <a title="98-tfidf-5" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>Author: Predrag Neskovic, Philip C. Davis, Leon N. Cooper</p><p>Abstract: In this work, we introduce an Interactive Parts (IP) model as an alternative to Hidden Markov Models (HMMs). We t ested both models on a database of on-line cursive script. We show that implementations of HMMs and the IP model, in which all letters are assumed to have the same average width , give comparable results. However , in contrast to HMMs, the IP model can handle duration modeling without an increase in computational complexity. 1</p><p>6 0.09502317 <a title="98-tfidf-6" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>7 0.094465099 <a title="98-tfidf-7" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>8 0.093992256 <a title="98-tfidf-8" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>9 0.093942054 <a title="98-tfidf-9" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>10 0.092896171 <a title="98-tfidf-10" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>11 0.08272218 <a title="98-tfidf-11" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>12 0.081641637 <a title="98-tfidf-12" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>13 0.077837363 <a title="98-tfidf-13" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>14 0.071121529 <a title="98-tfidf-14" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>15 0.070750214 <a title="98-tfidf-15" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>16 0.068998411 <a title="98-tfidf-16" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>17 0.068448186 <a title="98-tfidf-17" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>18 0.067267753 <a title="98-tfidf-18" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>19 0.063765034 <a title="98-tfidf-19" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>20 0.062648416 <a title="98-tfidf-20" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, -0.042), (2, -0.144), (3, -0.039), (4, 0.023), (5, -0.048), (6, -0.005), (7, 0.052), (8, 0.009), (9, 0.006), (10, 0.095), (11, 0.118), (12, -0.023), (13, 0.181), (14, -0.099), (15, 0.006), (16, -0.129), (17, -0.062), (18, 0.037), (19, 0.141), (20, -0.056), (21, -0.082), (22, 0.025), (23, -0.106), (24, 0.036), (25, 0.028), (26, -0.086), (27, 0.071), (28, 0.107), (29, 0.016), (30, -0.125), (31, -0.013), (32, 0.233), (33, -0.006), (34, -0.01), (35, -0.178), (36, 0.039), (37, -0.044), (38, -0.14), (39, -0.258), (40, -0.024), (41, 0.124), (42, -0.072), (43, -0.009), (44, 0.172), (45, -0.039), (46, 0.09), (47, -0.006), (48, -0.116), (49, -0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90239698 <a title="98-lsi-1" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>2 0.49240905 <a title="98-lsi-2" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>Author: Arno Schödl, Irfan A. Essa</p><p>Abstract: We present techniques for rendering and animation of realistic scenes by analyzing and training on short video sequences. This work extends the new paradigm for computer animation, video textures, which uses recorded video to generate novel animations by replaying the video samples in a new order. Here we concentrate on video sprites, which are a special type of video texture. In video sprites, instead of storing whole images, the object of interest is separated from the background and the video samples are stored as a sequence of alpha-matted sprites with associated velocity information. They can be rendered anywhere on the screen to create a novel animation of the object. We present methods to create such animations by finding a sequence of sprite samples that is both visually smooth and follows a desired path. To estimate visual smoothness, we train a linear classifier to estimate visual similarity between video samples. If the motion path is known in advance, we use beam search to find a good sample sequence. We can specify the motion interactively by precomputing the sequence cost function using Q-Iearning.</p><p>3 0.44187897 <a title="98-lsi-3" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>Author: Vasin Punyakanok, Dan Roth</p><p>Abstract: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.</p><p>4 0.42557248 <a title="98-lsi-4" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>Author: Serge Belongie, Jitendra Malik, Jan Puzicha</p><p>Abstract: We develop an approach to object recognition based on matching shapes and using a resulting measure of similarity in a nearest neighbor classifier. The key algorithmic problem here is that of finding pointwise correspondences between an image shape and a stored prototype shape. We introduce a new shape descriptor, the shape context, which makes this possible, using a simple and robust algorithm. The shape context at a point captures the distribution over relative positions of other shape points and thus summarizes global shape in a rich, local descriptor. We demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes. Once shapes are aligned, shape contexts are used to define a robust score for measuring shape similarity. We have used this score in a nearest-neighbor classifier for recognition of hand written digits as well as 3D objects, using exactly the same distance function. On the benchmark MNIST dataset of handwritten digits, this yields an error rate of 0.63%, outperforming other published techniques. 1</p><p>5 0.41435766 <a title="98-lsi-5" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>Author: Predrag Neskovic, Philip C. Davis, Leon N. Cooper</p><p>Abstract: In this work, we introduce an Interactive Parts (IP) model as an alternative to Hidden Markov Models (HMMs). We t ested both models on a database of on-line cursive script. We show that implementations of HMMs and the IP model, in which all letters are assumed to have the same average width , give comparable results. However , in contrast to HMMs, the IP model can handle duration modeling without an increase in computational complexity. 1</p><p>6 0.40054935 <a title="98-lsi-6" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>7 0.35610583 <a title="98-lsi-7" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>8 0.3520222 <a title="98-lsi-8" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>9 0.34680498 <a title="98-lsi-9" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>10 0.33726305 <a title="98-lsi-10" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>11 0.32570687 <a title="98-lsi-11" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>12 0.31825918 <a title="98-lsi-12" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>13 0.30967265 <a title="98-lsi-13" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>14 0.30508089 <a title="98-lsi-14" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>15 0.30316171 <a title="98-lsi-15" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>16 0.28620571 <a title="98-lsi-16" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>17 0.27705675 <a title="98-lsi-17" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>18 0.26921046 <a title="98-lsi-18" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>19 0.25943819 <a title="98-lsi-19" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>20 0.25934616 <a title="98-lsi-20" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.135), (11, 0.013), (16, 0.057), (21, 0.044), (23, 0.01), (38, 0.116), (54, 0.032), (67, 0.02), (74, 0.02), (76, 0.024), (80, 0.036), (82, 0.285), (84, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80372888 <a title="98-lda-1" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>same-paper 2 0.74898893 <a title="98-lda-2" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>3 0.59771454 <a title="98-lda-3" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>4 0.59082818 <a title="98-lda-4" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>Author: Vasin Punyakanok, Dan Roth</p><p>Abstract: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.</p><p>5 0.58492935 <a title="98-lda-5" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>6 0.57361317 <a title="98-lda-6" href="./nips-2000-The_Unscented_Particle_Filter.html">137 nips-2000-The Unscented Particle Filter</a></p>
<p>7 0.56962717 <a title="98-lda-7" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>8 0.56871837 <a title="98-lda-8" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>9 0.56594849 <a title="98-lda-9" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>10 0.56488419 <a title="98-lda-10" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>11 0.56480122 <a title="98-lda-11" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>12 0.56472039 <a title="98-lda-12" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>13 0.56391114 <a title="98-lda-13" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>14 0.56376576 <a title="98-lda-14" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>15 0.56360161 <a title="98-lda-15" href="./nips-2000-Support_Vector_Novelty_Detection_Applied_to_Jet_Engine_Vibration_Spectra.html">128 nips-2000-Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra</a></p>
<p>16 0.56261206 <a title="98-lda-16" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>17 0.56259966 <a title="98-lda-17" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>18 0.5614115 <a title="98-lda-18" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>19 0.55632085 <a title="98-lda-19" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>20 0.55528808 <a title="98-lda-20" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
