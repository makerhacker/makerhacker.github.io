<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-651" href="#">high_scalability-2009-651</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-651-html" href="http://highscalability.com//blog/2009/7/2/product-project-voldemort-a-distributed-database.html">html</a></p><p>Introduction: Update:Presentation from theNoSQL conference:slides,video 1,video 2.Project
Voldemort is an open source implementation of the basic parts ofDynamo
(Amazon's Highly Available Key-value Store)distributed key-value storage
system. LinkedIn is using it in their production environment for "certain
high-scalability storage problems where simple functional partitioning is not
sufficient."From their website:Data is automatically replicated over multiple
servers.Data is automatically partitioned so each server contains only a
subset of the total dataServer failure is handled transparentlyPluggable
serialization is supported to allow rich keys and values including lists and
tuples with named fields, as well as to integrate with common serialization
frameworks like Protocol Buffers, Thrift, and Java SerializationData items are
versioned to maximize data integrity in failure scenarios without compromising
availability of the systemEach node is independent of other nodes with no
central point of fa</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Just a hint when naming a project: don't name it after one of the most popular key words in muggledom. [sent-7, score-0.362]
</p><p>2 The only way someone will find your genius via search is with a dark spell. [sent-8, score-0.232]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('voldemort', 0.244), ('serialization', 0.238), ('witch', 0.183), ('failure', 0.171), ('tuples', 0.158), ('compromising', 0.149), ('pluggable', 0.146), ('hint', 0.146), ('versioned', 0.142), ('thenosql', 0.139), ('automatically', 0.13), ('naming', 0.13), ('layered', 0.13), ('versioning', 0.128), ('genius', 0.127), ('strict', 0.123), ('articlesthe', 0.121), ('project', 0.115), ('buffers', 0.113), ('node', 0.112), ('placement', 0.111), ('maximize', 0.11), ('assign', 0.109), ('integrity', 0.107), ('geographical', 0.106), ('dark', 0.105), ('delete', 0.103), ('definition', 0.103), ('nodes', 0.102), ('hashing', 0.102), ('fields', 0.102), ('named', 0.101), ('subset', 0.094), ('thrift', 0.093), ('integrate', 0.092), ('json', 0.092), ('scenarios', 0.091), ('linkedin', 0.091), ('couchdb', 0.091), ('store', 0.088), ('lists', 0.088), ('contains', 0.087), ('frameworks', 0.086), ('words', 0.086), ('joins', 0.086), ('partitioned', 0.085), ('depending', 0.084), ('schema', 0.083), ('supported', 0.083), ('architectural', 0.081)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="651-tfidf-1" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>Introduction: Update:Presentation from theNoSQL conference:slides,video 1,video 2.Project
Voldemort is an open source implementation of the basic parts ofDynamo
(Amazon's Highly Available Key-value Store)distributed key-value storage
system. LinkedIn is using it in their production environment for "certain
high-scalability storage problems where simple functional partitioning is not
sufficient."From their website:Data is automatically replicated over multiple
servers.Data is automatically partitioned so each server contains only a
subset of the total dataServer failure is handled transparentlyPluggable
serialization is supported to allow rich keys and values including lists and
tuples with named fields, as well as to integrate with common serialization
frameworks like Protocol Buffers, Thrift, and Java SerializationData items are
versioned to maximize data integrity in failure scenarios without compromising
availability of the systemEach node is independent of other nodes with no
central point of fa</p><p>2 0.2363247 <a title="651-tfidf-2" href="../high_scalability-2009/high_scalability-2009-06-20-Building_a_data_cycle_at_LinkedIn_with_Hadoop_and_Project_Voldemort.html">634 high scalability-2009-06-20-Building a data cycle at LinkedIn with Hadoop and Project Voldemort</a></p>
<p>Introduction: Update:Building Voldemort read-only stores with Hadoop.Awrite upon
whatLinkedInis doing to integrate large offlineHadoopdata processing jobs with
a fast, distributed online key-value storage system,Project Voldemort.</p><p>3 0.16717348 <a title="651-tfidf-3" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>Introduction: The abstract for the talk given by Bob Ippolito, co-founder and CTO of Mochi
Media, Inc:Building large systems on top of a traditional single-master RDBMS
data storage layer is no longer good enough. This talk explores the landscape
of new technologies available today to augment your data layer to improve
performance and reliability. Is your application a good fit for caches, bloom
filters, bitmap indexes, column stores, distributed key/value stores, or
document databases? Learn how they work (in theory and practice) and decide
for yourself.Bob does an excellent job highlighting different products and the
key concepts to understand when pondering the wide variety of new database
offerings. It's unlikely you'll be able to say oh, this is the database for me
after watching the presentation, but you will be much better informed on your
options. And I imagine slightly confused as to what to do :-)An interesting
observation in the talk is that the more robust products are internal to large</p><p>4 0.15214375 <a title="651-tfidf-4" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>Introduction: This is a guest post byIvan de PradoandPere Ferrera, founders ofDatasalt, the
company behindPangoolandSplout SQLBig Data open-source projects.The amount of
payments performed using credit cards is huge. It is clear that there is
inherent value in the data that can be derived from analyzing all the
transactions. Client fidelity, demographics, heat maps of activity, shop
recommendations, and many other statistics are useful to both clients and
shops for improving their relationship with the market. AtDatasaltwe have
developed a system in collaboration with theBBVA bankthat is able to analyze
years of data and serve insights and statistics to different low-latency web
and mobile applications.The main challenge we faced besides processing Big
Data input is thatthe output was also Big Data, and even bigger than the
input. And this output needed to be served quickly, under high load.The
solution we developed has an infrastructure cost of just a few thousands of
dollars per month thanks to th</p><p>5 0.12296713 <a title="651-tfidf-5" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>Introduction: If Twitter is the "nervous system of the web" as some people think, then what
is the brain that makes sense of all those signals (tweets) from the nervous
system? That brain is the Twitter Analytics System and Kevin Weil, as
Analytics Lead at Twitter, is the homunculus within in charge of figuring out
what those over 100 billion tweets (approximately the number of neurons in the
human brain) mean.Twitter has only 10% of the expected 100 billion tweets now,
but a good brain always plans ahead. Kevin gave a talk,Hadoop and Protocol
Buffers at Twitter, at theHadoop Meetup, explaining how Twitter plans to use
all that data to an answer key business questions.What type of questions is
Twitter interested in answering? Questions that help them better understand
Twitter. Questions like:How many requests do we serve in a day?What is the
average latency?How many searches happen in day?How many unique queries, how
many unique users, what is their geographic distribution?What can we tell
about as</p><p>6 0.12211573 <a title="651-tfidf-6" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>7 0.11994835 <a title="651-tfidf-7" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>8 0.11990328 <a title="651-tfidf-8" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>9 0.11870863 <a title="651-tfidf-9" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>10 0.11757761 <a title="651-tfidf-10" href="../high_scalability-2009/high_scalability-2009-03-17-IBM_WebSphere_eXtreme_Scale_%28IMDG%29.html">542 high scalability-2009-03-17-IBM WebSphere eXtreme Scale (IMDG)</a></p>
<p>11 0.11436669 <a title="651-tfidf-11" href="../high_scalability-2010/high_scalability-2010-09-02-Distributed_Hashing_Algorithms_by_Example%3A_Consistent_Hashing.html">892 high scalability-2010-09-02-Distributed Hashing Algorithms by Example: Consistent Hashing</a></p>
<p>12 0.11185476 <a title="651-tfidf-12" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>13 0.11018909 <a title="651-tfidf-13" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>14 0.10507216 <a title="651-tfidf-14" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<p>15 0.10308942 <a title="651-tfidf-15" href="../high_scalability-2013/high_scalability-2013-12-11-Using_Node.js_PayPal_Doubles_RPS%2C_Lowers_Latency%2C_with_Fewer_Developers%2C_but_Where_Do_the_Improvements_Really_Come_From%3F.html">1563 high scalability-2013-12-11-Using Node.js PayPal Doubles RPS, Lowers Latency, with Fewer Developers, but Where Do the Improvements Really Come From?</a></p>
<p>16 0.10145834 <a title="651-tfidf-16" href="../high_scalability-2007/high_scalability-2007-08-16-Scaling_Secret_%232%3A_Denormalizing_Your_Way_to_Speed_and_Profit.html">65 high scalability-2007-08-16-Scaling Secret #2: Denormalizing Your Way to Speed and Profit</a></p>
<p>17 0.10096635 <a title="651-tfidf-17" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>18 0.098402247 <a title="651-tfidf-18" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>19 0.097665682 <a title="651-tfidf-19" href="../high_scalability-2009/high_scalability-2009-08-05-Anti-RDBMS%3A_A_list_of_distributed_key-value_stores.html">670 high scalability-2009-08-05-Anti-RDBMS: A list of distributed key-value stores</a></p>
<p>20 0.096653454 <a title="651-tfidf-20" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Facebook%27s_Cassandra_-_A_Massive_Distributed_Store.html">649 high scalability-2009-07-02-Product: Facebook's Cassandra - A Massive Distributed Store</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, 0.082), (2, -0.021), (3, 0.028), (4, 0.019), (5, 0.113), (6, 0.032), (7, -0.059), (8, -0.001), (9, 0.015), (10, 0.026), (11, 0.04), (12, -0.018), (13, -0.061), (14, 0.063), (15, 0.019), (16, 0.01), (17, -0.006), (18, -0.027), (19, -0.055), (20, -0.001), (21, 0.054), (22, 0.02), (23, 0.023), (24, -0.09), (25, -0.082), (26, 0.054), (27, -0.004), (28, 0.013), (29, 0.017), (30, 0.004), (31, -0.011), (32, -0.027), (33, -0.058), (34, 0.047), (35, 0.001), (36, -0.03), (37, -0.014), (38, -0.044), (39, 0.028), (40, -0.013), (41, 0.005), (42, 0.026), (43, 0.039), (44, 0.021), (45, 0.017), (46, -0.032), (47, 0.03), (48, -0.039), (49, 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96349001 <a title="651-lsi-1" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>Introduction: Update:Presentation from theNoSQL conference:slides,video 1,video 2.Project
Voldemort is an open source implementation of the basic parts ofDynamo
(Amazon's Highly Available Key-value Store)distributed key-value storage
system. LinkedIn is using it in their production environment for "certain
high-scalability storage problems where simple functional partitioning is not
sufficient."From their website:Data is automatically replicated over multiple
servers.Data is automatically partitioned so each server contains only a
subset of the total dataServer failure is handled transparentlyPluggable
serialization is supported to allow rich keys and values including lists and
tuples with named fields, as well as to integrate with common serialization
frameworks like Protocol Buffers, Thrift, and Java SerializationData items are
versioned to maximize data integrity in failure scenarios without compromising
availability of the systemEach node is independent of other nodes with no
central point of fa</p><p>2 0.85676754 <a title="651-lsi-2" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>Introduction: Terrastoreis a new-born document store which provides advanced scalability and
elasticity features without sacrificing consistency.Here are a few
highlights:Ubiquitous: based on the universally supported HTTP
protocol.Distributed: nodes can run and live everywhere on your
network.Elastic: you can add and remove nodes dynamically to/from your running
cluster with no downtime and no changes at all to your configuration.Scalable
at the data layer: documents are partitioned and distributed among your nodes,
with automatic and transparent re-balancing when nodes join and leave.Scalable
at the computational layer: query and update operations are distributed to the
nodes which actually holds the queried/updated data, minimizing network
traffic and spreading computational load.Consistent: providing per-document
consistency, you're guaranteed to always get the latest value of a single
document, with read committed isolation for concurrent
modifications.Schemaless: providing a collection-based i</p><p>3 0.80670542 <a title="651-lsi-3" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>Introduction: When building a system on top of a set of wildly uncooperative and unruly
computers you have knowledge problems: knowing when other nodes are dead;
knowing when nodes become alive; getting information about other nodes so you
can make local decisions, like knowing which node should handle a request
based on a scheme for assigning nodes to a certain range of users; learning
about new configuration data; agreeing on data values; and so on.How do you
solve these problems? A common centralized approach is to use a database and
all nodes query it for information. Obvious availability and performance
issues for large distributed clusters. Another approach is to use Paxos, a
protocol for solving consensus in a network to maintain strict consistency
requirements for small groups of unreliable processes. Not practical when
larger number of nodes are involved.So what's the super cool decentralized way
to bring order to large clusters?Gossip protocols, which maintain relaxed
consistency requireme</p><p>4 0.79643571 <a title="651-lsi-4" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>Introduction: The abstract for the talk given by Bob Ippolito, co-founder and CTO of Mochi
Media, Inc:Building large systems on top of a traditional single-master RDBMS
data storage layer is no longer good enough. This talk explores the landscape
of new technologies available today to augment your data layer to improve
performance and reliability. Is your application a good fit for caches, bloom
filters, bitmap indexes, column stores, distributed key/value stores, or
document databases? Learn how they work (in theory and practice) and decide
for yourself.Bob does an excellent job highlighting different products and the
key concepts to understand when pondering the wide variety of new database
offerings. It's unlikely you'll be able to say oh, this is the database for me
after watching the presentation, but you will be much better informed on your
options. And I imagine slightly confused as to what to do :-)An interesting
observation in the talk is that the more robust products are internal to large</p><p>5 0.77189052 <a title="651-lsi-5" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>Introduction: We've seen a lot ofNoSQLaction lately built around distributed hash tables.
Btrees are getting jealous. Btrees, once the king of the database world, want
their throne back.Paul Buchheitsurfaced a paper:A practical scalable
distributed B-treeby Marcos K. Aguilera and Wojciech Golab, that might help
spark a revolution.From the Abstract:We propose a new algorithm for a
practical, fault tolerant, and scalable B-tree distributed over a set of
servers. Our algorithm supports practical features not present in prior work:
transactions that allow atomic execution of multiple operations over multiple
B-trees, online migration of B-tree nodes between servers, and dynamic
addition and removal of servers. Moreover, our algorithm is conceptually
simple: we use transactions to manipulate B-tree nodes so that clients need
not use complicated concurrency and locking protocols used in prior work. To
execute these transactions quickly, we rely on three techniques: (1) We use
optimistic concurrency contro</p><p>6 0.74920213 <a title="651-lsi-6" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>7 0.7448501 <a title="651-lsi-7" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>8 0.74481493 <a title="651-lsi-8" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>9 0.74282831 <a title="651-lsi-9" href="../high_scalability-2009/high_scalability-2009-10-08-Riak_-_web-shaped_data_storage_system.html">718 high scalability-2009-10-08-Riak - web-shaped data storage system</a></p>
<p>10 0.7364279 <a title="651-lsi-10" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>11 0.73505795 <a title="651-lsi-11" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>12 0.7321493 <a title="651-lsi-12" href="../high_scalability-2011/high_scalability-2011-01-27-Comet_-_An_Example_of_the_New_Key-Code_Databases.html">979 high scalability-2011-01-27-Comet - An Example of the New Key-Code Databases</a></p>
<p>13 0.72360724 <a title="651-lsi-13" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>14 0.71966177 <a title="651-lsi-14" href="../high_scalability-2010/high_scalability-2010-10-22-Paper%3A_Netflix%E2%80%99s_Transition_to_High-Availability_Storage_Systems_.html">925 high scalability-2010-10-22-Paper: Netflix’s Transition to High-Availability Storage Systems </a></p>
<p>15 0.71523857 <a title="651-lsi-15" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>16 0.71340001 <a title="651-lsi-16" href="../high_scalability-2007/high_scalability-2007-09-27-Product%3A_Ganglia_Monitoring_System.html">101 high scalability-2007-09-27-Product: Ganglia Monitoring System</a></p>
<p>17 0.70934469 <a title="651-lsi-17" href="../high_scalability-2008/high_scalability-2008-12-17-Ringo_-_Distributed_key-value_storage_for_immutable_data.html">468 high scalability-2008-12-17-Ringo - Distributed key-value storage for immutable data</a></p>
<p>18 0.70222974 <a title="651-lsi-18" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>19 0.70191252 <a title="651-lsi-19" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>20 0.6994437 <a title="651-lsi-20" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.128), (2, 0.258), (10, 0.063), (61, 0.124), (77, 0.019), (79, 0.08), (85, 0.046), (91, 0.122), (94, 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96397275 <a title="651-lda-1" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>Introduction: Update:Presentation from theNoSQL conference:slides,video 1,video 2.Project
Voldemort is an open source implementation of the basic parts ofDynamo
(Amazon's Highly Available Key-value Store)distributed key-value storage
system. LinkedIn is using it in their production environment for "certain
high-scalability storage problems where simple functional partitioning is not
sufficient."From their website:Data is automatically replicated over multiple
servers.Data is automatically partitioned so each server contains only a
subset of the total dataServer failure is handled transparentlyPluggable
serialization is supported to allow rich keys and values including lists and
tuples with named fields, as well as to integrate with common serialization
frameworks like Protocol Buffers, Thrift, and Java SerializationData items are
versioned to maximize data integrity in failure scenarios without compromising
availability of the systemEach node is independent of other nodes with no
central point of fa</p><p>2 0.94735837 <a title="651-lda-2" href="../high_scalability-2008/high_scalability-2008-07-22-Scaling_Bumper_Sticker%3A_A_1_Billion_Page_Per_Month_Facebook_RoR_App__.html">356 high scalability-2008-07-22-Scaling Bumper Sticker: A 1 Billion Page Per Month Facebook RoR App  </a></p>
<p>Introduction: Several months ago I attended a Joyent presentation where the spokesman hinted
that Joyent had the chops to support a one billion page per month Facebook
Ruby on Rails application. Even under a few seconds of merciless grilling he
would not give up the name of the application. Now we have the big reveal: it
was LinkedIn'sBumper Stickerapp. For those not currently sticking things on
bumps, Bumper Sticker is quite surprisingly aviral media sharing application
that allows users to express their individuality by sticking small virtual
stickers on Facebook profiles. At the time I was quite curious how Joyent's
cloud approach could be leveraged for this kind of app. Now that they've
released a few details, we get to find out.Site:
http://www.Facebook.com/apps/application.php?id=2427603417Information
SourcesVideo: Scaling to 1 Billion Page Views Per MonthVideo(very flashy)Web
Scalability Practices: Bumper Sticker on Railsby Ikai Lan and Jim Meyer from
LinkedIn1 Billion Page Views a Monthby Da</p><p>3 0.94027525 <a title="651-lda-3" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>Introduction: This is a guest post by Steve Newman, co-founder of Writely (Google Docs),
tech lead on the Paxos-based synchronous replication in Megastore, and founder
of cloud service providerScalyr.com.Microsoft's Azure service suffered a
widely publicized outage on February 28th / 29th. Microsoft recently published
an excellentpostmortem. For anyone trying to run a high-availability service,
this incident can teach several important lessons.The central lesson is that,
no matter how much work you put into redundancy, problems will arise. Murphy
is strong and, I might say, creative; things go wrong. So preventative
measures are important, but how you react to problems is just as important.
It's interesting to review the Azure incident in this light.The postmortem is
worth reading in its entirety, but here's a quick summary: each time Azure
launches a new VM, it creates a "transfer certificate" to secure
communications with that VM. There was a bug in the code that determines the
certificate expirat</p><p>4 0.93750095 <a title="651-lda-4" href="../high_scalability-2010/high_scalability-2010-05-12-The_Rise_of_the_Virtual_Cellular_Machines.html">826 high scalability-2010-05-12-The Rise of the Virtual Cellular Machines</a></p>
<p>Introduction: My apologies if you were looking for a post about cell phones. This post is
about high density nanodevices. It's a follow up toHow will memristors change
everything? for those wishing to pursue these revolutionary ideas in more
depth. This is one of those areas where if you are in the space then there's a
lot of available information and if you are on the outside then it doesn't
even seem to exist. Fortunately, Ben Chandler from The SyNAPSE Project, was
kind enough to point me to a great set of presentations given at the12th IEEE
CNNA - International Workshop on Cellular Nanoscale Networks and their
Applications- Towards Megaprocessor Computing. WARNING: these papers contain
extreme technical content. If you are like me and you aren't an electrical
engineer, much of it may make a sort of surface sense, but the deep and twisty
details will fly over head. For the more software minded there are a couple
more accessible presentations:Intelligent Machines built with Memristive
Nanodevicesby</p><p>5 0.93492931 <a title="651-lda-5" href="../high_scalability-2009/high_scalability-2009-10-01-Moving_Beyond_End-to-End_Path_Information_to_Optimize_CDN_Performance.html">712 high scalability-2009-10-01-Moving Beyond End-to-End Path Information to Optimize CDN Performance</a></p>
<p>Introduction: You go through the expense of installing CDNs all over the globe to make sure
users always have a node close by and you notice something curious and
furious: clients still experience poor latencies. What's up with that? What do
you do to find the problem? If you are Google you build a tool (WhyHigh) to
figure out what's up. This paper is about the tool and the unexpected problem
ofhigh latencieson CDNs. The main problems they found: inefficient routing to
nearby nodes and packet queuing. But more useful is the architecture of
WhyHigh and how it goes about identifying bottle necks. And even more useful
is the general belief in creating sophisticated tools to understand and
improve your service. That's what professionals do. From the
abstract:Replicating content across a geographically distributed set of
servers and redirecting clients to the closest server in terms of latency has
emerged as a common paradigm for improving client performance. In this paper,
we analyze latencies measured</p><p>6 0.92974567 <a title="651-lda-6" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>7 0.9270578 <a title="651-lda-7" href="../high_scalability-2010/high_scalability-2010-08-23-6_Ways_to_Kill_Your_Servers_-__Learning_How_to_Scale_the_Hard_Way.html">884 high scalability-2010-08-23-6 Ways to Kill Your Servers -  Learning How to Scale the Hard Way</a></p>
<p>8 0.92690212 <a title="651-lda-8" href="../high_scalability-2009/high_scalability-2009-03-19-Product%3A_Redis_-_Not_Just_Another_Key-Value_Store.html">545 high scalability-2009-03-19-Product: Redis - Not Just Another Key-Value Store</a></p>
<p>9 0.92676473 <a title="651-lda-9" href="../high_scalability-2013/high_scalability-2013-02-15-Stuff_The_Internet_Says_On_Scalability_For_February_15%2C_2013.html">1407 high scalability-2013-02-15-Stuff The Internet Says On Scalability For February 15, 2013</a></p>
<p>10 0.92598641 <a title="651-lda-10" href="../high_scalability-2009/high_scalability-2009-10-15-Hot_Scalability_Links_for_Oct_15_2009_.html">722 high scalability-2009-10-15-Hot Scalability Links for Oct 15 2009 </a></p>
<p>11 0.92569762 <a title="651-lda-11" href="../high_scalability-2011/high_scalability-2011-09-16-Stuff_The_Internet_Says_On_Scalability_For_September_16%2C_2011.html">1117 high scalability-2011-09-16-Stuff The Internet Says On Scalability For September 16, 2011</a></p>
<p>12 0.92568856 <a title="651-lda-12" href="../high_scalability-2011/high_scalability-2011-12-05-Stuff_The_Internet_Says_On_Scalability_For_December_5%2C_2011.html">1151 high scalability-2011-12-05-Stuff The Internet Says On Scalability For December 5, 2011</a></p>
<p>13 0.92476553 <a title="651-lda-13" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>14 0.92476267 <a title="651-lda-14" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>15 0.92440659 <a title="651-lda-15" href="../high_scalability-2012/high_scalability-2012-04-16-Instagram_Architecture_Update%3A_What%E2%80%99s_new_with_Instagram%3F.html">1228 high scalability-2012-04-16-Instagram Architecture Update: What’s new with Instagram?</a></p>
<p>16 0.92387539 <a title="651-lda-16" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>17 0.92333937 <a title="651-lda-17" href="../high_scalability-2014/high_scalability-2014-05-02-Stuff_The_Internet_Says_On_Scalability_For_May_2nd%2C_2014.html">1642 high scalability-2014-05-02-Stuff The Internet Says On Scalability For May 2nd, 2014</a></p>
<p>18 0.92273802 <a title="651-lda-18" href="../high_scalability-2014/high_scalability-2014-04-25-Stuff_The_Internet_Says_On_Scalability_For_April_25th%2C_2014.html">1637 high scalability-2014-04-25-Stuff The Internet Says On Scalability For April 25th, 2014</a></p>
<p>19 0.92223418 <a title="651-lda-19" href="../high_scalability-2008/high_scalability-2008-05-27-eBay_Architecture.html">331 high scalability-2008-05-27-eBay Architecture</a></p>
<p>20 0.92181826 <a title="651-lda-20" href="../high_scalability-2013/high_scalability-2013-03-22-Stuff_The_Internet_Says_On_Scalability_For_March_22%2C_2013.html">1428 high scalability-2013-03-22-Stuff The Internet Says On Scalability For March 22, 2013</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
