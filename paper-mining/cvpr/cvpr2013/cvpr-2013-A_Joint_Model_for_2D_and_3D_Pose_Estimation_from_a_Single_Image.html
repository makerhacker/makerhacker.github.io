<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-14" href="#">cvpr2013-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</h1>
<br/><p>Source: <a title="cvpr-2013-14-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Simo-Serra_A_Joint_Model_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>Reference: <a title="cvpr-2013-14-reference" href="../cvpr2013_reference/cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Moreno-Noguer1 1Institut de Rob o`tica iInform` atica Industrial (CSIC-UPC), Barcelona, Spain 2Universitat Polit `ecnica de Catalunya (UPC), Barcelona, Spain  Abstract We introduce a novel approach to automatically recover 3D human pose from a single image. [sent-5, score-0.42]
</p><p>2 Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. [sent-6, score-0.164]
</p><p>3 Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. [sent-7, score-0.205]
</p><p>4 For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. [sent-9, score-0.959]
</p><p>5 Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate. [sent-10, score-0.665]
</p><p>6 Introduction Estimating the 3D human pose using a single image is a  severely under-constrained problem, because many different body poses may have very similar image projections. [sent-12, score-0.572]
</p><p>7 In any event, most of these generative and discriminative approaches rely on the fact that 2D features, such as edges, silhouettes or joints may be easily obtained from the image. [sent-16, score-0.461]
</p><p>8 Top middle: Ground truth 2D pose (green) and the result of our approach (red). [sent-20, score-0.371]
</p><p>9 Additionally, we plot a few part detectors and their corresponding score, used to estimate 3D pose. [sent-21, score-0.289]
</p><p>10 Note that despite the detectors not being very precise, our generative model allows estimating a pose very close to the actual solution. [sent-24, score-0.769]
</p><p>11 novel approach to jointly detect the 2D position and estimate the 3D pose of a person from one single image acquired with a calibrated but potentially moving camera. [sent-26, score-0.319]
</p><p>12 For this purpose we formulate a Bayesian approach combining a generative latent variable model that constrains the space of possible 3D body poses with a HOG-based discriminative model that constrains the 2D location of the body parts. [sent-27, score-0.793]
</p><p>13 The two models are simultaneously updated using an evolutionary strategy. [sent-28, score-0.14]
</p><p>14 Our optimization framework simultaneously solves for both the 2D and 3D pose using an evolutionary strategy. [sent-30, score-0.459]
</p><p>15 A set of weighted samples are generated from the probabilistic generative model and are subsequently reweighted by the score given by the 2D part detectors. [sent-31, score-0.351]
</p><p>16 The rightmost figure shows results at convergence where the red shapes are the estimated poses and the green ones correspond to the ground truth. [sent-33, score-0.147]
</p><p>17 to update image evidence while 2D observations are used to update the 3D pose. [sent-34, score-0.13]
</p><p>18 1these strong ties make it possible to accurately detect and estimate the 3D pose even when image evidence is very poor. [sent-36, score-0.491]
</p><p>19 Results are competitive with the state-of-the-art despite our relaxation of restrictions as we do not use any  2D prior and instead use directly raw detector outputs. [sent-38, score-0.152]
</p><p>20 Related Work Without using prior information, monocular 3D human pose estimation is known to be an ill-posed problem. [sent-40, score-0.482]
</p><p>21 One of the most straightforward approaches consists of modeling the pose deformations as linear combinations of modes learned from training data [5]. [sent-42, score-0.366]
</p><p>22 Other approaches follow a discriminative strategy and use learning algorithms such as support vector machines, mixtures of experts or random forest to directly learn the mappings from image evidence to the 3D pose space [1, 16, 20, 23]. [sent-45, score-0.514]
</p><p>23 Modern approaches detect each individual part using strong detectors [2, 25, 27, 29] in order to obtain good 2D pose estimations. [sent-48, score-0.65]
</p><p>24 The deformable parts model has also been extended to use 3D models for 3D viewpoint estimation of rigid objects [17]. [sent-49, score-0.183]
</p><p>25 Recently, the estimations of an off-the-shelf 2D detector [29] have already been used for 3D pose estimation in [24]. [sent-50, score-0.564]
</p><p>26 Yet, and in contrast to the solution we propose here, the 2D estimations are not updated while inferring the 3D shape, and thus, the final 3D pose strongly depends on the result of the 2D detector. [sent-51, score-0.409]
</p><p>27 The same applies for [3], which computes 2D and 3D pose in two consecutive steps with no feedback. [sent-52, score-0.319]
</p><p>28 Joint Model for 2D and 3D Pose Estimation Figure 2 shows an overview of our model for simultaneous 2D people detection and 3D pose estimation. [sent-55, score-0.319]
</p><p>29 It consists of two main components: a 3D generative kinematic model, which generates pose hypotheses, and a discriminative part model, which weights the hypotheses based on image appearance. [sent-56, score-0.75]
</p><p>30 Drawing inspiration from the approach proposed in [2] for 2D articulated shapes, we represent this model  using a Bayesian formulation. [sent-57, score-0.129]
</p><p>31 , the maps for every part detector at different scales and for the whole image. [sent-69, score-0.185]
</p><p>32 Assuming conditional independence of the evidence maps given L, and that the part map di only depends on li, the likelihood 333666333533  of the image evidence given a specific body configuration  becomes:  N  p (D | L) =  ? [sent-70, score-0.535]
</p><p>33 (1) is further simplified under the assumption that the body configuration may be represented using a tree topology. [sent-75, score-0.145]
</p><p>34 , xN} be the 3D model that projects on the 2D pose L, where xi = (xi, yi , zi) is the 3D position of i-th part center. [sent-85, score-0.39]
</p><p>35 We write the posterior of X given the image evidence D by: N  p (X | D)  ∝  ? [sent-86, score-0.13]
</p><p>36 1  In order to handle the complexity of directly modeling p (X), we propose approximating X through a generative model based on latent variables H. [sent-90, score-0.465]
</p><p>37 inative  where the discriminative and generative components become clearly separated. [sent-100, score-0.297]
</p><p>38 Discriminative Detectors Recent literature proposes two principal alternatives for discriminative detectors: the shape context descriptors built by applying boosting on the limbs [2], and the HOG template matching approach [29]. [sent-104, score-0.16]
</p><p>39 For our purposes we have  found the HOG-based template matching to be more adequate because it matches our joint-based 3D model better as we can place a detector at each joint part instead of having to infer the limb positions from the joints. [sent-105, score-0.286]
</p><p>40 As mentioned above, each part li has an associated scale parameter si. [sent-107, score-0.118]
</p><p>41 This parameter is used to pick a specific scale among the evidence maps. [sent-108, score-0.13]
</p><p>42 Note that while some detectors, such as the head one in the first row, generally give good results, others do not, such as the left hand detector in the middle row. [sent-113, score-0.155]
</p><p>43 Our approach can handle these issues by combining these 2D part detectors with a generative model. [sent-114, score-0.521]
</p><p>44 it should be evaluated by a detector at a small scale (high resolution). [sent-115, score-0.114]
</p><p>45 , wN} be the set of templates in the HOG space associated to each body part. [sent-122, score-0.142]
</p><p>46 Given a body part li, its image evidence di is computed by evaluating the template wi over the entire image for a range of scales si. [sent-124, score-0.406]
</p><p>47 Figure 3 illustrates the response of three part detectors at different scales. [sent-125, score-0.289]
</p><p>48 By interpreting each detector as a log-likelihood, the image evidence of a configuration L can be computed as: N  logp(L | D)  ≈  score(L) =  ? [sent-126, score-0.293]
</p><p>49 1  1Indeed, each of the part detectors provided in [29] is formed by several templates and we use their maximum score for each coordinate (u, v, s). [sent-129, score-0.383]
</p><p>50 333666333644  variables G to represent the possible 3D human pose T from a large set of discrete poses. [sent-131, score-0.49]
</p><p>51 Latent variables can either be mapped to the conjoint motion of various parts or be used as internal states containing internal structure of the pose. [sent-132, score-0.186]
</p><p>52 It is meant to adapt the 2D detectors and 3D generative model due to the fact they were trained independently on different datasets. [sent-134, score-0.502]
</p><p>53 Additionally, when evaluating a part detector at a point, we consider a small window from which we use the largest  detector value in order to give additional noise tolerance to the detector. [sent-135, score-0.299]
</p><p>54 Latent Generative Model Our goal is to learn a compression function: φ(XL) : XL → H that maps points in the high dimensional local 3D pose space XL to a lower dimensional space H. [sent-139, score-0.402]
</p><p>55 The local pose space consists of aligning the coordinates X to a local reference so that they are independent of the global position in the world and represent only local deformation. [sent-140, score-0.319]
</p><p>56 To create the compression function we define a latent variable model of the joint distribution of a set of variables H ∈ H and a pose XL ∈ XL. [sent-142, score-0.686]
</p><p>57 As a first step we map the RN×3 continuous pose space to a discrete domain by doing vector quantization of groups of 3D joint positions. [sent-143, score-0.416]
</p><p>58 More specifically, we group the joints into five coarse parts: right arm (ra), left arm (la), right leg (rl), left leg (ll) and torso+head (th). [sent-144, score-0.37]
</p><p>59 Thus, we can map every pose to a discrete vector T = [ra, la, rl, ll, th] ∈ K5 where ra, la, rl, ll, th are cluster indexes belonging to K = {1, 2, . [sent-145, score-0.365]
</p><p>60 We will now define a joint distribution over latent variables H ∈ H = {1, . [sent-148, score-0.284]
</p><p>61 , n} (where n is the number of latent states), and observed variables T. [sent-150, score-0.233]
</p><p>62 p(h0)p(hra,ra H  | h0)p(hla,la | h0)  , ,  p (hrl rl | h0) p (hll, ll | h0) p (hu | hra, hla) p (hll ll | h0) p (hu | hra, hla) p (hl | hrl , hll ) p (hth ,th | hu , hl) . [sent-157, score-0.806]
</p><p>63 Figure 4 illustrates the graphical model corresponding to this joint distribution, where the graph G specifies the dependencies between the latent states. [sent-158, score-0.225]
</p><p>64 This leaves a total of 12 parameters to be learnt including the detector scale factor  β. [sent-177, score-0.114]
</p><p>65 Right: Representation of the trained part weight ki as a disk with an area proportional to the value. [sent-182, score-0.139]
</p><p>66 Note that part detectors which one would assume to be most useful, such as the head, have low values. [sent-183, score-0.289]
</p><p>67 This is caused by the mismatch between the annotations used to train the 2D detectors and the 3D ground truth. [sent-184, score-0.274]
</p><p>68 These parameters are learned by translating and rotating random pose samples from the 3D training set and using them as negatives such as those seen in Fig. [sent-185, score-0.319]
</p><p>69 We treat this as a global optimization problem where, given a set of 2D detections corresponding to the different parts from a single image, we optimize over both a rigid transformation and the latent states. [sent-197, score-0.26]
</p><p>70 We then  StRUatL es1 12 3 1 0 2878 3746 4 867517546194 769  Table 1: The influence of the number of latent states per node on the average reconstruction error (in mm). [sent-206, score-0.316]
</p><p>71 Experimental Results We numerically evaluate our algorithm on the HumanEva benchmark [22], which provides 3D ground truth for various actions. [sent-211, score-0.149]
</p><p>72 For both cases, we compute the 2D observations using the detectors from [29] trained independently on the PARSE dataset [19]. [sent-213, score-0.218]
</p><p>73 The 3D model we use consists of 14 joints, each roughly corresponding to one of the 26 detectors from [29]. [sent-214, score-0.269]
</p><p>74 Additionally, for the limbs we associate an extra detector for a total of 22 detectors (see Fig. [sent-215, score-0.377]
</p><p>75 The latent generative model consists of 8 nodes with 6 latent states. [sent-217, score-0.58]
</p><p>76 The effect of the number of latent states and  ×  structure of the model is shown in Table 1. [sent-218, score-0.264]
</p><p>77 Training  We train our 3D generative model separately for the walking and jogging actions of the HumanEva dataset using the training sequence for subjects S1, S2 and S3. [sent-221, score-0.637]
</p><p>78 The part weights ki and scale factor β are learnt conjointly on the walking and jogging actions. [sent-223, score-0.385]
</p><p>79 3D error is the mean euclidean distance, in mm, with the ground truth, and the 3D pose error is the mean euclidean distance with the ground truth after performing a rigid alignment of the two shapes. [sent-228, score-0.636]
</p><p>80 We evaluate three times on every 5 images using all three cameras and all three subjects for both the walking and jogging actions, for a total of 13 18 unique images. [sent-230, score-0.318]
</p><p>81 This would make the task of evaluating all 3 1318 images extremely 333666333866  (A1) and jogging (A2) actions with all subjects (S1,S2,S3) and cameras (C1,C2,C3). [sent-232, score-0.29]
</p><p>82 We compare with the 2D error obtained using the 2D model from [29], based on the same part detectors we use. [sent-233, score-0.341]
</p><p>83 Ideal detector corresponds to our approach using Gaussians with 20px covariance as 2D input instead of the 2D detections. [sent-235, score-0.114]
</p><p>84 With these assumptions, each 3D pose can be estimated in roughly one minute. [sent-242, score-0.37]
</p><p>85 An interesting result is that we outperform the 2D pose obtained by [29], using their own part detectors. [sent-252, score-0.39]
</p><p>86 The coarse initialization consists in estimating the 3D location based on a 2D pose estimate and using completely random orientation and latent states for the generative model. [sent-257, score-0.908]
</p><p>87 In the situations in which the detectors are less noisy both initializations perform roughly the same. [sent-258, score-0.314]
</p><p>88 A full sequence for the jogging action with subject S2 is shown in Fig. [sent-261, score-0.205]
</p><p>89 Table 3 summarizes the results using the pose error, corresponding to the “aligned error” in [24]. [sent-265, score-0.319]
</p><p>90 −68T0−642 0 0 2he−f34i0rs6t−380hre  columns correspond to successful 2D and 3D pose estimations for the three subjects (S1,S2,S3). [sent-272, score-0.481]
</p><p>91 In the fourth column we see that occasionally we suffer from depth errors, where the 3D pose is correct but its depth is not. [sent-274, score-0.319]
</p><p>92 We present results for both the walking and jogging actions for all three subjects and camera C1. [sent-292, score-0.368]
</p><p>93 priors on the initialization of the 3D pose were provided, except the rough bounding boxes for each person (Fig. [sent-293, score-0.377]
</p><p>94 Note that while the global pose seems generally correct, there are still some errors in the 3D pose due to the occlusions and to the fact  that the walking style in the wild is largely different from that of the subjects of the HumanEva dataset used to train the generative model. [sent-297, score-1.059]
</p><p>95 Conclusions and Future Work The approach presented in this paper addresses the illposed problem of estimating the 3D pose in single images using a Bayesian framework. [sent-299, score-0.319]
</p><p>96 We use a combination of a strong kinematic generative model based on latent variables with a set of discriminative 2D part detectors to jointly estimate both the 3D and 2D poses. [sent-300, score-0.924]
</p><p>97 We believe the model we have presented in this paper is a step forward to combining the works of 2D pose estimation with 3D pose estimation. [sent-303, score-0.679]
</p><p>98 We have shown it is not only possible to estimate 3D poses by applying part detectors used in 2D pose estimation, but that it is also beneficial to the 2D pose estimation itself, as the 2D deformations are being generated by an underlying 3D model. [sent-304, score-1.106]
</p><p>99 More exhaustive research on different graphical models that can better represent human  poses and a deeper analysis of the hyper-parameters chosen are also likely to improve the current method. [sent-306, score-0.157]
</p><p>100 The ground truth is displayed in green and the estimated 3D pose in red. [sent-341, score-0.427]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pose', 0.319), ('generative', 0.232), ('detectors', 0.218), ('xl', 0.177), ('humaneva', 0.176), ('latent', 0.174), ('rl', 0.174), ('jogging', 0.168), ('hll', 0.163), ('hrl', 0.158), ('hra', 0.14), ('evolutionary', 0.14), ('evidence', 0.13), ('ll', 0.118), ('detector', 0.114), ('joints', 0.101), ('body', 0.096), ('tud', 0.092), ('poses', 0.091), ('estimations', 0.09), ('states', 0.09), ('articulated', 0.086), ('hla', 0.084), ('compression', 0.083), ('decompression', 0.079), ('stadmitte', 0.079), ('walking', 0.078), ('hu', 0.075), ('la', 0.074), ('subjects', 0.072), ('part', 0.071), ('hl', 0.071), ('pictorial', 0.07), ('sigal', 0.069), ('ki', 0.068), ('human', 0.066), ('discriminative', 0.065), ('balan', 0.065), ('failures', 0.064), ('kinematic', 0.063), ('leg', 0.063), ('silhouettes', 0.063), ('ra', 0.06), ('di', 0.059), ('variables', 0.059), ('bayesian', 0.059), ('initialization', 0.058), ('deformable', 0.056), ('monocular', 0.056), ('ground', 0.056), ('andriluka', 0.055), ('acyclic', 0.054), ('arm', 0.054), ('meant', 0.052), ('error', 0.052), ('truth', 0.052), ('joint', 0.051), ('roughly', 0.051), ('template', 0.05), ('actions', 0.05), ('expectations', 0.05), ('barcelona', 0.05), ('spain', 0.05), ('configuration', 0.049), ('rigid', 0.049), ('additionally', 0.048), ('indicative', 0.048), ('score', 0.048), ('li', 0.047), ('deformations', 0.047), ('discrete', 0.046), ('templates', 0.046), ('limbs', 0.045), ('initializations', 0.045), ('tracking', 0.044), ('inspiration', 0.043), ('mm', 0.042), ('zi', 0.042), ('strong', 0.042), ('drawing', 0.041), ('numerically', 0.041), ('estimation', 0.041), ('roth', 0.041), ('head', 0.041), ('purpose', 0.039), ('errors', 0.039), ('loopy', 0.039), ('competitive', 0.038), ('sequence', 0.037), ('parts', 0.037), ('double', 0.037), ('directed', 0.036), ('subtraction', 0.036), ('probable', 0.035), ('coarse', 0.035), ('atica', 0.035), ('catalunya', 0.035), ('chistera', 0.035), ('cma', 0.035), ('competitiveness', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="14-tfidf-1" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>2 0.27644655 <a title="14-tfidf-2" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>3 0.25145712 <a title="14-tfidf-3" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>Author: Michele Fenzi, Laura Leal-Taixé, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models, i.e., regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset [17] and the Pointing’04 face dataset [8]. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.</p><p>4 0.24802776 <a title="14-tfidf-4" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>Author: Fang Wang, Yi Li</p><p>Abstract: Simple tree models for articulated objects prevails in the last decade. However, it is also believed that these simple tree models are not capable of capturing large variations in many scenarios, such as human pose estimation. This paper attempts to address three questions: 1) are simple tree models sufficient? more specifically, 2) how to use tree models effectively in human pose estimation? and 3) how shall we use combined parts together with single parts efficiently? Assuming we have a set of single parts and combined parts, and the goal is to estimate a joint distribution of their locations. We surprisingly find that no latent variables are introduced in the Leeds Sport Dataset (LSP) during learning latent trees for deformable model, which aims at approximating the joint distributions of body part locations using minimal tree structure. This suggests one can straightforwardly use a mixed representation of single and combined parts to approximate their joint distribution in a simple tree model. As such, one only needs to build Visual Categories of the combined parts, and then perform inference on the learned latent tree. Our method outperformed the state of the art on the LSP, both in the scenarios when the training images are from the same dataset and from the PARSE dataset. Experiments on animal images from the VOC challenge further support our findings.</p><p>5 0.23828387 <a title="14-tfidf-5" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>6 0.2354957 <a title="14-tfidf-6" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>7 0.22316332 <a title="14-tfidf-7" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>8 0.20614763 <a title="14-tfidf-8" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>9 0.20049016 <a title="14-tfidf-9" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>10 0.19232148 <a title="14-tfidf-10" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>11 0.16624577 <a title="14-tfidf-11" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>12 0.16255352 <a title="14-tfidf-12" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>13 0.1549475 <a title="14-tfidf-13" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>14 0.15431865 <a title="14-tfidf-14" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>15 0.15296832 <a title="14-tfidf-15" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>16 0.15292743 <a title="14-tfidf-16" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>17 0.14918698 <a title="14-tfidf-17" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>18 0.14904632 <a title="14-tfidf-18" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>19 0.13994588 <a title="14-tfidf-19" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>20 0.13886954 <a title="14-tfidf-20" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.316), (1, -0.006), (2, -0.008), (3, -0.166), (4, -0.036), (5, 0.001), (6, 0.144), (7, 0.079), (8, 0.084), (9, -0.125), (10, -0.146), (11, 0.212), (12, -0.142), (13, -0.016), (14, -0.021), (15, 0.032), (16, 0.017), (17, -0.036), (18, -0.047), (19, -0.049), (20, -0.012), (21, 0.008), (22, -0.06), (23, -0.018), (24, -0.049), (25, -0.0), (26, -0.033), (27, -0.036), (28, 0.024), (29, 0.015), (30, 0.098), (31, -0.001), (32, -0.03), (33, -0.04), (34, -0.021), (35, 0.019), (36, 0.006), (37, 0.03), (38, 0.036), (39, -0.034), (40, 0.021), (41, -0.074), (42, 0.003), (43, 0.008), (44, -0.033), (45, 0.026), (46, -0.001), (47, -0.034), (48, -0.013), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98574305 <a title="14-lsi-1" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>2 0.92918658 <a title="14-lsi-2" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>Author: Magnus Burenius, Josephine Sullivan, Stefan Carlsson</p><p>Abstract: We consider the problem of automatically estimating the 3D pose of humans from images, taken from multiple calibrated views. We show that it is possible and tractable to extend the pictorial structures framework, popular for 2D pose estimation, to 3D. We discuss how to use this framework to impose view, skeleton, joint angle and intersection constraints in 3D. The 3D pictorial structures are evaluated on multiple view data from a professional football game. The evaluation is focused on computational tractability, but we also demonstrate how a simple 2D part detector can be plugged into the framework.</p><p>3 0.9172526 <a title="14-lsi-3" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: In this paper we consider the challenging problem of articulated human pose estimation in still images. We observe that despite high variability of the body articulations, human motions and activities often simultaneously constrain the positions of multiple body parts. Modelling such higher order part dependencies seemingly comes at a cost of more expensive inference, which resulted in their limited use in state-of-the-art methods. In this paper we propose a model that incorporates higher order part dependencies while remaining efficient. We achieve this by defining a conditional model in which all body parts are connected a-priori, but which becomes a tractable tree-structured pictorial structures model once the image observations are available. In order to derive a set of conditioning variables we rely on the poselet-based features that have been shown to be effective for people detection but have so far found limited application for articulated human pose estimation. We demon- strate the effectiveness of our approach on three publicly available pose estimation benchmarks improving or being on-par with state of the art in each case.</p><p>4 0.89346433 <a title="14-lsi-4" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>Author: Kota Hara, Rama Chellappa</p><p>Abstract: We present a hierarchical method for human pose estimation from a single still image. In our approach, a dependency graph representing relationships between reference points such as bodyjoints is constructed and thepositions of these reference points are sequentially estimated by a successive application of multidimensional output regressions along the dependency paths, starting from the root node. Each regressor takes image features computed from an image patch centered on the current node ’s position estimated by the previous regressor and is specialized for estimating its child nodes ’ positions. The use of the dependency graph allows us to decompose a complex pose estimation problem into a set of local pose estimation problems that are less complex. We design a dependency graph for two commonly used human pose estimation datasets, the Buffy Stickmen dataset and the ETHZ PASCAL Stickmen dataset, and demonstrate that our method achieves comparable accuracy to state-of-the-art results on both datasets with significantly lower computation time than existing methods. Furthermore, we propose an importance weighted boosted re- gression trees method for transductive learning settings and demonstrate the resulting improved performance for pose estimation tasks.</p><p>5 0.8908757 <a title="14-lsi-5" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>Author: Ben Sapp, Ben Taskar</p><p>Abstract: We propose a multimodal, decomposable model for articulated human pose estimation in monocular images. A typical approach to this problem is to use a linear structured model, which struggles to capture the wide range of appearance present in realistic, unconstrained images. In this paper, we instead propose a model of human pose that explicitly captures a variety of pose modes. Unlike other multimodal models, our approach includes both global and local pose cues and uses a convex objective and joint training for mode selection and pose estimation. We also employ a cascaded mode selection step which controls the trade-off between speed and accuracy, yielding a 5x speedup in inference and learning. Our model outperforms state-of-theart approaches across the accuracy-speed trade-off curve for several pose datasets. This includes our newly-collected dataset of people in movies, FLIC, which contains an order of magnitude more labeled data for training and testing than existing datasets. The new dataset and code are avail- able online. 1</p><p>6 0.88920367 <a title="14-lsi-6" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>7 0.87289578 <a title="14-lsi-7" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>8 0.86494809 <a title="14-lsi-8" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>9 0.86286736 <a title="14-lsi-9" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>10 0.82663864 <a title="14-lsi-10" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>11 0.82614517 <a title="14-lsi-11" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>12 0.81935334 <a title="14-lsi-12" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>13 0.80923611 <a title="14-lsi-13" href="./cvpr-2013-Tensor-Based_Human_Body_Modeling.html">426 cvpr-2013-Tensor-Based Human Body Modeling</a></p>
<p>14 0.79865623 <a title="14-lsi-14" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>15 0.70766783 <a title="14-lsi-15" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>16 0.69986707 <a title="14-lsi-16" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>17 0.69129312 <a title="14-lsi-17" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>18 0.65133309 <a title="14-lsi-18" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>19 0.63038349 <a title="14-lsi-19" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>20 0.60780621 <a title="14-lsi-20" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.15), (16, 0.019), (26, 0.057), (33, 0.288), (40, 0.158), (67, 0.097), (69, 0.044), (87, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92590457 <a title="14-lda-1" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>Author: Rui Caseiro, Pedro Martins, João F. Henriques, Fátima Silva Leite, Jorge Batista</p><p>Abstract: In the past few years there has been a growing interest on geometric frameworks to learn supervised classification models on Riemannian manifolds [31, 27]. A popular framework, valid over any Riemannian manifold, was proposed in [31] for binary classification. Once moving from binary to multi-class classification thisparadigm is not valid anymore, due to the spread of multiple positive classes on the manifold [27]. It is then natural to ask whether the multi-class paradigm could be extended to operate on a large class of Riemannian manifolds. We propose a mathematically well-founded classification paradigm that allows to extend the work in [31] to multi-class models, taking into account the structure of the space. The idea is to project all the data from the manifold onto an affine tangent space at a particular point. To mitigate the distortion induced by local diffeomorphisms, we introduce for the first time in the computer vision community a well-founded mathematical concept, so-called Rolling map [21, 16]. The novelty in this alternate school of thought is that the manifold will be firstly rolled (without slipping or twisting) as a rigid body, then the given data is unwrapped onto the affine tangent space, where the classification is performed.</p><p>same-paper 2 0.91990894 <a title="14-lda-2" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>3 0.91376752 <a title="14-lda-3" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>4 0.91316998 <a title="14-lda-4" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>5 0.90853924 <a title="14-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.90084893 <a title="14-lda-6" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>7 0.8987689 <a title="14-lda-7" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>8 0.89857608 <a title="14-lda-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.89797521 <a title="14-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.89624989 <a title="14-lda-10" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>11 0.89467108 <a title="14-lda-11" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>12 0.89462507 <a title="14-lda-12" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>13 0.89399874 <a title="14-lda-13" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>14 0.89364094 <a title="14-lda-14" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>15 0.89306414 <a title="14-lda-15" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>16 0.89285511 <a title="14-lda-16" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>17 0.89277595 <a title="14-lda-17" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>18 0.89145976 <a title="14-lda-18" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>19 0.89133328 <a title="14-lda-19" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>20 0.89036667 <a title="14-lda-20" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
