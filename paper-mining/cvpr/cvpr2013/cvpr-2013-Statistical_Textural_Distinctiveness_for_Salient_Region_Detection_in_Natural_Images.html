<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-411" href="#">cvpr2013-411</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</h1>
<br/><p>Source: <a title="cvpr-2013-411-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Scharfenberger_Statistical_Textural_Distinctiveness_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>Reference: <a title="cvpr-2013-411-reference" href="../cvpr2013_reference/cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Clausi University of Waterloo, Vision and Image Processing (VIP) Research Group Waterloo, Ontario, Canada  { cs charfenbe  rger  , a2 8wong , kfergani ,  Abstract A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. [sent-3, score-1.292]
</p><p>2 Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. [sent-4, score-1.44]
</p><p>3 Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. [sent-5, score-1.747]
</p><p>4 Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. [sent-6, score-1.852]
</p><p>5 Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods. [sent-7, score-0.433]
</p><p>6 Introduction  The underlying goal of saliency detection in natural images is to identify and localize objects of interest that attract the visual attention of a human observer compared to the rest of the scene. [sent-9, score-0.534]
</p><p>7 The research area of saliency detection from natural images has gained tremendous interest in the field of computer vision given its wide applicability for many computer vision tasks such as image segmentation [9], image retargeting [3], object detection [20], and object recognition [24]. [sent-12, score-0.468]
</p><p>8 To achieve saliency detection in an automatic manner, one must define what constitutes as a salient object based on some quantifiable visual attributes such as intensity, color, structure, texture, size, or shape that makes that object apj z e lek dclaus i @uwat e rloo . [sent-13, score-0.546]
</p><p>9 In the context of saliency in natural images, one can then view salient objects of interest as objects that possess textural characteristics that are highly distinctive from a human observer perspective when compared with that of the rest of the scene. [sent-19, score-1.426]
</p><p>10 As such, we are interested in explicitly taking advantage of textural characteristics in a quantitative manner to detect saliency objects of interest within a scene. [sent-20, score-1.238]
</p><p>11 Two important challenging aspects associated with explicitly accounting for textural characteristics are: 1. [sent-21, score-0.841]
</p><p>12 the added computational complexity associated with textural characteristics compared to simpler visual attributes such as color and intensity, particularly if one were to analyze and compare all possible texture pattern pairings in the image in a direct fashion. [sent-23, score-1.053]
</p><p>13 999997777799777  Prior work that incorporated textural characteristics [25] has attempted to address these two issues by making use of lowlevel filter-based texture features and relied on image segmentation to reduce computational complexity while enforcing feature coherence within local regions. [sent-24, score-1.042]
</p><p>14 However, the reliance on advanced pre-processing algorithms such as image segmentation means that the computational complexity and performance of the saliency detection method depends heavily on the properties of the segmentation method used, even if oversegmentation is performed. [sent-25, score-0.422]
</p><p>15 Therefore, an efficient method for performing saliency detection based explicitly on descriptive textural characteristics that does not rely on additional pre-processing would be much desired. [sent-26, score-1.233]
</p><p>16 The main contribution of this paper is the introduction of a novel approach to saliency detection based on the concept of statistical textural distinctiveness. [sent-27, score-1.235]
</p><p>17 Rotational-invariant neighborhood-based texture representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. [sent-28, score-0.876]
</p><p>18 Based on the learnt sparse texture model, a statistical textural distinctiveness graphical model is constructed to characterize the distinctiveness between all texture atom pairs. [sent-29, score-1.944]
</p><p>19 Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms within the image, their respective statistical textural distinctiveness based on the constructed graphical model,  and general visual attentive constraints. [sent-30, score-2.012]
</p><p>20 By incorporating sparse texture modeling within a statistical textural distinctiveness framework, the proposed approach is designed to take explicit advantage of the textural characteristics in the image to detect salient regions in an efficient yet characteristic manner. [sent-31, score-2.336]
</p><p>21 To the best of the authors’ knowledge, the use of sparse texture modeling within a statistical textural distinctiveness framework to characterize and compare textural characteristics within an image for the purpose of saliency detection has not been previously proposed or investigated. [sent-32, score-2.564]
</p><p>22 Related Work Existing saliency are either biologically motivated, computational oriented, or perform local or global analysis of contrast using intensity only, and/or different colorspaces. [sent-34, score-0.385]
</p><p>23 Biologically inspired techniques [13, 10] for saliency detection are commonly based on the approach of Koch et al. [sent-35, score-0.392]
</p><p>24 All these approaches are designed to identify salient regions with high visual stimuli, but tend to blur saliency maps and to highlight local features such as small objects. [sent-40, score-0.59]
</p><p>25 [8] proposed to extract the residuals  of input images in either the amplitude or phase spectrum of input images, and to use the residuals to construct saliency maps in the spatial domain. [sent-44, score-0.414]
</p><p>26 Local saliency detection methods usually evaluate saliency of input image with respect to small neighborhoods. [sent-48, score-0.762]
</p><p>27 [5] generates a color histogram of the entire image, and compute the saliency based on the dissimilarity between the histogram bins, and also use image segmentation for improving saliency estimation. [sent-64, score-0.771]
</p><p>28 However, both image segmentation and abstraction remove textural information which might indicate salient regions in images. [sent-67, score-0.957]
</p><p>29 [25] (LR) explicitly incorporated textural characteristics obtained from lowlevel filter-based texture features of segmented regions for saliency detection. [sent-69, score-1.448]
</p><p>30 The textural characteristic of a region is represented by a feature vector, which all together build a feature matrix. [sent-70, score-0.784]
</p><p>31 However, none of these approaches explicitly consider  rotational-invariant neighborhood-based texture representations (atoms) for salient region detection. [sent-73, score-0.395]
</p><p>32 In contrast to approaches that rely on image segmentation and image abstraction where each region can only characterize a small area in the image, each learnt sparse texture atom can represent large or disjoint regions without explicit spatial context. [sent-74, score-0.441]
</p><p>33 The overall good performance of LR is the result of three strong priors applied to saliency computation. [sent-76, score-0.37]
</p><p>34 lized in the form of a sparsified radially-sorted textural representation based upon the work by Li et al. [sent-89, score-0.803]
</p><p>35 This form of textural representation has been found to be beneficial in striking a balance between robustness to distortional variations and preservation of spatial-intensity context, making it well-suited for local textural representation in the proposed  × ×  work (see Section 4, Fig. [sent-91, score-1.538]
</p><p>36 The sparsified radially-sorted textural representation can be described as follows. [sent-94, score-0.803]
</p><p>37 e G image aI n(exi)g, htbheo corresponding leodc aalt textural representation hc(x) for each color channel c can be defined as:  hc(x) = hIc. [sent-98, score-0.769]
</p><p>38 An illustration of this local textural representation for single channel images is shown in Fig. [sent-102, score-0.769]
</p><p>39 bg images, we whi hsh to produce a compact version of this local textural representation to increase the variance between the elements of the texture descriptor and to improve the efficiency of the subsequent sparse texture model and statistical textural distinctive model stages. [sent-108, score-2.056]
</p><p>40 In this work, a sparsified textural representation t(x) is produced by taking the u principal components of the local textural representation h(x) with the highest variance using PCA: t(x) hΦi(h(x)) | 1 ≤ i≤ u i, (2)  =  where Φi is the ith principal component of h(x). [sent-109, score-1.632]
</p><p>41 We selected the u principal components of h(x) that represent 95% of the variance of all textural representations as suggested for many machine learning approaches [4]. [sent-111, score-0.848]
</p><p>42 Sparse texture model via texture learning Given the set of M N local texture feature representationGs vexetnra thceted se ftr oofm M Mthe × image afl( texx) : T  = {t1, t2, t3,  . [sent-114, score-0.573]
</p><p>43 ,  tM×N}  ,  (3)  let us now define a global texture model to represent the heterogeneous textural characteristics for the entire image f(x). [sent-117, score-1.048]
</p><p>44 One simple strategy to construct such a global texture model is to simply utilize the entire set of extracted local textural representations. [sent-118, score-0.946]
</p><p>45 To address this issue, we first generalize a natural image as being composed of a set of areas where a particular texture pattern is repeated over each area, where the number of areas with unique texture patterns is much smaller than the total number of pixels within the image. [sent-120, score-0.447]
</p><p>46 Based on this gen-  ×  eralization of a natural image, we can then establish a textural sparsity assumption for natural images, where the global textural characteristics of an image can be well-represented by a small set of distinctive local textural representations. [sent-121, score-2.397]
</p><p>47 This compact, sparse representation of the global, heterogeneous textural characteristics of an image motivates the use of a sparse texture model. [sent-122, score-1.144]
</p><p>48 is significantly reduced since only the representative texture atoms need to be analyzed (e. [sent-126, score-0.421]
</p><p>49 sAhsi plast ears presented oin 1 S/2ec·tiMon · 4N, a Mset · oNf m 1=) 2re0l representative texture atoms is an appropriate choice to represent the global textural characteristics of natural images. [sent-131, score-1.269]
</p><p>50 Statistical textural distinctiveness model construction  graphical  In natural images, salient regions of interest can be characterized as regions that are visually distinct from the rest of the scene in terms of their visual attributes. [sent-134, score-1.354]
</p><p>51 In this work, we first consider a salient region of interest as regions that have highly unique and distinctive textural characteristics when compared to the rest of the scene (see Fig. [sent-135, score-1.057]
</p><p>52 As such, we are motivated to introduce a metric for quantifying the uniqueness and distinctiveness of texture patterns within an image relative to each other. [sent-137, score-0.502]
</p><p>53 Here, we introduce the con-  cept of statistical textural distinctiveness, where an area of interest is salient if it has low textural pattern commonality compared to the rest of the scene. [sent-138, score-1.796]
</p><p>54 As such, the concept of statistical textural distinctiveness takes explicit advantage of the statistical relationships between texture patterns within an image to discern underlying saliency. [sent-139, score-1.431]
</p><p>55 Given the learnt sparse texture model, let us first define the statistical textural distinctiveness between two texture patterns. [sent-140, score-1.535]
</p><p>56 Lettir andtjr denote apairofrepresentativetexture atoms in the sparse texture model. [sent-141, score-0.392]
</p><p>57 Suppose that tir can be seen as a realization of tjr in the presence of noise: tjr = tir ηi,j , (6) where ηi,j is a noise process between the representative texture atoms tir and tjr following some distribution P(ηi,j). [sent-142, score-1.17]
</p><p>58 If the noise process ηi,j is assumed to be independent and identically distributed, the probability of tir being a realization of tjr can be written as:  +  P(tir|tjr) =  YP(tir,k|tjr,k),  (7)  Yk  where tir,k is the kth element in the texture atom tir. [sent-143, score-0.562]
</p><p>59 , var( ktrj − tir kp), as it was froepunreds eton provide strong saliency ,d veaterc(tkiton− performance. [sent-147, score-0.561]
</p><p>60 Given the aforementioned definition of statistical textural distinctiveness, one can then construct a weighted graphical model to characterize all pair-wise statistical textural distinctiveness within the sparse texture model of the image, which can be described as follows. [sent-149, score-2.219]
</p><p>61 Let G be a weighted complete graph defined by G = {V, E}, where V is the set cofo m vleetreti gcreasp representing tGhe = representative rteex Vtur ise hateo msest and E is the set of edges representing every pair of representative texture atoms in the sparse texture model. [sent-150, score-0.751]
</p><p>62 Each edge ei,j is associated with a weight equal to the statistical textural distinctiveness (βi,j) between a pair of representative texture atoms tir and tjr. [sent-151, score-1.682]
</p><p>63 Saliency map computation  m(m2−1)  Using the aforementioned statistical textural distinctiveness graphical model, and complimented by general visual attentive constraints, the saliency map for an image I(x) can now be computed based on the following extended as-  sumptions: 1. [sent-154, score-1.578]
</p><p>64 Salient objects are associated with texture patterns that are highly distinct from that of the rest of the scene (statistical textural distinctiveness). [sent-155, score-0.994]
</p><p>65 Given these two key assumptions, the saliency of a representative texture atom tir (which we will denote as αi) can be computed as the product of: 1. [sent-158, score-0.889]
</p><p>66 the expected statistical textural distinctiveness of tir given the image I(x), and 2. [sent-159, score-1.261]
</p><p>67 the weighted spatial proximity of pixels whose texture patterns represented in the sparse texture model by tir (i. [sent-160, score-0.622]
</p><p>68 , Si) to the center of the image (denoted by xc) as suggested by [14], As such, the saliency αi can be defined in the context of the proposed work:  αi= jXm=1βi,jP (tir|I(x))! [sent-162, score-0.39]
</p><p>69 Only m saliency computations are needed, one for each representative texture atom in the sparse texture model. [sent-165, score-0.955]
</p><p>70 As such, the computational complexity of the saliency computations is independent of the size of the image and thus scales linearly (i. [sent-166, score-0.37]
</p><p>71 , O(m)) as the number of texture atom in the sparse texture model increases, not as the image size increases. [sent-168, score-0.515]
</p><p>72 The occurrence probability of texture atoms P (tri |I(x)) used to compute the saliency of each representative tseedxtu tore a ctoommp only tnheeed ssa tlioe bncey computed once per image. [sent-170, score-0.818]
</p><p>73 Experimental Results To investigate the potential of our proposed statistical texture distinctiveness approach (TD) for robustly detecting salient regions, we evaluated our method based on the public EPFL database [1]. [sent-172, score-0.686]
</p><p>74 It contains 1000 natural images with accurate human-marked labels as ground truth, and is widely used as a benchmark for comparing saliency approaches, e. [sent-173, score-0.395]
</p><p>75 In this paper, we  compared our approach with 12 state-of-the-art saliency detection methods. [sent-176, score-0.392]
</p><p>76 Our textural distinctiveness approach (TD, dashed line) achieves the state-of-the-art precision and recall rates. [sent-195, score-1.118]
</p><p>77 98) of the variance of all textural representations, for sorted (SRT) and unsorted (UTR) textural representations. [sent-198, score-1.585]
</p><p>78 Following this scheme, we performed binary segmentation of saliency maps using each possible fixed threshold ∆ to compute precision-recall curves in a first experiment. [sent-200, score-0.444]
</p><p>79 In a first experiment, we segmented the saliency maps using a fixed threshold ∆fix ∈ [0, 255] to obtain binary images, with highlighting regions w [0it,h2 saliency bvtaaliune bs larger mthaagne ∆fix as foreground. [sent-204, score-0.813]
</p><p>80 Our approach also benefits from the rotational-invariant sorted textural representations (STR) which help to better reduce the influence of cluttered or textured background on saliency computation, as compared to an implementation with unsorted textural representations (UTR) as shown in Fig. [sent-213, score-2.042]
</p><p>81 Increasing the number of atoms to 50 does not improve recall and precision, whereas 5 atoms might be to few for representing the texture characteristic of natural images. [sent-221, score-0.614]
</p><p>82 In the second experiment, we applied an image dependent threshold on the saliency maps to segment salient regions. [sent-223, score-0.549]
</p><p>83 [1] defined this threshold as twice the mean of saliency maps S(x), i. [sent-225, score-0.41]
</p><p>84 However, a closer analysis of the saliency maps o·bEta(iSn(exd) s)h. [sent-228, score-0.388]
</p><p>85 oHwoewde tvheart, the distribution of saliency values follows a Gaussian mixture model, with non-salient values having larger probabilities than salient values. [sent-229, score-0.509]
</p><p>86 In comparison to other approaches, the textural distinctiveness scheme can detect more salient regions with high precision. [sent-235, score-1.194]
</p><p>87 ocd) Precision, recall and F-measure for cut-based (GrabCut [23]) segmentation of salient objects, initialized with saliency maps  from all tested saliency approaches. [sent-238, score-0.962]
</p><p>88 Figure 7: GrabCut  segmentation  [23] based on statistical  textural distinctiveness. [sent-239, score-0.843]
</p><p>89 From left to right: Input image, saliency map computed with our approach, segmented image after adaptive thresholding, and GrabCut segmentation. [sent-240, score-0.396]
</p><p>90 precision such as the region contrast (RC) saliency approach [5] and low-rank (LR) saliency approach [25]. [sent-241, score-0.801]
</p><p>91 [5] suggested to perform GrabCut [23] as a post processing step on thresholded saliency maps. [sent-245, score-0.39]
</p><p>92 However, this depends on the chosen saliency appraoch, and requires prior knowlegde which is difficult to extract from unknown images. [sent-247, score-0.37]
</p><p>93 6c also shows that our method (TD) achieves the best precision and F-measure due to the consideration of texture and the sparse texture model for saliency computation, which can help to reduce the influence of cluttered background on saliency computation. [sent-254, score-1.226]
</p><p>94 Conclusions  In this paper, a novel saliency detection approach for natural images based on the concept of statistical texture distinctiveness was presented. [sent-256, score-0.963]
</p><p>95 Experimental results using a public natural image dataset demonstrated strong potential for identifying salient regions in images in an efficient manner, thus illustrating the usefulness of explicitly incorporating textural characteristics. [sent-257, score-0.986]
</p><p>96 Future work involves investigating alternative sparse textural representation and textural models to evaluate whether improvements in saliency detection can be achieved. [sent-258, score-1.97]
</p><p>97 This also involves investigating schemes for automatically determining the optimal number of textural representations, i. [sent-259, score-0.768]
</p><p>98 , number of atoms, which explicitly take into account textural relationships between individual representations for better sparse texture model learning. [sent-261, score-1.065]
</p><p>99 Furthermore, it would also be of great interest in exploring the extension of the proposed statistical textural distinctiveness approach to higher-dimensional data such as volumetric data as well as video data. [sent-262, score-1.109]
</p><p>100 Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. [sent-323, score-0.392]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('textural', 0.755), ('saliency', 0.37), ('distinctiveness', 0.267), ('texture', 0.191), ('tir', 0.166), ('atoms', 0.16), ('salient', 0.139), ('atom', 0.092), ('tjr', 0.076), ('statistical', 0.073), ('representative', 0.07), ('characteristics', 0.068), ('grabcut', 0.064), ('ada', 0.063), ('attentive', 0.056), ('lr', 0.055), ('recall', 0.05), ('precision', 0.046), ('td', 0.045), ('koch', 0.042), ('sparse', 0.041), ('hc', 0.038), ('unsorted', 0.038), ('heterogeneous', 0.034), ('achanta', 0.034), ('attention', 0.034), ('sparsified', 0.034), ('regions', 0.033), ('representations', 0.032), ('graphical', 0.028), ('quantifying', 0.028), ('textured', 0.028), ('relationships', 0.028), ('occurrence', 0.027), ('tri', 0.026), ('adaptive', 0.026), ('natural', 0.025), ('ktrj', 0.025), ('waterloo', 0.025), ('pattern', 0.024), ('realization', 0.023), ('threshold', 0.022), ('utr', 0.022), ('observer', 0.022), ('pages', 0.022), ('characterize', 0.022), ('detection', 0.022), ('variance', 0.022), ('itti', 0.022), ('sf', 0.022), ('rc', 0.021), ('epfl', 0.02), ('suggested', 0.02), ('principal', 0.019), ('curves', 0.019), ('rest', 0.019), ('pca', 0.018), ('std', 0.018), ('perazzi', 0.018), ('explicitly', 0.018), ('maps', 0.018), ('learnt', 0.017), ('commonality', 0.017), ('proximity', 0.017), ('dissimilarities', 0.017), ('cluttered', 0.017), ('cheng', 0.017), ('patterns', 0.016), ('zoomed', 0.016), ('dissimilarity', 0.016), ('public', 0.016), ('abstraction', 0.015), ('visual', 0.015), ('sorted', 0.015), ('thresholds', 0.015), ('segmentation', 0.015), ('biologically', 0.015), ('highlight', 0.015), ('region', 0.015), ('concept', 0.015), ('aforementioned', 0.014), ('characteristic', 0.014), ('interest', 0.014), ('identically', 0.014), ('representing', 0.014), ('xc', 0.014), ('hou', 0.014), ('segmenting', 0.014), ('distinctive', 0.014), ('representation', 0.014), ('manner', 0.013), ('distinct', 0.013), ('lowlevel', 0.013), ('residuals', 0.013), ('underlying', 0.013), ('neighborhood', 0.013), ('investigating', 0.013), ('visually', 0.013), ('si', 0.013), ('duan', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="411-tfidf-1" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>2 0.34541777 <a title="411-tfidf-2" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>3 0.34025839 <a title="411-tfidf-3" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>4 0.3335003 <a title="411-tfidf-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.32562566 <a title="411-tfidf-5" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>6 0.30179748 <a title="411-tfidf-6" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>7 0.29540467 <a title="411-tfidf-7" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>8 0.22963226 <a title="411-tfidf-8" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>9 0.17103313 <a title="411-tfidf-9" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>10 0.15404925 <a title="411-tfidf-10" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>11 0.13168873 <a title="411-tfidf-11" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>12 0.12891889 <a title="411-tfidf-12" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>13 0.11771066 <a title="411-tfidf-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.092983246 <a title="411-tfidf-14" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>15 0.082458377 <a title="411-tfidf-15" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>16 0.075306304 <a title="411-tfidf-16" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>17 0.049069829 <a title="411-tfidf-17" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>18 0.047273032 <a title="411-tfidf-18" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>19 0.046483353 <a title="411-tfidf-19" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>20 0.046310306 <a title="411-tfidf-20" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.148), (2, 0.349), (3, 0.219), (4, -0.096), (5, -0.02), (6, -0.023), (7, -0.048), (8, 0.042), (9, 0.03), (10, -0.01), (11, 0.028), (12, -0.033), (13, -0.014), (14, -0.001), (15, 0.016), (16, 0.006), (17, 0.004), (18, 0.009), (19, 0.006), (20, 0.002), (21, 0.004), (22, -0.011), (23, 0.004), (24, -0.012), (25, -0.021), (26, -0.013), (27, -0.019), (28, -0.023), (29, 0.018), (30, 0.011), (31, -0.011), (32, -0.029), (33, -0.023), (34, -0.026), (35, -0.011), (36, -0.004), (37, 0.015), (38, -0.042), (39, 0.029), (40, 0.012), (41, 0.021), (42, 0.061), (43, -0.026), (44, -0.003), (45, -0.014), (46, 0.03), (47, 0.009), (48, 0.002), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94939905 <a title="411-lsi-1" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>Author: Long Mai, Yuzhen Niu, Feng Liu</p><p>Abstract: A variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First, different methods perform differently in saliency analysis. Second, the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neighboringpixels. To account for the dependence of aggregation on an individual image, our approach selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods.</p><p>same-paper 2 0.94423252 <a title="411-lsi-2" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>3 0.94257748 <a title="411-lsi-3" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>4 0.92877436 <a title="411-lsi-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.89408356 <a title="411-lsi-5" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>6 0.8926509 <a title="411-lsi-6" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>7 0.78284305 <a title="411-lsi-7" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>8 0.77897871 <a title="411-lsi-8" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>9 0.76409352 <a title="411-lsi-9" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>10 0.61678857 <a title="411-lsi-10" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>11 0.57580632 <a title="411-lsi-11" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>12 0.52077723 <a title="411-lsi-12" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>13 0.36546636 <a title="411-lsi-13" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>14 0.36414024 <a title="411-lsi-14" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>15 0.34785986 <a title="411-lsi-15" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>16 0.29727352 <a title="411-lsi-16" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>17 0.26926699 <a title="411-lsi-17" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>18 0.25708905 <a title="411-lsi-18" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>19 0.25575408 <a title="411-lsi-19" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>20 0.25405073 <a title="411-lsi-20" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.089), (16, 0.024), (26, 0.042), (33, 0.224), (40, 0.23), (67, 0.128), (69, 0.038), (76, 0.013), (80, 0.011), (87, 0.051), (89, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81199086 <a title="411-lda-1" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>Author: Rui Caseiro, Pedro Martins, João F. Henriques, Fátima Silva Leite, Jorge Batista</p><p>Abstract: In the past few years there has been a growing interest on geometric frameworks to learn supervised classification models on Riemannian manifolds [31, 27]. A popular framework, valid over any Riemannian manifold, was proposed in [31] for binary classification. Once moving from binary to multi-class classification thisparadigm is not valid anymore, due to the spread of multiple positive classes on the manifold [27]. It is then natural to ask whether the multi-class paradigm could be extended to operate on a large class of Riemannian manifolds. We propose a mathematically well-founded classification paradigm that allows to extend the work in [31] to multi-class models, taking into account the structure of the space. The idea is to project all the data from the manifold onto an affine tangent space at a particular point. To mitigate the distortion induced by local diffeomorphisms, we introduce for the first time in the computer vision community a well-founded mathematical concept, so-called Rolling map [21, 16]. The novelty in this alternate school of thought is that the manifold will be firstly rolled (without slipping or twisting) as a rigid body, then the given data is unwrapped onto the affine tangent space, where the classification is performed.</p><p>same-paper 2 0.80227333 <a title="411-lda-2" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>3 0.78153479 <a title="411-lda-3" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>4 0.77572417 <a title="411-lda-4" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>5 0.76762831 <a title="411-lda-5" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>Author: Magnus Burenius, Josephine Sullivan, Stefan Carlsson</p><p>Abstract: We consider the problem of automatically estimating the 3D pose of humans from images, taken from multiple calibrated views. We show that it is possible and tractable to extend the pictorial structures framework, popular for 2D pose estimation, to 3D. We discuss how to use this framework to impose view, skeleton, joint angle and intersection constraints in 3D. The 3D pictorial structures are evaluated on multiple view data from a professional football game. The evaluation is focused on computational tractability, but we also demonstrate how a simple 2D part detector can be plugged into the framework.</p><p>6 0.76407766 <a title="411-lda-6" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>7 0.76318276 <a title="411-lda-7" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>8 0.76176506 <a title="411-lda-8" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>9 0.76150155 <a title="411-lda-9" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>10 0.76112276 <a title="411-lda-10" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>11 0.7600348 <a title="411-lda-11" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>12 0.75916415 <a title="411-lda-12" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>13 0.75671732 <a title="411-lda-13" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>14 0.75446635 <a title="411-lda-14" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>15 0.75372434 <a title="411-lda-15" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>16 0.75330228 <a title="411-lda-16" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>17 0.75139767 <a title="411-lda-17" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>18 0.74859929 <a title="411-lda-18" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>19 0.74852991 <a title="411-lda-19" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>20 0.74802357 <a title="411-lda-20" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
