<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2001-Optimising Synchronisation Times for Mobile Devices</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-140" href="#">nips2001-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 nips-2001-Optimising Synchronisation Times for Mobile Devices</h1>
<br/><p>Source: <a title="nips-2001-140-pdf" href="http://papers.nips.cc/paper/2012-optimising-synchronisation-times-for-mobile-devices.pdf">pdf</a></p><p>Author: Neil D. Lawrence, Antony I. T. Rowstron, Christopher M. Bishop, Michael J. Taylor</p><p>Abstract: With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. ill this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 1</p><p>Reference: <a title="nips-2001-140-reference" href="../nips2001_reference/nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract With the increasing number of users of mobile computing devices (e. [sent-16, score-0.32]
</p><p>2 personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. [sent-18, score-0.264]
</p><p>3 Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. [sent-19, score-0.506]
</p><p>4 ill this paper we explore the question of the optimal strategy for synchronising such replicas. [sent-20, score-0.108]
</p><p>5 We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. [sent-21, score-0.381]
</p><p>6 We then formulate objective functions which can be minimised with respect to the synchronisation timings. [sent-22, score-0.373]
</p><p>7 We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. [sent-23, score-0.341]
</p><p>8 1  Introduction  As the available bandwidth for wireless devices increases, new challenges are presented in the utilisation of such bandwidth. [sent-24, score-0.128]
</p><p>9 Given that always up connections are generally considered infeasible an important area of research in mobile devices is the development of intelligent strategies for communicating between mobile devices and servers. [sent-25, score-0.564]
</p><p>10 ill this paper we consider the scenario where we are interested in maintaining, on a personal digital assistant (PDA) with wireless access, an up-to-date replica of some, perhaps disparate, data-structures which are evolving in time. [sent-26, score-0.202]
</p><p>11 The objective is to make sure our replica is not 'stale'. [sent-27, score-0.209]
</p><p>12 Each synchronisation involves a reconciliation between the replica on the mobile device and the data-structures of interest on the server. [sent-29, score-0.737]
</p><p>13 Later in the paper we shall examine two typical examples of such an application,an internet news database and a user's e-mail messages. [sent-30, score-0.341]
</p><p>14 We will make the timings of the synchronisations adaptable by developing a cost function that can be optimised with respect to the timings, thereby improving system performance. [sent-36, score-0.448]
</p><p>15 2  Cost Function  We wish to minimise the staleness of the replica, where we define staleness as the time between an update of a portion of the data-structure on the server and the time of the synchronisation of that update with the PDA. [sent-37, score-0.999]
</p><p>16 Thus, after synchronisation the replica on the mobile device is consistent with the master copy on the server. [sent-39, score-0.737]
</p><p>17 Therefore, if skis the time of the kth synchronisation in a day, and updates to the data-structure occur at times Uj then the average staleness of the updates transferred during synchronisation Sk will be  (1) As well as staleness, we may be interested in optimising other criteria. [sent-40, score-1.152]
</p><p>18 For example, mobile phone companies may seek to equalise demand across the network by introducing time varying costs for the synchronisations, c(t). [sent-41, score-0.371]
</p><p>19 Additionally one could argue that there is little point in keeping the replica fresh during periods when the user is unlikely to check his PDA, for example when he or she is sleeping. [sent-42, score-0.532]
</p><p>20 Unfortunately, of course, whilst we are likely to have knowledge of the call cost schedule, c(t), we won't know the true timings {Uj} and {ai} and the cost function will be a priori incomputable. [sent-45, score-0.421]
</p><p>21 If, though, we have historic data2 relating to these times, we can seek to make progress by modelling these timings probabilistically. [sent-46, score-0.38]
</p><p>22 Then, rather than minimising the actual cost function, we can look to minimise the expectation of the cost function under these probabilistic models. [sent-47, score-0.302]
</p><p>23 3  Expected Cost  There are several different possibilities for our modelling strategy. [sent-48, score-0.168]
</p><p>24 e-mail and business news can be modelled separately), however, there may be dependencies between update times which occur within the same part. [sent-51, score-0.572]
</p><p>25 The 2When modelling user ru:cess times, if historic data is not available, models could also be constructed by querying the user about their likely ru:tivities. [sent-52, score-0.896]
</p><p>26 periodicity of the data may be something we can take advantage of, but any nonstationarity in the data may cause problems. [sent-53, score-0.102]
</p><p>27 Naturally more than one update may occur in that interval, therefore our probability distribution really specifies a distribution over time given one that one update (or user access) has occurred. [sent-58, score-0.373]
</p><p>28 We now have an objective function which is a function of the variables we wish to optimise, the synchronisation times, but whilst we have mentioned some characteristics of the models Pu(t) and Pa(t) we have not yet fully specified their form. [sent-64, score-0.424]
</p><p>29 In effect we are modelling data which is mapped to a circle. [sent-66, score-0.168]
</p><p>30 In order to maintain periodicity, we must select a basis function for our KDE which represents a distribution on a circle, one simple way of achieving this aim is to wrap a distribution that is defined along a line to the circle (Mardia, 1972). [sent-68, score-0.126]
</p><p>31 A traditional density which represents a distribution on the line, p(t), may be wrapped around a circle of circumference T to give us a distribution defined on the circle, p( 0), where 0 = t mod T. [sent-69, score-0.165]
</p><p>32 The wrapped Gaussian distribution 3 that we make use of takes the form  (6) The final kernel density estimate thus consists of mapping the data points tn  -t  On  3In practice we must approximate the wrapped distribution by restricting the number of terms in the sum. [sent-71, score-0.16]
</p><p>33 S 1)520 ~  OJ  H  Co>  OJ ""0  Thu Fri Sat Sun  Thu Fri Sat Sun  ~ -20  Figure 1: Left: part of the KDE developed for the business category together with a piecewise constant approximation. [sent-74, score-0.201]
</p><p>34 Right: percent decrease in staleness vs number of synchronisations per day for e-mail data. [sent-76, score-0.484]
</p><p>35 IT we were modelling financial market's news, for example, we would expect weekdays to have similar characteristics to each other, but differing characteristics from the weekend. [sent-83, score-0.26]
</p><p>36 We then modelled these two categories separately, making sure that we maintained a continuous function for the whole week by wrapping basis functions between weekdays and weekends. [sent-89, score-0.393]
</p><p>37 Secondly we split the datainto weekdays, Saturdays and Sundays, modelling each category separately and again wrapping basis functions across the days. [sent-90, score-0.319]
</p><p>38 To select the basis function widths, and to determine which periodicity assumption best matched the data, we ·utilised ten fold cross validation. [sent-93, score-0.102]
</p><p>39 For each different periodicity we used cross validation to first select the basis function width. [sent-94, score-0.102]
</p><p>40 We then compared the average likelihood across the ten validation sets, selecting the periodicity with the highest associated value. [sent-95, score-0.137]
</p><p>41 4  Optimising the Synchronisation Times  Given that our user model, Pa(t), and our data model, Pu(t) will be a KDE based on wrapped Gaussians, we should be in a position to compute the required integrals  '"  C/J  Q)  Q)  t{360 >=1  t{360 >=1   <  ! [sent-96, score-0.421]
</p><p>42 ,  4 X  "0  ~  j  ~  C/J  Q)  2  2 X  X  -20  Figure 2: Results from the news database tests. [sent-103, score-0.341]
</p><p>43 The lower line of the box represents the 25th percentile of the data, the upper line the 75th percentile and the central line the median. [sent-108, score-0.251]
</p><p>44 24  X X  xx  Q)  "0  X  X X  ~40  X  X  -60  +  xx X  xXx  in (5) and evaluate our objective function and derivatives thereof. [sent-116, score-0.18]
</p><p>45 Secondly, integrating across the cost function results in an objective function which is dependent on a large number of evaluations of the cumulative Gaussian distribution. [sent-120, score-0.218]
</p><p>46 Given that we envisage that such optimisations could be occurring within a PDA or mobile phone, it would seem prudent to seek a simpler approach to the required minimisation. [sent-121, score-0.247]
</p><p>47 We chose to reduce the optimisation to a series of one-dimensional line minimisations. [sent-127, score-0.096]
</p><p>48 First, note that the objective function, as a function of a particular synchronisation time Sk, may be written:  (8)  In other words, each synchronisation is only dependent on that of its neighbours. [sent-129, score-0.684]
</p><p>49 We may therefore perform the optimisation by visiting each synchronisation time, Sk, in a random order and optimising its position between its neighbours, which involves a one dimensional line minimisation of (8). [sent-130, score-0.487]
</p><p>50 5  Results  In this section we mainly explore the effectiveness of modelling the data-structures of interest. [sent-132, score-0.206]
</p><p>51 We will briefly touch upon the utility of modelling the cost evolution and user accesses in Section 5. [sent-133, score-0.719]
</p><p>52 1  Modelling Data Structures  To determine the effectiveness of our approach, we utilised two different sources of data: a news web-site and e-mail on a mail server. [sent-136, score-0.358]
</p><p>53 The news database data-set was collected from the BBC News web site4 . [sent-137, score-0.444]
</p><p>54 We had six months of data from February to July 2000 for 24 categories of the database. [sent-140, score-0.138]
</p><p>55 We modelled the data by decomposing it into the different categories and modelling each separately. [sent-141, score-0.35]
</p><p>56 This allowed us to explore the periodicity of each category independently. [sent-142, score-0.184]
</p><p>57 Two extreme examples are Business news and FA Carling Premiership news 5 , Figure 1. [sent-145, score-0.578]
</p><p>58 Business news predominantly arrives during the week whereas FA Carling Premiership news arrives typically just after soccer games finish on a Saturday. [sent-146, score-0.703]
</p><p>59 Business news was best modelled on a Weekday/Weekend basis, and FA Carling Premiership news was best modelled on a Weekday /Saturday /Sunday basis. [sent-147, score-0.74]
</p><p>60 To restrict our investigations to the nature of the data evolution only, user access frequency was taken to be uniform and cost of connection was considered to be constant. [sent-150, score-0.576]
</p><p>61 For the decision step we considered 1 to 24 synchronisations a day. [sent-151, score-0.162]
</p><p>62 The synchronisation times were optimised for each category separately, they were initialised with a uniformly-spaced strategy, optimisation of the timings then proceeded as described in Section 4. [sent-152, score-0.695]
</p><p>63 The staleness associated with these timings was then computed for the third month. [sent-153, score-0.382]
</p><p>64 This value was compared with the staleness resulting from the uniformly-spaced strategy containing the same number of synchronisations 6 . [sent-154, score-0.486]
</p><p>65 The percentage decrease in staleness is shown in figures 2 and 3 in the form of box-plots. [sent-155, score-0.254]
</p><p>66 Generally an improvement in performance is observed, however, we note that in Figure 3 the performance for several categories is extremely 4http://news. [sent-160, score-0.101]
</p><p>67 6The uniformly-spaced strategy's staleness varies with the timing of the first of the K synchronisations. [sent-165, score-0.254]
</p><p>68 This figure was therefore an average of the staleness from all possible starting points taken at five minute intervals. [sent-166, score-0.33]
</p><p>69 The other poor performers are also sports related categories exhibiting non-stationarities. [sent-172, score-0.101]
</p><p>70 The e-mail data-set was collected by examining the logs of e-mail arrival times for 9 researchers from Microsoft's Cambridge research lab. [sent-173, score-0.156]
</p><p>71 In practice, a user is more likely to be interested in a combination of different categories of data. [sent-177, score-0.442]
</p><p>72 Perhaps several different streams of news and his e-mail. [sent-178, score-0.289]
</p><p>73 Therefore, to recreate a more realistic situation where a user has a combination of interests, we also collected e-mail arrivals for three users from February, March and April 2000. [sent-179, score-0.487]
</p><p>74 We randomly generated user profiles by sampling, without replacement, five categories from the available twenty-seven, rejecting samples where more than one e-mail stream was selected. [sent-180, score-0.478]
</p><p>75 We then modelled the users' interests by constructing an unweighted mixture of the five categories and proceeded to optimise the synchronisation times based on this model. [sent-181, score-0.717]
</p><p>76 The average staleness for the different numbers of synchronisations per day is shown in Figure 4. [sent-183, score-0.484]
</p><p>77 Note that the performance for the combined categories is worse than it is for each individually. [sent-184, score-0.101]
</p><p>78 2  Affect of Cost and User Model  In the previous sections we focussed on modelling the evolution of the databases. [sent-187, score-0.202]
</p><p>79 Here we now briefly turn our attention to the other portions of the system, user behaviour and connection cost. [sent-188, score-0.375]
</p><p>80 For this preliminary study, it proved difficult to obtain high quality data representing user access times. [sent-189, score-0.421]
</p><p>81 We therefore artificially generated a model which represents a user who accesses there device frequently at breakfast, lunchtime and during the evening, and rarely at night. [sent-190, score-0.466]
</p><p>82 Figure 4 simply shows the user model, Pa(t), along with the result of optimising the cost function for uniform data arrivals and fixed cost under this user model. [sent-191, score-1.064]
</p><p>83 Note how synchronisation times are suggested just before high periods of user activity are about to occur. [sent-192, score-0.77]
</p><p>84 Currently most mobile internet access providers appear to be charging a flat fee for call costs (typically in the U. [sent-194, score-0.289]
</p><p>85 However, when demand on their systems rise they may wish to incorporate a varying cost to flatten peak demands. [sent-197, score-0.176]
</p><p>86 This cost could be an actual cost for the user, or alternatively a 'shadow price' specified by service provider for controlling demand (Kelly, 2000). [sent-198, score-0.297]
</p><p>87 We give a simple example of such a call cost in Figure 4. [sent-199, score-0.121]
</p><p>88 For this we considered user access and data update rates to be constant. [sent-200, score-0.421]
</p><p>89 Note how the times move away from periods of high cost. [sent-201, score-0.118]
</p><p>90 7The uniformly-spaced strategy can be shown to be optimal when the entropy of the underlying distribution is maximised (a uniform distribution across the interval). [sent-202, score-0.105]
</p><p>91 S  00:00  08:00  16:00  00:00  X  -20  Figure 4: Left: change in synchronisation times for variable user access rates. [sent-219, score-0.806]
</p><p>92 Middle: change in synchronisation times for a variable cost. [sent-221, score-0.385]
</p><p>93 Right: performance improvements for the combination of news and e-mail. [sent-222, score-0.289]
</p><p>94 6  Discussion  The optimisation strategy we suggest could be sensitive to local minima, we did not try a range of different initialisations to explore this phenomena. [sent-223, score-0.169]
</p><p>95 However, by initialising with the uniformly-spaced strategy we ensured that we increased the objective function relative to the standard strategy. [sent-224, score-0.132]
</p><p>96 The system we have explored in this work assumed that the data replicated on the mobile device was only modified on the server. [sent-227, score-0.311]
</p><p>97 Typical applications of such technology include mobile databases, where sales personnel modify portions of the database whilst on the road, and a calendar application on a PDA, where the user adds appointments on the PDA. [sent-229, score-0.687]
</p><p>98 Finally there are many other applications of this type of technology beyond mobile devices. [sent-230, score-0.209]
</p><p>99 Towards a better understanding of web resources and server responses for improved caching. [sent-263, score-0.123]
</p><p>100 On the scale and performance of co-operative web proxy caching. [sent-275, score-0.101]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('user', 0.341), ('synchronisation', 0.311), ('news', 0.289), ('staleness', 0.254), ('mobile', 0.209), ('modelling', 0.168), ('carling', 0.162), ('pda', 0.162), ('premiership', 0.162), ('synchronisations', 0.162), ('replica', 0.147), ('sk', 0.131), ('timings', 0.128), ('cost', 0.121), ('kde', 0.121), ('periodicity', 0.102), ('categories', 0.101), ('business', 0.096), ('weekdays', 0.092), ('february', 0.092), ('fa', 0.091), ('week', 0.085), ('pa', 0.082), ('modelled', 0.081), ('optimising', 0.08), ('wrapped', 0.08), ('access', 0.08), ('times', 0.074), ('percentile', 0.073), ('devices', 0.073), ('strategy', 0.07), ('device', 0.07), ('utilised', 0.069), ('day', 0.068), ('server', 0.068), ('objective', 0.062), ('optimisation', 0.061), ('piecewise', 0.061), ('minimise', 0.06), ('arrivals', 0.06), ('xx', 0.059), ('pu', 0.056), ('demand', 0.055), ('accesses', 0.055), ('wireless', 0.055), ('web', 0.055), ('database', 0.052), ('portion', 0.052), ('whilst', 0.051), ('circle', 0.051), ('collected', 0.048), ('fri', 0.046), ('historic', 0.046), ('kelly', 0.046), ('mardia', 0.046), ('mikhailov', 0.046), ('proxy', 0.046), ('thu', 0.046), ('willis', 0.046), ('updates', 0.045), ('category', 0.044), ('periods', 0.044), ('articles', 0.044), ('uj', 0.041), ('utilise', 0.04), ('proceeded', 0.04), ('interests', 0.04), ('soccer', 0.04), ('wolman', 0.04), ('minute', 0.04), ('rowstron', 0.04), ('maintain', 0.04), ('cl', 0.039), ('seek', 0.038), ('users', 0.038), ('separately', 0.038), ('explore', 0.038), ('july', 0.037), ('aa', 0.037), ('ru', 0.037), ('sat', 0.037), ('months', 0.037), ('optimised', 0.037), ('five', 0.036), ('across', 0.035), ('line', 0.035), ('mod', 0.034), ('phone', 0.034), ('portions', 0.034), ('neil', 0.034), ('road', 0.034), ('arrival', 0.034), ('optimise', 0.034), ('wrapping', 0.034), ('evolution', 0.034), ('operating', 0.032), ('sun', 0.032), ('replicated', 0.032), ('january', 0.032), ('occur', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="140-tfidf-1" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>Author: Neil D. Lawrence, Antony I. T. Rowstron, Christopher M. Bishop, Michael J. Taylor</p><p>Abstract: With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. ill this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 1</p><p>2 0.18734559 <a title="140-tfidf-2" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>Author: Tommi Jaakkola, Hava T. Siegelmann</p><p>Abstract: In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration. 1</p><p>3 0.12360072 <a title="140-tfidf-3" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><p>4 0.11255993 <a title="140-tfidf-4" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>Author: Charles Lee Isbell Jr., Christian R. Shelton</p><p>Abstract: We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modiﬁng his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment. 1</p><p>5 0.074996471 <a title="140-tfidf-5" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>6 0.068332791 <a title="140-tfidf-6" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>7 0.065564074 <a title="140-tfidf-7" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>8 0.055641819 <a title="140-tfidf-8" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>9 0.053205226 <a title="140-tfidf-9" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>10 0.046681304 <a title="140-tfidf-10" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>11 0.046312239 <a title="140-tfidf-11" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>12 0.044886276 <a title="140-tfidf-12" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>13 0.043487802 <a title="140-tfidf-13" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>14 0.042792097 <a title="140-tfidf-14" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>15 0.041293092 <a title="140-tfidf-15" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>16 0.040418662 <a title="140-tfidf-16" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>17 0.039231107 <a title="140-tfidf-17" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>18 0.037860237 <a title="140-tfidf-18" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>19 0.03689643 <a title="140-tfidf-19" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>20 0.036732238 <a title="140-tfidf-20" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.141), (1, -0.005), (2, 0.02), (3, -0.074), (4, -0.013), (5, -0.046), (6, -0.014), (7, 0.048), (8, -0.039), (9, 0.019), (10, 0.159), (11, -0.066), (12, -0.118), (13, -0.019), (14, -0.015), (15, 0.089), (16, 0.161), (17, 0.074), (18, -0.133), (19, 0.038), (20, 0.076), (21, 0.029), (22, -0.1), (23, -0.002), (24, -0.209), (25, 0.048), (26, -0.023), (27, -0.215), (28, -0.077), (29, -0.067), (30, -0.025), (31, -0.034), (32, 0.039), (33, -0.021), (34, 0.081), (35, -0.009), (36, 0.048), (37, 0.187), (38, 0.076), (39, 0.043), (40, -0.072), (41, 0.012), (42, 0.107), (43, 0.07), (44, -0.162), (45, -0.129), (46, 0.001), (47, -0.065), (48, 0.006), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94612509 <a title="140-lsi-1" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>Author: Neil D. Lawrence, Antony I. T. Rowstron, Christopher M. Bishop, Michael J. Taylor</p><p>Abstract: With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. ill this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 1</p><p>2 0.75774062 <a title="140-lsi-2" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>Author: Charles Lee Isbell Jr., Christian R. Shelton</p><p>Abstract: We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modiﬁng his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment. 1</p><p>3 0.67132437 <a title="140-lsi-3" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>Author: Tommi Jaakkola, Hava T. Siegelmann</p><p>Abstract: In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration. 1</p><p>4 0.67078549 <a title="140-lsi-4" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><p>5 0.27671942 <a title="140-lsi-5" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>6 0.26042876 <a title="140-lsi-6" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>7 0.23768552 <a title="140-lsi-7" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>8 0.23361039 <a title="140-lsi-8" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>9 0.23074934 <a title="140-lsi-9" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>10 0.22794236 <a title="140-lsi-10" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>11 0.22324142 <a title="140-lsi-11" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>12 0.2220539 <a title="140-lsi-12" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>13 0.21257077 <a title="140-lsi-13" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>14 0.21029805 <a title="140-lsi-14" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>15 0.2052699 <a title="140-lsi-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.20519805 <a title="140-lsi-16" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>17 0.20517644 <a title="140-lsi-17" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>18 0.20407178 <a title="140-lsi-18" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>19 0.19867182 <a title="140-lsi-19" href="./nips-2001-A_Bayesian_Model_Predicts_Human_Parse_Preference_and_Reading_Times_in_Sentence_Processing.html">5 nips-2001-A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing</a></p>
<p>20 0.19384193 <a title="140-lsi-20" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.051), (17, 0.03), (19, 0.029), (27, 0.104), (30, 0.062), (38, 0.017), (59, 0.028), (69, 0.374), (72, 0.077), (79, 0.044), (83, 0.015), (91, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8026312 <a title="140-lda-1" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>Author: Neil D. Lawrence, Antony I. T. Rowstron, Christopher M. Bishop, Michael J. Taylor</p><p>Abstract: With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. ill this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 1</p><p>2 0.66273057 <a title="140-lda-2" href="./nips-2001-Grammatical_Bigrams.html">86 nips-2001-Grammatical Bigrams</a></p>
<p>Author: Mark A. Paskin</p><p>Abstract: Unsupervised learning algorithms have been derived for several statistical models of English grammar, but their computational complexity makes applying them to large data sets intractable. This paper presents a probabilistic model of English grammar that is much simpler than conventional models, but which admits an efficient EM training algorithm. The model is based upon grammatical bigrams, i.e. , syntactic relationships between pairs of words. We present the results of experiments that quantify the representational adequacy of the grammatical bigram model, its ability to generalize from labelled data, and its ability to induce syntactic structure from large amounts of raw text. 1</p><p>3 0.46203727 <a title="140-lda-3" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>4 0.44381687 <a title="140-lda-4" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>5 0.44196323 <a title="140-lda-5" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>Author: Mário Figueiredo</p><p>Abstract: In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classiﬁcation. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparsenesscontrolling hyper-parameters.</p><p>6 0.43683314 <a title="140-lda-6" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>7 0.43643457 <a title="140-lda-7" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>8 0.43591058 <a title="140-lda-8" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>9 0.43525332 <a title="140-lda-9" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>10 0.43345147 <a title="140-lda-10" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>11 0.43314844 <a title="140-lda-11" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>12 0.43242234 <a title="140-lda-12" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>13 0.43239075 <a title="140-lda-13" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>14 0.43230253 <a title="140-lda-14" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>15 0.43164957 <a title="140-lda-15" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>16 0.43150738 <a title="140-lda-16" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>17 0.43132868 <a title="140-lda-17" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>18 0.43126675 <a title="140-lda-18" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>19 0.43093395 <a title="140-lda-19" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>20 0.43081921 <a title="140-lda-20" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
