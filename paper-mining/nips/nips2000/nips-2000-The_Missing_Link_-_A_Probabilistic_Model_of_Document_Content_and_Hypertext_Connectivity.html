<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-136" href="#">nips2000-136</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</h1>
<br/><p>Source: <a title="nips-2000-136-pdf" href="http://papers.nips.cc/paper/1846-the-missing-link-a-probabilistic-model-of-document-content-and-hypertext-connectivity.pdf">pdf</a></p><p>Author: David A. Cohn, Thomas Hofmann</p><p>Abstract: We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.</p><p>Reference: <a title="nips-2000-136-reference" href="../nips2000_reference/nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. [sent-5, score-1.176]
</p><p>2 The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. [sent-6, score-0.884]
</p><p>3 Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. [sent-7, score-0.376]
</p><p>4 Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis. [sent-8, score-0.567]
</p><p>5 1 Introduction No text, no paper, no book can be isolated from the all-embracing corpus of documents it is embedded in. [sent-9, score-0.277]
</p><p>6 Ideas, thoughts, and work described in a document inevitably relate to and build upon previously published material. [sent-10, score-0.655]
</p><p>7 More recently, a vast number of documents have been "published" electronically on the world wide web; here, interdependencies between documents take the form of hyperlinks, and allow instant access to the referenced material. [sent-12, score-0.641]
</p><p>8 We would like to have some way of modeling these interdependencies, to understand the structure implicit in the contents and connections of a given document base without resorting to manual clustering, classification and ranking of documents. [sent-13, score-0.738]
</p><p>9 The main goal of this paper is to present a joint probabilistic model of document content and connectivity, i. [sent-14, score-0.897]
</p><p>10 , a parameterized stochastic process which mimics the generation of documents as part of a larger collection, and which could make accurate predictions about the existence of hyperlinks and citations. [sent-16, score-0.325]
</p><p>11 Such a model can be extremely useful in many applications, a few of which are: • Identifying topics and common subjects covered by documents. [sent-18, score-0.198]
</p><p>12 documents in a low-dimensional space can help understanding of relations between documents and the topics they cover. [sent-20, score-0.715]
</p><p>13 Combining evidence from terms and links yields potentially more meaningful and stable factors and better predictions. [sent-21, score-0.237]
</p><p>14 The authority of a document is correlated with how frequently it is cited, and by whom. [sent-23, score-0.695]
</p><p>15 By predicting what content might be found "behind" a link, a content/connectivity model directly supports navigation in a document collection, either through interaction with human users or for intelligent spidering. [sent-26, score-0.757]
</p><p>16 Predictions about links based on document contents can support authoring and maintenance of hypertext documents, e. [sent-28, score-0.946]
</p><p>17 Much of this difficulty, which has led to the emergence of an entire new industry, is due to the impoverished explicit structure of the web as a whole. [sent-32, score-0.229]
</p><p>18 Manually created hyperlinks and citations are limited in scope - the annotator can only add links and pointers to other document they are aware of and have access to. [sent-33, score-0.974]
</p><p>19 Moreover, these links are static; once the annotator creates a link between documents, it is unchanging. [sent-34, score-0.328]
</p><p>20 If a different, more relevant document appears (or if the cited document disappears), the link may not get updated appropriately. [sent-35, score-1.466]
</p><p>21 These and other deficiencies make the web inherently "noisy" - links between relevant documents may not exist and existing links might sometimes be more or less arbitrary. [sent-36, score-0.78]
</p><p>22 In Section 3, we show how these two models can be combined into a joint probabilistic term-citation model. [sent-39, score-0.171]
</p><p>23 2 PLSA and PHITS PLSA [7] is a statistical variant of Latent Semantic Analysis (LSA) [4] that builds a factored multinomial model based on the assumption of an underlying document generation process. [sent-42, score-0.719]
</p><p>24 , N ij denotes how often a term (single word or phrase) ti occurs in document dj . [sent-45, score-0.843]
</p><p>25 In LSA, N is decomposed by a SVD and factors are identified with the left/right principal eigenvectors. [sent-46, score-0.166]
</p><p>26 In contrast, PLSA performs a probabilistic decomposition which is closely related to the non-negative matrix decomposition presented in [9]. [sent-47, score-0.177]
</p><p>27 Each factor is identified with a state Zk (1 :::; k :::; K) of a latent variable with associated relative frequency estimates P(ti IZk) for each term in the corpus. [sent-48, score-0.181]
</p><p>28 A document d j is then represented as a convex combination of factors with mixing weights P( Zk Idj ), i. [sent-49, score-0.76]
</p><p>29 , the predictive probabilities for terms in a particular document are constrained to be of the functional form P(ti ldj) = I:k P(ti lzk) P( Zk Idj ), with non-negative probabilities and two sets of normalization constraints I:i P(ti IZk) = 1 for all k and I:k P(Zk Idj ) = 1 for all. [sent-51, score-0.665]
</p><p>30 Both the factors and the document -specific mixing weights are learned by maximizing the likelihood of the observed term frequencies. [sent-53, score-0.76]
</p><p>31 Since factors Zk can be interpreted as states of a latent mixing variable associated with each observation (i. [sent-55, score-0.261]
</p><p>32 Empirically, different factors usually capture distinct "topics" of a document collection; by clustering documents according to their dominant factors, useful topic-specific document clusters often emerge (using the Gaussian factors of LSA, this approach is known as "spectral clustering"). [sent-59, score-1.779]
</p><p>33 It is important to distinguish the factored model used here from standard probabilistic mixture models. [sent-60, score-0.192]
</p><p>34 In a mixture model, each object (such as a document) is usually assumed to come from one of a set of latent sources (e. [sent-61, score-0.134]
</p><p>35 In contrast, a factored model assumes that each object comes from a mixture of sources - without ambiguity, it can assert that a document is half Z l and half Z2. [sent-65, score-0.78]
</p><p>36 This is because the latent variables are associated with each observation and not with each document (set of observations). [sent-66, score-0.729]
</p><p>37 PHITS [3] performs a probabilistic factoring of document citations used for bibliometric analysis. [sent-67, score-0.868]
</p><p>38 Bibliometrics attempts to identify topics in a document collection, as well as influential authors and papers on those topics, based on patterns in citation frequency. [sent-68, score-0.866]
</p><p>39 This analysis has traditionally been applied to references in printed literature, but the same techniques have proven successful in analyzing hyperlink structure on the world wide web [8]. [sent-69, score-0.256]
</p><p>40 Entry Aij is nonzero if and only if document di is cited by document dj or, equivalently, if dj contains a hyperlink to di . [sent-71, score-1.717]
</p><p>41 The coefficient of a document in one of these eigenvectors is interpreted as the "authority" of that document within the community - how likely it is to by cited within that community. [sent-73, score-1.413]
</p><p>42 A document's coefficient in the principal eigenvectors of A' A is interpreted as its "hub" value in the community - how many authoritative documents it cites within the community. [sent-74, score-0.517]
</p><p>43 In PHITS, a probabilistic model replaces the eigenvector analysis, yielding a model that has clear statistical interpretations. [sent-75, score-0.153]
</p><p>44 PHITS is mathematically identical to PLSA, with one distinction: instead of modeling the citations contained within a document (corresponding to PLSA's modeling of terms in a document), PHITS models "inlinks," the citations to a document. [sent-76, score-0.902]
</p><p>45 As with PLSA and spectral clustering, the principal factors of the model are interpreted as indicating the principal citation communities (and by inference, the principal topics). [sent-78, score-0.386]
</p><p>46 For a given factor/topic Z b the probability that a document is cited, P( dj IZk ), is interpreted as the document's authority with respect to that topic. [sent-79, score-0.858]
</p><p>47 3  A Joint Probabilistic Model for Content and Connectivity  Linked and hyperlinked documents are generally composed of terms and citations; as such, both term-based PLSA and citation-based PHITS analyses are applicable. [sent-80, score-0.325]
</p><p>48 Rather than applying each separately, it is reasonable to merge the two analyses into ajoint probabilistic model, explaining terms and citations in terms of a common set of underlying factors . [sent-81, score-0.274]
</p><p>49 Since both PLSA and PHITS are based on a similar decomposition, one can define the following joint model for predicting citationsllinks and terms in documents:  P(til dj ) = LP(ti! [sent-82, score-0.292]
</p><p>50 has some probability P( cllzk) of linking to document dl as well as some probability P(ti lzk) of containing an occurrence of tenn ti. [sent-87, score-0.63]
</p><p>51 Since the mixing proportions are shared, the learned decomposition must be consistent with content and link statistics. [sent-89, score-0.325]
</p><p>52 In particular, this coupling allows the model to take evidence about link structure into account when making predictions about document content and vice versa. [sent-90, score-0.869]
</p><p>53 Once a decomposition is learned, the model may be used to address questions like "What words are likely to be found in a document with this link structure? [sent-91, score-0.859]
</p><p>54 " or "What link structure is likely to go with this document? [sent-92, score-0.143]
</p><p>55 The relative importance one assigns to predicting terms and links will depend on the specific application. [sent-94, score-0.153]
</p><p>56 log LP(CI IZk)P( Zkldj )]  I'  I J  (2)  k  The normalization by term/citation counts ensures that each document is given the same weight in the decomposition, regardless of the number of observations associated with it. [sent-97, score-0.656]
</p><p>57 For the E-step one gets formulae for the posterior probabilities of the latent variables associated with each observation3  4  Experiments  In the introduction, we described many potential applications of the the joint probabilistic model. [sent-99, score-0.27]
</p><p>58 Others, such as intelligent web crawling, are unique to the joint model and require its simultaneous modelling of a document's contents and connections. [sent-101, score-0.44]
</p><p>59 We then describe a quantity called "reference flow" which can be computed from the joint model, and demonstrate its use in guiding a web crawler to pages of interest. [sent-103, score-0.322]
</p><p>60 The WebKB data set [11], consists of approximately 6000 web pages from computer science departments, classified by school and category (student, course, faculty, etc. [sent-136, score-0.23]
</p><p>61 1  Classification  Although the joint probabilistic model performs unsupervised learning, there are a number of ways it may be used for classification. [sent-140, score-0.208]
</p><p>62 One way is to associate each document with its dominant factor, in a form of spectral clustering. [sent-141, score-0.666]
</p><p>63 Test documents are judged by whether their dominant factor shares their label. [sent-143, score-0.389]
</p><p>64 Test documents are judged against the label of their nearest neighbor, but the "nearest" neighbor is determined by cosines of their projections in factor space. [sent-145, score-0.401]
</p><p>65 For the Cora and WebKB data, we used seven factors and six factors respectively, arbitrarily selecting the number to correspond to the number of human-derived classes. [sent-147, score-0.168]
</p><p>66 First, the accuracy of the joint model (where a is neither  onor 1), is greater than that of either model in isolation, indicating that the contents and link structure of a document collection do indeed corroborate each other. [sent-154, score-1.09]
</p><p>67 A document d may be thought of as occupying a point Z = {P(zl ld) , . [sent-159, score-0.63]
</p><p>68 , P(Zk ld)} in the joint model's space of factor mixtures. [sent-162, score-0.145]
</p><p>69 The terms in d act as "signposts" describing Z, and the links act as directed connections between that point and others. [sent-163, score-0.153]
</p><p>70 Together, they provide a reference flow, indicating a referential connection between one topic and another. [sent-164, score-0.215]
</p><p>71 This reference flow exists between arbitrary points in the factor space, even in the absence of documents that map directly to those points. [sent-165, score-0.601]
</p><p>72 1J  depa~ent  faculty ¢::::::l course Consider a reference from document di to document ~ dj , and two points in factor space zm and zn, not particularly associated with di or dj . [sent-169, score-1.885]
</p><p>73 Our model Figure 2: Principal reference allows us to compute P(di lzm) and P(dj l~) , the flow between the primary topics probability that the combination of factors at zm identified in the examined subset and ~ are responsible for di and d j respectively. [sent-170, score-0.663]
</p><p>74 Their product P(dil~) P(dj lzn) is then the probability that the observed link represents a reference between those two points in factor space. [sent-172, score-0.307]
</p><p>75 By integrating over all links in the corpus we can compute, fmn = 2:. [sent-173, score-0.153]
</p><p>76 Figure 2 shows the principal reference flow between several topics in the WebKB archive. [sent-181, score-0.485]
</p><p>77 3  Intelligent Web Crawling with Reference Flow  Let us suppose that we want to find new web pages on a certain topic, described by a set of words composed into a target pseudodocument dt . [sent-184, score-0.263]
</p><p>78 We can project dt into our model to identify the point Zt in factor space that represents that topic . [sent-185, score-0.168]
</p><p>79 Now, when we explore web pages, we want to follow links that will lead us to new documents that also project to £,; . [sent-186, score-0.627]
</p><p>80 Consider a web page ds (or section of a web page4 ). [sent-188, score-0.422]
</p><p>81 Although we don't know where its links point, we do know what words it contains. [sent-189, score-0.153]
</p><p>82 We can then use our model to compute the reference flow fst indicating the (unnormalized) probability that a document at Zs would contain a link to one at Zt . [sent-191, score-1.107]
</p><p>83 350'-~~---t=,u-e~s-ou~m~e-_ ~_~_-' 'placebo'  --- ---  300  250  ~  200  ~  I  150  100  50  102030405060708090100 As a greedy solution, we could simply rank follow links in documents or sections that have the highest reference flow toFigure 3: When ranked according to magward the target topic. [sent-192, score-0.792]
</p><p>84 Or if computation nitude of reference flow to a designated taris no barrier, we could (in theory) use refget, a "true source" scores much higher than erence flow as state transition probabilia placebo source document drawn at random. [sent-193, score-1.123]
</p><p>85 ties and find an optimal link to follow by treating the system as a continuous-state Markov decision process. [sent-194, score-0.143]
</p><p>86 4Tbough not described here, we have had success using our model for document segmentation, following an approach similar to that of [6]. [sent-195, score-0.667]
</p><p>87 By projecting successive n-sentence windows of a document into the factored model, we can observe its trajectory through "topic space. [sent-196, score-0.682]
</p><p>88 " A large jump in the factor mixture between successive windows indicates a probable topic boundary in document. [sent-197, score-0.155]
</p><p>89 To test our model's utility in intelligent web crawling, we conducted experiments on the WebKB data set using the greedy solution. [sent-198, score-0.228]
</p><p>90 One "source page" d s containing a link to the target was identified, and the reference flow 1st computed. [sent-200, score-0.447]
</p><p>91 The larger the reference flow, the stronger our model's expectation that there is a directed link from the source to the target. [sent-201, score-0.284]
</p><p>92 We ranked this flow against the reference flow to the target from 100 randomly chosen "distractor" pages dr1 , dr2 . [sent-202, score-0.529]
</p><p>93 As seen in Figure 3, reference flow provides significant predictive power. [sent-206, score-0.306]
</p><p>94 Note that the dis tractors were not screened to ensure that they did not also contain links to the target; as such, some of the high-ranking dis tractors may also have been valid sources for the target in question. [sent-208, score-0.287]
</p><p>95 5  Discussion and Related Work  There have been many attempts to combine link and term information on web pages, though most approaches are ad hoc and have been aimed at increasing the retrieval of authoritative documents relevant to a given query. [sent-209, score-0.784]
</p><p>96 Bharat and Henzinger [1] provide a good overview of research in that area, as well as an algorithm that computes bibliometric authority after weighting links based on the relevance of the neighboring terms. [sent-210, score-0.266]
</p><p>97 [5] describe a general framework for learning probabilistic relational models from a database, and present experiments in a variety of domains. [sent-213, score-0.135]
</p><p>98 In this paper, we have described a specific probabilistic model which attempts to explain both the contents and connections of documents in an unstructured document base. [sent-214, score-1.106]
</p><p>99 While we have demonstrated preliminary results in several application areas, this paper only scratches the surface of potential applications of a joint probabilistic document model. [sent-215, score-0.801]
</p><p>100 The anatomy of a large-scale hypertextual web search engine. [sent-225, score-0.197]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('document', 0.63), ('documents', 0.277), ('plsa', 0.258), ('web', 0.197), ('phits', 0.178), ('topics', 0.161), ('flow', 0.16), ('links', 0.153), ('link', 0.143), ('dj', 0.131), ('zk', 0.128), ('citations', 0.111), ('reference', 0.111), ('authoritative', 0.097), ('webkb', 0.097), ('joint', 0.092), ('factors', 0.084), ('contents', 0.083), ('probabilistic', 0.079), ('topic', 0.078), ('latent', 0.073), ('authority', 0.065), ('crawling', 0.065), ('idj', 0.065), ('izk', 0.065), ('cited', 0.063), ('content', 0.059), ('relational', 0.056), ('lsa', 0.056), ('principal', 0.053), ('factor', 0.053), ('factored', 0.052), ('ti', 0.052), ('di', 0.05), ('decomposition', 0.049), ('authoring', 0.048), ('bibliometric', 0.048), ('citation', 0.048), ('cora', 0.048), ('hyperlinked', 0.048), ('hyperlinks', 0.048), ('lzk', 0.048), ('zkldj', 0.048), ('retrieval', 0.047), ('mixing', 0.046), ('collection', 0.042), ('faculty', 0.042), ('clustering', 0.038), ('semantic', 0.038), ('model', 0.037), ('sources', 0.037), ('dominant', 0.036), ('identifying', 0.036), ('predictive', 0.035), ('community', 0.034), ('pages', 0.033), ('target', 0.033), ('annotator', 0.032), ('bharat', 0.032), ('bibliometrics', 0.032), ('citationsllinks', 0.032), ('distractor', 0.032), ('hyperlink', 0.032), ('hypertext', 0.032), ('impoverished', 0.032), ('interdependencies', 0.032), ('placebo', 0.032), ('ranked', 0.032), ('tractors', 0.032), ('interpreted', 0.032), ('intelligent', 0.031), ('zm', 0.031), ('word', 0.03), ('source', 0.03), ('connectivity', 0.03), ('identified', 0.029), ('lp', 0.028), ('page', 0.028), ('getoor', 0.028), ('electronically', 0.028), ('proportions', 0.028), ('ld', 0.028), ('papers', 0.027), ('wide', 0.027), ('rank', 0.026), ('cohn', 0.026), ('indicating', 0.026), ('associated', 0.026), ('modeling', 0.025), ('neighbor', 0.025), ('published', 0.025), ('tempered', 0.025), ('student', 0.025), ('median', 0.025), ('zs', 0.025), ('mixture', 0.024), ('eigenvectors', 0.024), ('hoc', 0.023), ('judged', 0.023), ('nearest', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="136-tfidf-1" href="./nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>Author: David A. Cohn, Thomas Hofmann</p><p>Abstract: We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.</p><p>2 0.17402026 <a title="136-tfidf-2" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>Author: Huma Lodhi, John Shawe-Taylor, Nello Cristianini, Christopher J. C. H. Watkins</p><p>Abstract: We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results. 1</p><p>3 0.059047196 <a title="136-tfidf-3" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>Author: Yoshua Bengio, Réjean Ducharme, Pascal Vincent</p><p>Abstract: A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.</p><p>4 0.055687729 <a title="136-tfidf-4" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>Author: John W. Fisher III, Trevor Darrell, William T. Freeman, Paul A. Viola</p><p>Abstract: People can understand complex auditory and visual information, often using one to disambiguate the other. Automated analysis, even at a lowlevel, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates. Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships. We learn the joint distribution of the visual and auditory signals using a non-parametric approach. First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation. We then model the complicated stochastic relationships between the signals using a nonparametric density estimator. These learned densities allow processing across signal modalities. We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video.</p><p>5 0.050514616 <a title="136-tfidf-5" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>Author: Dirk Ormoneit, Hedvig Sidenbladh, Michael J. Black, Trevor Hastie</p><p>Abstract: We present methods for learning and tracking human motion in video. We estimate a statistical model of typical activities from a large set of 3D periodic human motion data by segmenting these data automatically into</p><p>6 0.045935676 <a title="136-tfidf-6" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>7 0.045308273 <a title="136-tfidf-7" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>8 0.044435844 <a title="136-tfidf-8" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>9 0.044348463 <a title="136-tfidf-9" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>10 0.044092964 <a title="136-tfidf-10" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>11 0.040156409 <a title="136-tfidf-11" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>12 0.040103856 <a title="136-tfidf-12" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>13 0.039844565 <a title="136-tfidf-13" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>14 0.039753929 <a title="136-tfidf-14" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>15 0.039034609 <a title="136-tfidf-15" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>16 0.03757092 <a title="136-tfidf-16" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>17 0.035030864 <a title="136-tfidf-17" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>18 0.034788068 <a title="136-tfidf-18" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>19 0.03281787 <a title="136-tfidf-19" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>20 0.032474965 <a title="136-tfidf-20" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, -0.01), (2, 0.053), (3, 0.037), (4, -0.046), (5, 0.062), (6, -0.013), (7, 0.015), (8, -0.037), (9, 0.103), (10, 0.052), (11, 0.041), (12, 0.042), (13, 0.042), (14, -0.031), (15, 0.105), (16, 0.083), (17, -0.063), (18, 0.033), (19, 0.118), (20, 0.077), (21, 0.025), (22, 0.027), (23, -0.097), (24, 0.129), (25, -0.234), (26, 0.012), (27, -0.054), (28, -0.003), (29, 0.213), (30, 0.077), (31, 0.001), (32, 0.062), (33, 0.084), (34, 0.12), (35, 0.07), (36, -0.091), (37, -0.154), (38, -0.268), (39, 0.02), (40, 0.166), (41, -0.258), (42, 0.161), (43, -0.16), (44, 0.014), (45, 0.141), (46, -0.048), (47, 0.162), (48, 0.22), (49, 0.103)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96752721 <a title="136-lsi-1" href="./nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>Author: David A. Cohn, Thomas Hofmann</p><p>Abstract: We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.</p><p>2 0.46790576 <a title="136-lsi-2" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>Author: Huma Lodhi, John Shawe-Taylor, Nello Cristianini, Christopher J. C. H. Watkins</p><p>Abstract: We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results. 1</p><p>3 0.27292433 <a title="136-lsi-3" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>Author: Yoshua Bengio, Réjean Ducharme, Pascal Vincent</p><p>Abstract: A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.</p><p>4 0.24709725 <a title="136-lsi-4" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>Author: Vasin Punyakanok, Dan Roth</p><p>Abstract: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.</p><p>5 0.21470556 <a title="136-lsi-5" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>Author: Marina Meila, Jianbo Shi</p><p>Abstract: We present a new view of image segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation. In particular, we prove that the Normalized Cut method arises naturally from our framework. Finally, the framework provides a principled method for learning the similarity function as a combination of features. 1</p><p>6 0.21208814 <a title="136-lsi-6" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>7 0.20738284 <a title="136-lsi-7" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>8 0.20533982 <a title="136-lsi-8" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>9 0.20126145 <a title="136-lsi-9" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>10 0.18744545 <a title="136-lsi-10" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>11 0.18484524 <a title="136-lsi-11" href="./nips-2000-Ensemble_Learning_and_Linear_Response_Theory_for_ICA.html">46 nips-2000-Ensemble Learning and Linear Response Theory for ICA</a></p>
<p>12 0.18361476 <a title="136-lsi-12" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>13 0.18338923 <a title="136-lsi-13" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>14 0.17767547 <a title="136-lsi-14" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>15 0.1706339 <a title="136-lsi-15" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>16 0.16876315 <a title="136-lsi-16" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>17 0.16618183 <a title="136-lsi-17" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>18 0.16576275 <a title="136-lsi-18" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>19 0.15811327 <a title="136-lsi-19" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>20 0.14813194 <a title="136-lsi-20" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.026), (17, 0.102), (26, 0.015), (32, 0.014), (33, 0.055), (36, 0.013), (53, 0.347), (54, 0.012), (55, 0.037), (62, 0.041), (65, 0.025), (67, 0.047), (75, 0.022), (76, 0.02), (79, 0.019), (81, 0.031), (90, 0.023), (91, 0.035), (97, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83514959 <a title="136-lda-1" href="./nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>Author: David A. Cohn, Thomas Hofmann</p><p>Abstract: We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.</p><p>2 0.37295529 <a title="136-lda-2" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>3 0.36498162 <a title="136-lda-3" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>4 0.36477718 <a title="136-lda-4" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>Author: Yee Whye Teh, Geoffrey E. Hinton</p><p>Abstract: We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.</p><p>5 0.36234525 <a title="136-lda-5" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.</p><p>6 0.36088437 <a title="136-lda-6" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>7 0.35900027 <a title="136-lda-7" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>8 0.35852057 <a title="136-lda-8" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>9 0.35732308 <a title="136-lda-9" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>10 0.35684884 <a title="136-lda-10" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>11 0.35537696 <a title="136-lda-11" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>12 0.35434482 <a title="136-lda-12" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>13 0.35409096 <a title="136-lda-13" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>14 0.35392457 <a title="136-lda-14" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>15 0.35333824 <a title="136-lda-15" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>16 0.35316232 <a title="136-lda-16" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>17 0.35264206 <a title="136-lda-17" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>18 0.35215634 <a title="136-lda-18" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>19 0.35188723 <a title="136-lda-19" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>20 0.35118333 <a title="136-lda-20" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
