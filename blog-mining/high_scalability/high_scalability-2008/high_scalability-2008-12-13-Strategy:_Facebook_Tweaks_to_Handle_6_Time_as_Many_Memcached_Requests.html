<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-464" href="#">high_scalability-2008-464</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-464-html" href="http://highscalability.com//blog/2008/12/13/strategy-facebook-tweaks-to-handle-6-time-as-many-memcached.html">html</a></p><p>Introduction: Our latest strategy is taken from agreat post by Paul Saab of Facebook,
detailing how with changes Facebook has made to memcached they have:...been
able to scale memcached to handle 200,000 UDP requests per second with an
average latency of 173 microseconds. The total throughput achieved is 300,000
UDP requests/s, but the latency at that request rate is too high to be useful
in our system. This is an amazing increase from 50,000 UDP requests/s using
the stock version of Linux and memcached.To scale Facebook has hundreds of
thousands of TCP connections open to their memcached processes. First, this is
still amazing. It's not so long ago you could have never done this. Optimizing
connection use was always a priority because the OS simply couldn't handle
large numbers of connections or large numbers of threads or large numbers of
CPUs. To get to this point is a big accomplishment. Still, at that scale there
are problems that are often solved.Some of the problem Facebook faced and
fixed:Pe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our latest strategy is taken from agreat post by Paul Saab of Facebook, detailing how with changes Facebook has made to memcached they have:. [sent-1, score-0.523]
</p><p>2 been able to scale memcached to handle 200,000 UDP requests per second with an average latency of 173 microseconds. [sent-4, score-0.249]
</p><p>3 To scale Facebook has hundreds of thousands of TCP connections open to their memcached processes. [sent-7, score-0.232]
</p><p>4 Optimizing connection use was always a priority because the OS simply couldn't handle large numbers of connections or large numbers of threads or large numbers of CPUs. [sent-10, score-0.684]
</p><p>5 Some of the problem Facebook faced and fixed:Per connection consumption of resources. [sent-13, score-0.176]
</p><p>6 What works well at low number of inputs can totally kill a system as inputs grow. [sent-14, score-0.264]
</p><p>7 Memcached uses a per- connection buffer which adds up to a lot of memory that could be used to store data. [sent-15, score-0.29]
</p><p>8 Nothing wrong with this design choice, but Facebook made changes to use a per-thread shared connection buffer and reclaimed gigabytes of RAM on each server. [sent-16, score-0.565]
</p><p>9 Facebook discovered under load there was lock contention when transmitting through a single UDP socket from multiple threads. [sent-18, score-0.485]
</p><p>10 Sockets are data structures too and they are subject to the usual lock contention issues. [sent-19, score-0.391]
</p><p>11 Facebook got around this issue by maintaining separate reply sockets in different threads so they would not contend with the receive sockets. [sent-20, score-0.371]
</p><p>12 They changed the dequeue algorithm to batch dequeues so more work was done when they had the CPU. [sent-22, score-0.272]
</p><p>13 Nothing brings out lock issues like moving to more cores. [sent-24, score-0.286]
</p><p>14 Facebook found when they moved to 8 core machines a global lock protecting stats collection used 20-30% of CPU usage. [sent-25, score-0.46]
</p><p>15 In application that require little processing per request, as does memcached, this is not unexpected, but doing real work with your CPU is a better idea. [sent-26, score-0.167]
</p><p>16 So they collected stats on a per thread basis and then calculated a global view on demand. [sent-27, score-0.189]
</p><p>17 Notice using a nifty new parallel language or moving to a cloud wouldn't have made a bit difference. [sent-35, score-0.189]
</p><p>18 When you have the CPU do all the work you possibly can in the quantum or the whole system grinds to a halt in processing overhead. [sent-46, score-0.248]
</p><p>19 Otherwise locking for shared resources takes more and more time when there's less and less time to do the work that needs to be done. [sent-48, score-0.322]
</p><p>20 You can find their changes ongithub, the hub that says "git. [sent-52, score-0.183]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('udp', 0.296), ('lock', 0.286), ('facebook', 0.264), ('connection', 0.176), ('memcached', 0.156), ('sockets', 0.138), ('inputs', 0.132), ('paul', 0.127), ('cpu', 0.125), ('profiling', 0.123), ('numbers', 0.116), ('buffer', 0.114), ('floods', 0.109), ('dequeue', 0.109), ('contention', 0.105), ('made', 0.103), ('halt', 0.102), ('ongithub', 0.102), ('tanks', 0.102), ('hub', 0.097), ('stats', 0.096), ('tcp', 0.095), ('detailing', 0.094), ('transmitting', 0.094), ('per', 0.093), ('changed', 0.089), ('multiplexing', 0.088), ('resources', 0.088), ('nifty', 0.086), ('reclaimed', 0.086), ('changes', 0.086), ('strategies', 0.085), ('threads', 0.084), ('agreat', 0.084), ('regression', 0.083), ('flood', 0.081), ('proportion', 0.081), ('less', 0.08), ('protecting', 0.078), ('contend', 0.078), ('interrupts', 0.078), ('connections', 0.076), ('work', 0.074), ('quantum', 0.072), ('tweaking', 0.071), ('nothing', 0.071), ('reply', 0.071), ('sits', 0.07), ('offload', 0.069), ('stock', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="464-tfidf-1" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>Introduction: Our latest strategy is taken from agreat post by Paul Saab of Facebook,
detailing how with changes Facebook has made to memcached they have:...been
able to scale memcached to handle 200,000 UDP requests per second with an
average latency of 173 microseconds. The total throughput achieved is 300,000
UDP requests/s, but the latency at that request rate is too high to be useful
in our system. This is an amazing increase from 50,000 UDP requests/s using
the stock version of Linux and memcached.To scale Facebook has hundreds of
thousands of TCP connections open to their memcached processes. First, this is
still amazing. It's not so long ago you could have never done this. Optimizing
connection use was always a priority because the OS simply couldn't handle
large numbers of connections or large numbers of threads or large numbers of
CPUs. To get to this point is a big accomplishment. Still, at that scale there
are problems that are often solved.Some of the problem Facebook faced and
fixed:Pe</p><p>2 0.1915437 <a title="464-tfidf-2" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>Introduction: For solutions take a look at:7 Life Saving Scalability Defenses Against Load
Monster Attacks.This is a look at all the bad things that can happen to your
carefully crafted program as loads increase: all hell breaks lose. Sure, you
can scale out or scale up, but you can also choose to program better. Make
your system handle larger loads. This saves money because fewer boxes are
needed and it will make the entire application more reliable and have better
response times. And it can be quite satisfying as a programmer.Large Number Of
ObjectsWe usually get into scaling problems when the number of objects gets
larger. Clearly resource usage of all types is stressed as the number of
objects grow.Continuous Failures Makes An Infinite Event StreamDuring large
network failure scenarios there is never time for the system recover. We are
in a continual state of stress.Lots of High Priority WorkFor example,
rerouting is a high priority activity. If there is a large amount of rerouting
work that can</p><p>3 0.18549258 <a title="464-tfidf-3" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>Introduction: There's some amount of debate whether Facebookreallycrossed over the one
trillion page view per month threshold. While one report says it did,another
respected firm says it did not; that its monthly page views are a mere 467
billion per month.In the big scheme of things, the discrepancy is somewhat
irrelevant, as neither show thetrueload on Facebook's infrastructure - which
is far more impressive a set of numbers than its externally measured "page
view" metric.  Mashable reported in "Facebook Surpasses 1 Trillion Pageviews
per Month" that the social networking giant saw "approximately 870 million
unique visitors in June and 860 million in July" and followed up with some per
visitor statistics, indicating "each visitor averaged approximately 1,160 page
views in July and 40 per visit -- enormous by any standard. Time spent on the
site was around 25 minutes per user."From an architectural standpoint it's
notjustabout the page views. It's about requests and responses, many of which
occur u</p><p>4 0.18485948 <a title="464-tfidf-4" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>Introduction: There's not a lot  of talk about application architectures at the process
level. You have your threads, pools of threads, and you have your callback
models. That's about it. Languages/frameworks making a virtue out of simple
models, like Go and Erlang, do so at the price of control. It's difficult to
make a low latency well conditioned application when a power full tool, like
work scheduling, is taken out of the hands of the programmer.But that's not
all there is my friend. We'll dive into different ways an application can be
composed across threads of control.Your favorite language may not give you
access to all the capabilities we are going to talk about, but lately there
has been a sort of revival in considering performance important, especially
for controlling latency variance, so I think it's time to talk about these
kind of issues. When it was do everything in the thread of a web server thread
pool none of these issues really mattered. But now that developers are
creating sophist</p><p>5 0.17128894 <a title="464-tfidf-5" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have theC10K concurrent connection problemlicked, how do we level
up and support 10 million concurrent connections? Impossible you say. Nope,
systems right now are delivering 10 million concurrent connections using
techniques that are as radical as they may be unfamiliar.To learn how it's
done we turn toRobert Graham, CEO of Errata Security, and his absolutely
fantastic talk atShmoocon 2013calledC10M Defending The Internet At
Scale.Robert has a brilliant way of framing the problem that I've never heard
of before. He starts with a little bit of history, relating how Unix wasn't
originally designed to be a general server OS, it was designed to be a control
system for a telephone network. It was the telephone network that actually
transported the data so there was a clean separation between the control plane
and the data plane. Theproblem is we now use Unix servers as part of the data
plane, which we shouldn't do at all. If we were designing a kernel for
handling one applicati</p><p>6 0.16141103 <a title="464-tfidf-6" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>7 0.15724073 <a title="464-tfidf-7" href="../high_scalability-2010/high_scalability-2010-06-22-Exploring_the_software_behind_Facebook%2C_the_world%E2%80%99s_largest_site.html">845 high scalability-2010-06-22-Exploring the software behind Facebook, the world’s largest site</a></p>
<p>8 0.15638378 <a title="464-tfidf-8" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>9 0.15207493 <a title="464-tfidf-9" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>10 0.15174113 <a title="464-tfidf-10" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>11 0.14964284 <a title="464-tfidf-11" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>12 0.14497665 <a title="464-tfidf-12" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>13 0.14224376 <a title="464-tfidf-13" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>14 0.14038907 <a title="464-tfidf-14" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>15 0.13847324 <a title="464-tfidf-15" href="../high_scalability-2009/high_scalability-2009-06-10-Hive_-_A_Petabyte_Scale_Data_Warehouse_using_Hadoop.html">624 high scalability-2009-06-10-Hive - A Petabyte Scale Data Warehouse using Hadoop</a></p>
<p>16 0.13060611 <a title="464-tfidf-16" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>17 0.13019513 <a title="464-tfidf-17" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>18 0.1300644 <a title="464-tfidf-18" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>19 0.12878838 <a title="464-tfidf-19" href="../high_scalability-2010/high_scalability-2010-11-04-Facebook_at_13_Million_Queries_Per_Second_Recommends%3A_Minimize_Request_Variance.html">934 high scalability-2010-11-04-Facebook at 13 Million Queries Per Second Recommends: Minimize Request Variance</a></p>
<p>20 0.12830909 <a title="464-tfidf-20" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.228), (1, 0.157), (2, -0.01), (3, -0.045), (4, 0.009), (5, -0.015), (6, 0.031), (7, 0.136), (8, -0.08), (9, -0.034), (10, 0.023), (11, 0.043), (12, 0.071), (13, 0.055), (14, -0.086), (15, -0.009), (16, 0.047), (17, 0.004), (18, -0.014), (19, 0.097), (20, 0.089), (21, 0.081), (22, 0.07), (23, -0.016), (24, 0.136), (25, -0.035), (26, 0.07), (27, 0.015), (28, 0.112), (29, 0.016), (30, -0.048), (31, -0.016), (32, 0.054), (33, 0.053), (34, 0.023), (35, 0.057), (36, 0.062), (37, -0.016), (38, -0.013), (39, 0.049), (40, -0.031), (41, 0.036), (42, 0.012), (43, 0.001), (44, -0.001), (45, -0.008), (46, -0.03), (47, -0.024), (48, 0.032), (49, 0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98358309 <a title="464-lsi-1" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>Introduction: Our latest strategy is taken from agreat post by Paul Saab of Facebook,
detailing how with changes Facebook has made to memcached they have:...been
able to scale memcached to handle 200,000 UDP requests per second with an
average latency of 173 microseconds. The total throughput achieved is 300,000
UDP requests/s, but the latency at that request rate is too high to be useful
in our system. This is an amazing increase from 50,000 UDP requests/s using
the stock version of Linux and memcached.To scale Facebook has hundreds of
thousands of TCP connections open to their memcached processes. First, this is
still amazing. It's not so long ago you could have never done this. Optimizing
connection use was always a priority because the OS simply couldn't handle
large numbers of connections or large numbers of threads or large numbers of
CPUs. To get to this point is a big accomplishment. Still, at that scale there
are problems that are often solved.Some of the problem Facebook faced and
fixed:Pe</p><p>2 0.80971706 <a title="464-lsi-2" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>Introduction: Snabb Switch \- a toolkit for solving novel problems in networking. If you are
building a new packet-processing network appliance then you can use Snabb
Switch to get the job done more quickly.Here's a great impassioned overview
from erichocean:Or, you could just avoid the OS
altogether:https://github.com/SnabbCo/snabbswitchOur current engineering
target is 1 million writes/sec and > 10 million reads/sec on top of an
architecture similar to that, on a single box, to our fully transactional,
MVCC database (write do not block reads, and vice versa) that runs in the same
process (a la SQLite), which we've also merged with our application code and
our caching tier, so we're down to--literally--a single process for what would
have been at least three separate tiers in a traditional setup.The result is
that we had to move to measuring request latency in microseconds exclusively.
The architecture (without additional application-specific processing) supports
a wire-to-wire messaging speed of 2</p><p>3 0.78344786 <a title="464-lsi-3" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>4 0.76812822 <a title="464-lsi-4" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update: Erlang at Facebook by Eugene Letuchy. How Facebook uses Erlang to
implement Chat, AIM Presence, and Chat Jabber support. I've done
someXMPPdevelopment so when I readFacebook was making a Jabber chat clientI
was really curious how they would make it work. While core XMPP is
straightforward, a number of protocol extensions like discovery, forms, chat
states, pubsub, multi user chat, and privacy lists really up the
implementation complexity. Some real engineering challenges were involved to
make this puppy scale and perform. It's not clear what extensions they've
implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the
architectural challenges they faced and how they overcame them.A web based
Jabber client poses a few problems because XMPP, like most IM protocols, is an
asynchronous event driven system that pretty much assumes you have a full time
open connection. After logging in the server sends a client roster information
and presence information. Your client</p><p>5 0.76093239 <a title="464-lsi-5" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have theC10K concurrent connection problemlicked, how do we level
up and support 10 million concurrent connections? Impossible you say. Nope,
systems right now are delivering 10 million concurrent connections using
techniques that are as radical as they may be unfamiliar.To learn how it's
done we turn toRobert Graham, CEO of Errata Security, and his absolutely
fantastic talk atShmoocon 2013calledC10M Defending The Internet At
Scale.Robert has a brilliant way of framing the problem that I've never heard
of before. He starts with a little bit of history, relating how Unix wasn't
originally designed to be a general server OS, it was designed to be a control
system for a telephone network. It was the telephone network that actually
transported the data so there was a clean separation between the control plane
and the data plane. Theproblem is we now use Unix servers as part of the data
plane, which we shouldn't do at all. If we were designing a kernel for
handling one applicati</p><p>6 0.75250477 <a title="464-lsi-6" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>7 0.72837824 <a title="464-lsi-7" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>8 0.71970695 <a title="464-lsi-8" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>9 0.70676482 <a title="464-lsi-9" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>10 0.70428783 <a title="464-lsi-10" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>11 0.69941497 <a title="464-lsi-11" href="../high_scalability-2010/high_scalability-2010-12-31-Facebook_in_20_Minutes%3A_2.7M_Photos%2C_10.2M_Comments%2C_4.6M_Messages.html">966 high scalability-2010-12-31-Facebook in 20 Minutes: 2.7M Photos, 10.2M Comments, 4.6M Messages</a></p>
<p>12 0.69748145 <a title="464-lsi-12" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>13 0.6838699 <a title="464-lsi-13" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>14 0.67974997 <a title="464-lsi-14" href="../high_scalability-2014/high_scalability-2014-01-31-Stuff_The_Internet_Says_On_Scalability_For_January_31st%2C_2014.html">1588 high scalability-2014-01-31-Stuff The Internet Says On Scalability For January 31st, 2014</a></p>
<p>15 0.67507911 <a title="464-lsi-15" href="../high_scalability-2012/high_scalability-2012-09-15-4_Reasons_Facebook_Dumped_HTML5_and_Went_Native.html">1323 high scalability-2012-09-15-4 Reasons Facebook Dumped HTML5 and Went Native</a></p>
<p>16 0.67217845 <a title="464-lsi-16" href="../high_scalability-2010/high_scalability-2010-11-04-Facebook_at_13_Million_Queries_Per_Second_Recommends%3A_Minimize_Request_Variance.html">934 high scalability-2010-11-04-Facebook at 13 Million Queries Per Second Recommends: Minimize Request Variance</a></p>
<p>17 0.67125845 <a title="464-lsi-17" href="../high_scalability-2008/high_scalability-2008-09-03-Some_Facebook_Secrets_to_Better_Operations.html">378 high scalability-2008-09-03-Some Facebook Secrets to Better Operations</a></p>
<p>18 0.67024261 <a title="464-lsi-18" href="../high_scalability-2012/high_scalability-2012-04-06-Stuff_The_Internet_Says_On_Scalability_For_April_6%2C_2012.html">1223 high scalability-2012-04-06-Stuff The Internet Says On Scalability For April 6, 2012</a></p>
<p>19 0.66914159 <a title="464-lsi-19" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>20 0.66904145 <a title="464-lsi-20" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.088), (2, 0.296), (10, 0.071), (30, 0.287), (61, 0.051), (79, 0.109), (85, 0.012), (94, 0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96059048 <a title="464-lda-1" href="../high_scalability-2008/high_scalability-2008-02-25-Make_Your_Site_Run_10_Times_Faster.html">261 high scalability-2008-02-25-Make Your Site Run 10 Times Faster</a></p>
<p>Introduction: This is whatMike Peters says he can do: make your site run 10 times faster.
His test bed is "half a dozen servers parsing 200,000 pages per hour over 40
IP addresses, 24 hours a day." Before optimization CPU spiked to 90% with 50
concurrent connections. After optimization each machine "was effectively
handling 500 concurrent connections per second with CPU at 8% and no
degradation in performance."Mike identifies six major bottlenecks:Database
write access (read is cheaper)Database read accessPHP, ASP, JSP and any other
server side scriptingClient side JavaScriptMultiple/Fat Images, scripts or css
files from different domains on your pageSlow keep-alive client connections,
clogging your available socketsMike's solutions:breakSwitch all database
writes to offline processingMinimize number of database read access to the
bare minimum. No more than two queries per page.Denormalize your database and
Optimize MySQL tablesImplement MemCached and change your database-access layer
to fetch infor</p><p>2 0.95640856 <a title="464-lda-2" href="../high_scalability-2010/high_scalability-2010-05-26-End-To-End_Performance_Study_of_Cloud_Services.html">831 high scalability-2010-05-26-End-To-End Performance Study of Cloud Services</a></p>
<p>Introduction: Cloud computing promises a number of advantages for the deployment of data-
intensive applications. Most prominently, these include reducing cost with a
pay-as-you-go pricing model and (virtually) unlimited throughput by adding
servers if the workload increases. At theSystems Group, ETH Zurich, we did an
extensiveend-to-end performance studyto compare the major cloud offerings
regarding their ability to fulfill these promises and their implied cost.The
focus of the work is on transaction processing (i.e., read and update work-
loads), rather than analytics workloads. We used theTPC-W, a standardized
benchmark simulating a Web-shop, as the baseline for our comparison. The TPC-W
defines that users are simulated through emulated browsers (EB) and issue page
requests, called web-interactions (WI), against the system. As a major
modification to the benchmark, we constantly increase the load from 1 to 9000
simultaneous users to measure the scalability and cost variance of the system.
Figure</p><p>3 0.95256305 <a title="464-lda-3" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>Introduction: Looks like an interesting take on "a completely asynchronous, low-latency
transaction management protocol, in line with the fully distributed NoSQL
architecture."Warp: Multi-Key Transactions for Key-Value Stores
overview:Implementing ACID transactions has been a longstanding challenge for
NoSQL systems. Because these systems are based on a sharded architecture,
transactions necessarily require coordination across multiple servers. Past
work in this space has relied either on heavyweight protocols such as Paxos or
clock synchronization for this coordination.This paper presents a novel
protocol for coordinating distributed transactions with ACID semantics on top
of a sharded data store. Called linear transactions, this protocol achieves
scalability by distributing the coordination task to only those servers that
hold relevant data for each transaction. It achieves high performance by
serializing only those transactions whose concurrent execution could
potentially yield a violation of ACI</p><p>4 0.94226545 <a title="464-lda-4" href="../high_scalability-2011/high_scalability-2011-04-04-Scaling_Social_Ecommerce_Architecture_Case_study.html">1016 high scalability-2011-04-04-Scaling Social Ecommerce Architecture Case study</a></p>
<p>Introduction: A recent study showed that over 92 percent of executives from leading
retailers are focusing their marketing efforts on Facebook and subsequent
applications. Furthermore, over 71 percent of users have confirmed they are
more likely to make a purchase after "liking" a brand they find online.
(source)Sears Architect Tomer Gabel provides an insightful overview on how
they built a Social Ecommerce solution for Sears.com that can handle complex
relationship quires in real time. Tomer goes through:the architectural
considerations behind their solutionwhy they chose memory over diskhow they
partitioned the data to gain scalabilitywhy they chose to execute code with
the data using GigaSpaces Map/Reduce execution frameworkhow they integrated
with Facebookwhy they chose GigaSpaces over Coherence and Terracotta for in-
memory caching and scaleIn this post I tried to summarize the main takeaway
from the interview.You can also watch the full interview (highly
recomended).Read the full storyhere</p><p>5 0.94171792 <a title="464-lda-5" href="../high_scalability-2010/high_scalability-2010-02-24-Hot_Scalability_Links_for_February_24%2C_2010.html">783 high scalability-2010-02-24-Hot Scalability Links for February 24, 2010</a></p>
<p>Introduction: Cassandra @ Twitter: An Interview with Ryan King. Great interview by Alex
Popescu on Twitter's thought process for switching to Cassandra. Twitter chose
Cassandra because it had more big system features out of the box. Is that
Cassandra FTW?I Had Downtime Today. Here's What I'm Doing About Itby Patrick
McKenzie. Awesome deep dive into went wrong with Bingo Card Creator. Sh*t
happens. How do you design a process to help prevent it from happening and how
do you deal with problems with integrity when they do?High Availability
Principle : Request Queueingby Ashish Soni. Queue request to ride out traffic
spikes: 1) Request Queuing allows your system to operate at optimal
throughput. 2) Your users only experience linear degradation versus
exponential degradation. 3) Your system experiences NO degradation.pfffft
twatter tweeterby Knowbuddy.The reason you should care [about NoSQL] is
because now you have more options--you're not stuck trying to wedge your
system into a relational model if you</p><p>6 0.93652874 <a title="464-lda-6" href="../high_scalability-2008/high_scalability-2008-05-31-Biggest_Under_Reported_Story%3A_Google%27s_BigTable_Costs_10_Times_Less_than_Amazon%27s_SimpleDB.html">336 high scalability-2008-05-31-Biggest Under Reported Story: Google's BigTable Costs 10 Times Less than Amazon's SimpleDB</a></p>
<p>7 0.9340024 <a title="464-lda-7" href="../high_scalability-2008/high_scalability-2008-02-27-Product%3A_System_Imager_-_Automate_Deployment_and_Installs.html">263 high scalability-2008-02-27-Product: System Imager - Automate Deployment and Installs</a></p>
<p>same-blog 8 0.92928129 <a title="464-lda-8" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>9 0.92512995 <a title="464-lda-9" href="../high_scalability-2010/high_scalability-2010-10-08-4_Scalability_Themes_from_Surgecon.html">917 high scalability-2010-10-08-4 Scalability Themes from Surgecon</a></p>
<p>10 0.92295831 <a title="464-lda-10" href="../high_scalability-2009/high_scalability-2009-01-22-Heterogeneous_vs._Homogeneous_System_Architectures.html">500 high scalability-2009-01-22-Heterogeneous vs. Homogeneous System Architectures</a></p>
<p>11 0.89837581 <a title="464-lda-11" href="../high_scalability-2007/high_scalability-2007-07-16-Book%3A_High_Performance_MySQL.html">16 high scalability-2007-07-16-Book: High Performance MySQL</a></p>
<p>12 0.89363658 <a title="464-lda-12" href="../high_scalability-2011/high_scalability-2011-02-16-Paper%3A_An_Experimental_Investigation_of_the_Akamai_Adaptive_Video_Streaming.html">991 high scalability-2011-02-16-Paper: An Experimental Investigation of the Akamai Adaptive Video Streaming</a></p>
<p>13 0.88566023 <a title="464-lda-13" href="../high_scalability-2008/high_scalability-2008-04-30-Rather_small_site_architecture..html">312 high scalability-2008-04-30-Rather small site architecture.</a></p>
<p>14 0.88389611 <a title="464-lda-14" href="../high_scalability-2007/high_scalability-2007-10-25-Should_JSPs_be_avoided_for_high_scalability%3F.html">131 high scalability-2007-10-25-Should JSPs be avoided for high scalability?</a></p>
<p>15 0.8725121 <a title="464-lda-15" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>16 0.86828089 <a title="464-lda-16" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>17 0.86686242 <a title="464-lda-17" href="../high_scalability-2008/high_scalability-2008-08-08-Separation_into_read-write_only_databases.html">361 high scalability-2008-08-08-Separation into read-write only databases</a></p>
<p>18 0.86054641 <a title="464-lda-18" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>19 0.85615855 <a title="464-lda-19" href="../high_scalability-2012/high_scalability-2012-07-16-Cinchcast_Architecture_-_Producing_1%2C500_Hours_of_Audio_Every_Day.html">1284 high scalability-2012-07-16-Cinchcast Architecture - Producing 1,500 Hours of Audio Every Day</a></p>
<p>20 0.85277992 <a title="464-lda-20" href="../high_scalability-2009/high_scalability-2009-06-06-Graph_server.html">621 high scalability-2009-06-06-Graph server</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
