<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>464 hunch net-2012-05-03-Microsoft Research, New York City</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-464" href="#">hunch_net-2012-464</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>464 hunch net-2012-05-03-Microsoft Research, New York City</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-464-html" href="http://hunch.net/?p=2341">html</a></p><p>Introduction: Yahoo! laid off people. Unlike every previous time there have been layoffs,
this is serious forYahoo! Research.We had advanced warning
fromPrabhakarthrough thesimple act of leaving. Yahoo! Research was a world
class organization that Prabhakar recruited much of personally, so it is
deeply implausible that he would spontaneously decide to leave. My first
thought when I saw the news was "Uhoh,Robsaid that he knew it was serious when
the head of ATnT Research left." In this case it was even more significant,
because Prabhakar recruited me on the premise that Y!R was an experiment in
how research should be done: via a combination of high quality people and high
engagement with the company. Prabhakar's departure is a clear end to that
experiment.The result is ambiguous from a business perspective. Y!R clearly
was not capable of saving the company from its illnesses. I'm not privy to the
internal accounting of impact and this is the kind of subject where there can
easily be great disagreemen</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Research was a world class organization that Prabhakar recruited much of personally, so it is deeply implausible that he would spontaneously decide to leave. [sent-7, score-0.453]
</p><p>2 R was an experiment in how research should be done: via a combination of high quality people and high engagement with the company. [sent-10, score-0.355]
</p><p>3 R clearly was not capable of saving the company from its illnesses. [sent-14, score-0.237]
</p><p>4 It turns out that talking to the rest of the organization doing consulting, architecting, and prototyping on a minority basis helps research by sharpening the questions you ask more than it hinders by taking up time. [sent-22, score-0.465]
</p><p>5 R in a company that has been under prolonged stress was challenging leadership-wise. [sent-28, score-0.231]
</p><p>6 Consequently, the abrupt departure of Prabhakar and an apparent lack of appreciation by the new CEO created a crisis of confidence. [sent-29, score-0.278]
</p><p>7 Many people who were sitting on strong offers quickly left, and everyone else started looking around. [sent-30, score-0.358]
</p><p>8 In this situation, my first concern was for colleagues, both in Machine Learning across the company and the Yahoo! [sent-31, score-0.237]
</p><p>9 Every company and government in the world is drowning in data, and Machine Learning is the prime tool for actually using it to do interesting things. [sent-34, score-0.32]
</p><p>10 More generally, the demand for high quality seasoned machine learning researchers across startups, mature companies, government labs, and academia has been astonishing, and I expect the outcome to reflect that. [sent-35, score-0.425]
</p><p>11 This is remarkably different from the cuts that hit ATnT research in late 2001 and early 2002 where the famous machine learning group there took many months to disperse to new positions. [sent-36, score-0.34]
</p><p>12 While that article is wrong in specifics (we ended up not fired for example, although it is difficult to discern cause and effect), we certainly shook the job tree very hard to see what would fall out. [sent-38, score-0.457]
</p><p>13 To my surprise, amongst all the companies we investigated,Microsofthad a uniquely sufficient agility, breadth of interest, and technical culture, enabling them to make offers that I and a significant fraction of the Y! [sent-39, score-0.238]
</p><p>14 In light of this, I would encourage people in academia to consider Yahoo! [sent-45, score-0.279]
</p><p>15 There are and will be some serious hard feelings about the outcome as various top researchers elsewhere in the organization feel compelled to look for jobs and leave. [sent-47, score-0.447]
</p><p>16 took a real gamble supporting a research organization about 7 years ago, and many positive things have come of this gamble from all perspectives. [sent-49, score-0.778]
</p><p>17 I considered all possibilities in accepting the job and was prepared to simply put aside a job search for some time if necessary, but the timing was surreally perfect. [sent-55, score-0.458]
</p><p>18 Deepakwas sitting on an offer atLinkedinand simply took it, so the disruption there was even more minimal. [sent-59, score-0.331]
</p><p>19 Amongst other things, VW is theultrascale learning algorithm, not the kind of thing that you would want to put aside lightly. [sent-62, score-0.305]
</p><p>20 This surprised me greatly--Microsoft has made serious commitments to supporting open source in various ways and that commitment is what sealed the deal for me. [sent-64, score-0.223]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prabhakar', 0.346), ('yahoo', 0.278), ('departure', 0.207), ('organization', 0.179), ('company', 0.17), ('research', 0.139), ('took', 0.13), ('sitting', 0.123), ('recruited', 0.123), ('serious', 0.121), ('gamble', 0.114), ('academia', 0.111), ('atnt', 0.107), ('job', 0.107), ('york', 0.103), ('supporting', 0.102), ('offers', 0.098), ('surprise', 0.095), ('light', 0.092), ('lab', 0.092), ('government', 0.089), ('possibilities', 0.089), ('ended', 0.089), ('outcome', 0.085), ('aside', 0.085), ('basis', 0.085), ('even', 0.078), ('companies', 0.078), ('would', 0.076), ('deeply', 0.075), ('kind', 0.074), ('high', 0.073), ('else', 0.073), ('new', 0.071), ('experiment', 0.07), ('put', 0.07), ('clearly', 0.067), ('across', 0.067), ('looking', 0.064), ('certainly', 0.062), ('hard', 0.062), ('turns', 0.062), ('amongst', 0.062), ('investigated', 0.061), ('distraction', 0.061), ('noticing', 0.061), ('stress', 0.061), ('prime', 0.061), ('office', 0.061), ('discern', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999958 <a title="464-tfidf-1" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>Introduction: Yahoo! laid off people. Unlike every previous time there have been layoffs,
this is serious forYahoo! Research.We had advanced warning
fromPrabhakarthrough thesimple act of leaving. Yahoo! Research was a world
class organization that Prabhakar recruited much of personally, so it is
deeply implausible that he would spontaneously decide to leave. My first
thought when I saw the news was "Uhoh,Robsaid that he knew it was serious when
the head of ATnT Research left." In this case it was even more significant,
because Prabhakar recruited me on the premise that Y!R was an experiment in
how research should be done: via a combination of high quality people and high
engagement with the company. Prabhakar's departure is a clear end to that
experiment.The result is ambiguous from a business perspective. Y!R clearly
was not capable of saving the company from its illnesses. I'm not privy to the
internal accounting of impact and this is the kind of subject where there can
easily be great disagreemen</p><p>2 0.24864639 <a title="464-tfidf-2" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>Introduction: I will joinYahoo Research(in New York) after my contract ends atTTI-
Chicago.The deciding reasons are:Yahoo is running into many hard learning
problems. This is precisely the situation where basic research might hope to
have the greatest impact.Yahoo Research understands research including
publishing, conferences, etc…Yahoo Research is growing, so there is a chance I
can help it grow well.Yahoo understands the internet, including (but not at
all limited to) experimenting with research blogs.In the end, Yahoo Research
seems like the place where I might have a chance to make the greatest
difference.Yahoo (as a company) has made a strong bet on Yahoo Research. We-
the-researchers all hope that bet will pay off, and this seems plausible. I'll
certainly have fun trying.</p><p>3 0.18854964 <a title="464-tfidf-3" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research? Here are some essential
ingredients that I see.Stability. University-based research is relatively good
at this. On any particular day, researchers face choices in what they will
work on. A very common tradeoff is between:easy smalldifficult bigFor
researchers without stability, the 'easy small' option wins. This is often
"ok"--a series of incremental improvements on the state of the art can add up
to something very beneficial. However, it misses one of the big potentials of
research: finding entirely new and better ways of doing things.Stability comes
in many forms. The prototypical example is tenure at a university--a tenured
professor is almost imposssible to fire which means that the professor has the
freedom to consider far horizon activities. An iron-clad guarantee of a
paycheck is not necessary--industrial research labs have succeeded well with
research positions of indefinite duration. Atnt research was a great example
of th</p><p>4 0.16334844 <a title="464-tfidf-4" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>Introduction: I attended theIBM research 60th anniversary. IBM research is, by any
reasonable account, the industrial research lab which has managed to bring the
most value to it's parent company over the long term. This can be seen by
simply counting the survivors: IBM research is the only older research lab
which has not gone through a period of massive firing. (Note that there are
alsonew research labs.)Despite this impressive record, IBM research has
failed, by far, to achieve it's potential. Examples which came up in this
meeting include:It took about a decade to produce DRAM after it was invented
in the lab. (In fact, Intel produced it first.)Relational databases and SQL
were invented and then languished. It was only under external competition that
IBM released it's own relational database. Why didn't IBM grow anOracle
division?An early lead in IP networking hardware did not result in IBM growing
aCisco division. Why not?And remember â&euro;Ś IBM research is a stark success story
compared to it's com</p><p>5 0.15980589 <a title="464-tfidf-5" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>Introduction: With a worldwide recession on, my impression is that the carnage in research
has not been as severe as might be feared, at least in the United States. I
know of two notable negative impacts:It's quite difficult to get a job this
year, as many companies and universities simply aren't hiring. This is
particularly tough on graduating students.Perhaps 10% ofIBM researchwas
fired.In contrast, around the time of the dot com bust,ATnT
ResearchandLucenthad one or several 50% size firings wiping out much of the
remainder ofBell Labs, triggering a notable diaspora for the respected machine
learning group there. As the recession progresses, we may easily see more
firings as companies in particular reach a point where they can no longer
support research.There are a couple positives to the recession as well.Both
the implosion of Wall Street (which siphoned off smart people) and the general
difficulty of getting a job coming out of an undergraduate education suggest
that the quality of admitted phd</p><p>6 0.15951295 <a title="464-tfidf-6" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>7 0.13414223 <a title="464-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>8 0.13380539 <a title="464-tfidf-8" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>9 0.13008417 <a title="464-tfidf-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.12383781 <a title="464-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>11 0.1205351 <a title="464-tfidf-11" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>12 0.11990102 <a title="464-tfidf-12" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>13 0.11412563 <a title="464-tfidf-13" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>14 0.11161513 <a title="464-tfidf-14" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>15 0.1115533 <a title="464-tfidf-15" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>16 0.10638752 <a title="464-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>17 0.10524217 <a title="464-tfidf-17" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>18 0.10288751 <a title="464-tfidf-18" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>19 0.098558612 <a title="464-tfidf-19" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>20 0.097051091 <a title="464-tfidf-20" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.253), (1, 0.126), (2, 0.148), (3, -0.082), (4, 0.118), (5, 0.002), (6, 0.059), (7, 0.075), (8, 0.035), (9, -0.081), (10, 0.032), (11, 0.071), (12, -0.037), (13, 0.096), (14, -0.016), (15, -0.056), (16, -0.05), (17, -0.039), (18, 0.023), (19, 0.017), (20, -0.016), (21, -0.144), (22, -0.043), (23, -0.022), (24, 0.049), (25, -0.044), (26, -0.047), (27, 0.084), (28, 0.044), (29, -0.017), (30, -0.028), (31, -0.076), (32, 0.022), (33, -0.017), (34, -0.103), (35, 0.049), (36, -0.063), (37, -0.114), (38, -0.026), (39, -0.017), (40, 0.039), (41, -0.009), (42, 0.026), (43, -0.071), (44, 0.04), (45, -0.021), (46, 0.03), (47, 0.052), (48, -0.002), (49, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94916964 <a title="464-lsi-1" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>Introduction: Yahoo! laid off people. Unlike every previous time there have been layoffs,
this is serious forYahoo! Research.We had advanced warning
fromPrabhakarthrough thesimple act of leaving. Yahoo! Research was a world
class organization that Prabhakar recruited much of personally, so it is
deeply implausible that he would spontaneously decide to leave. My first
thought when I saw the news was "Uhoh,Robsaid that he knew it was serious when
the head of ATnT Research left." In this case it was even more significant,
because Prabhakar recruited me on the premise that Y!R was an experiment in
how research should be done: via a combination of high quality people and high
engagement with the company. Prabhakar's departure is a clear end to that
experiment.The result is ambiguous from a business perspective. Y!R clearly
was not capable of saving the company from its illnesses. I'm not privy to the
internal accounting of impact and this is the kind of subject where there can
easily be great disagreemen</p><p>2 0.80913389 <a title="464-lsi-2" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>Introduction: I will joinYahoo Research(in New York) after my contract ends atTTI-
Chicago.The deciding reasons are:Yahoo is running into many hard learning
problems. This is precisely the situation where basic research might hope to
have the greatest impact.Yahoo Research understands research including
publishing, conferences, etc…Yahoo Research is growing, so there is a chance I
can help it grow well.Yahoo understands the internet, including (but not at
all limited to) experimenting with research blogs.In the end, Yahoo Research
seems like the place where I might have a chance to make the greatest
difference.Yahoo (as a company) has made a strong bet on Yahoo Research. We-
the-researchers all hope that bet will pay off, and this seems plausible. I'll
certainly have fun trying.</p><p>3 0.74694747 <a title="464-lsi-3" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>Introduction: I attended theIBM research 60th anniversary. IBM research is, by any
reasonable account, the industrial research lab which has managed to bring the
most value to it's parent company over the long term. This can be seen by
simply counting the survivors: IBM research is the only older research lab
which has not gone through a period of massive firing. (Note that there are
alsonew research labs.)Despite this impressive record, IBM research has
failed, by far, to achieve it's potential. Examples which came up in this
meeting include:It took about a decade to produce DRAM after it was invented
in the lab. (In fact, Intel produced it first.)Relational databases and SQL
were invented and then languished. It was only under external competition that
IBM released it's own relational database. Why didn't IBM grow anOracle
division?An early lead in IP networking hardware did not result in IBM growing
aCisco division. Why not?And remember â&euro;Ś IBM research is a stark success story
compared to it's com</p><p>4 0.69929361 <a title="464-lsi-4" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research? Here are some essential
ingredients that I see.Stability. University-based research is relatively good
at this. On any particular day, researchers face choices in what they will
work on. A very common tradeoff is between:easy smalldifficult bigFor
researchers without stability, the 'easy small' option wins. This is often
"ok"--a series of incremental improvements on the state of the art can add up
to something very beneficial. However, it misses one of the big potentials of
research: finding entirely new and better ways of doing things.Stability comes
in many forms. The prototypical example is tenure at a university--a tenured
professor is almost imposssible to fire which means that the professor has the
freedom to consider far horizon activities. An iron-clad guarantee of a
paycheck is not necessary--industrial research labs have succeeded well with
research positions of indefinite duration. Atnt research was a great example
of th</p><p>5 0.6772005 <a title="464-lsi-5" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>Introduction: The funding of research (and machine learning research) is an issue which
seems to have become more significant in the United States over the last
decade. The word "research" is applied broadly here to science, mathematics,
and engineering.There are two essential difficulties with funding
research:LongshotPaying a researcher is often a big gamble. Most research
projects don't pan out, but a few big payoffs can make it all
worthwhile.Information OnlyMuch of research is about finding the right way to
think about or do something.The Longshot difficulty means that there is high
variance in payoffs. This can be compensated for by funding many different
research projects, reducing variance.The Information-Only difficulty means
that it's hard to extract a profit directly from many types of research, so
companies have difficulty justifying basic research. (Patents are a mechanism
for doing this. They are often extraordinarily clumsy or simply not
applicable.)These two difficulties together imp</p><p>6 0.67428094 <a title="464-lsi-6" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>7 0.66494411 <a title="464-lsi-7" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>8 0.62207115 <a title="464-lsi-8" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>9 0.60341293 <a title="464-lsi-9" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>10 0.59850782 <a title="464-lsi-10" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>11 0.58601689 <a title="464-lsi-11" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>12 0.5748837 <a title="464-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>13 0.57284474 <a title="464-lsi-13" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>14 0.56950837 <a title="464-lsi-14" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>15 0.56719279 <a title="464-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>16 0.56623453 <a title="464-lsi-16" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>17 0.55415606 <a title="464-lsi-17" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>18 0.55204809 <a title="464-lsi-18" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>19 0.54814273 <a title="464-lsi-19" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<p>20 0.54799294 <a title="464-lsi-20" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (2, 0.023), (6, 0.021), (35, 0.332), (38, 0.021), (42, 0.219), (45, 0.031), (68, 0.031), (69, 0.038), (74, 0.088), (76, 0.013), (82, 0.032), (83, 0.011), (95, 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98259366 <a title="464-lda-1" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I've releasedversion 5.0of theVowpal Wabbitonline learning software. The major
number has changed since thelast releasebecause I regard all earlier versions
as obsolete--there are several new algorithms & features including substantial
changes and upgrades to the default learning algorithm.The biggest changes are
new algorithms:Nikosand I improved the default algorithm. The basic update
rule still uses gradient descent, but the size of the update is carefully
controlled so that it's impossible to overrun the label. In addition, the
normalization has changed. Computationally, these changes are virtually free
and yield better results, sometimes much better. Less careful updates can be
reenabled with -loss_function classic, although results are still not
identical to previous due to normalization changes.Nikos also implemented the
per-feature learning rates as per thesetwopapers. Often, this works better
than the default algorithm. It isn't the default because it isn't (yet) as
adaptable</p><p>2 0.96572208 <a title="464-lda-2" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly
scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting
example of the second class of structured classifiers (where the
classification of one label depends on the others), and one of my favorite
papers. I'm not alone: the paper won the best student paper award at NIPS in
2003.There are some things I find odd about the paper. For instance, it says
of probabilistic models"cannot handle high dimensional feature spaces and lack
strong theoretical guarrantees."I'm aware of no such limitations.
Also:"Unfortunately, even probabilistic graphical models that are trained
discriminatively do not achieve the same level of performance as SVMs,
especially when kernel features are used."This is quite interesting and
contradicts my own experience as well as that of a number of
peopleIgreatlyrespect. I wonder what the root cause is: perhaps there is
something different about the data Ben+Carlos were working</p><p>3 0.96494895 <a title="464-lda-3" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>Introduction: The term "boosting" comes from the idea of using a meta-algorithm which takes
"weak" learners (that may be able to only barely predict slightly better than
random) and turn them into strongly capable learners (which predict very
well).Adaboostin 1995 was the first widely used (and useful) boosting
algorithm, although there were theoretical boosting algorithms floating around
since 1990 (see the bottom ofthis page).Since then, many different
interpretations of why boosting works have arisen. There is significant
discussion about these different views in theannals of statistics, including
aresponsebyYoav FreundandRobert Schapire.I believe there is a great deal of
value to be found in the original view of boosting (meta-algorithm for
creating a strong learner from a weak learner). This is not a claim that one
particular viewpoint obviates the value of all others, but rather that no
other viewpoint seems to really capture important properties.Comparing with
all other views of boosting is t</p><p>4 0.9564029 <a title="464-lda-4" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>Introduction: Chicago '05ended a couple of weeks ago. This was the sixthMachine Learning
Summer School, and the second one that used awiki. (The first was Berder '04,
thanks to Gunnar Raetsch.) Wikis are relatively easy to set up, greatly aid
social interaction, and should be used a lot more at summer schools and
workshops. They can even be used as the meeting's webpage, as a permanent
record of its participants' collaborations -- see for example the wiki/website
for last year'sNVO Summer School.A basic wiki is a collection of editable
webpages, maintained by software called awiki engine. The engine used at both
Berder and Chicago wasTikiWiki-- it is well documented and gets you something
running fast. It uses PHP and MySQL, but doesn't require you to know either.
Tikiwiki has far more features than most wikis, as it is really a fullContent
Management System. (My thanks to Sebastian Stark for pointing this out.) Here
are the features we found most useful:Bulletin boards, or forums. The most-
used on</p><p>5 0.95131975 <a title="464-lda-5" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>Introduction: AtICML 2003,Marty Zinkevichproposedthe online convex optimization setting and
showed that a particular gradient descent algorithm has regret O(T0.5) with
respect to the best predictor where T is the number of rounds. This seems to
be a nice model for online learning, and there has been some significant
follow-up work.AtCOLT 2006Elad Hazan,Adam Kalai,Satyen Kale, andAmit
Agarwalpresenteda modification which takes a Newton stepguaranteeing O(log T)
regret when the first and second derivatives are bounded.Then they applied
these algorithms to portfolio managementatICML 2006(withRobert Schapire)
yielding some very fun graphs.</p><p>6 0.94683069 <a title="464-lda-6" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>7 0.92731297 <a title="464-lda-7" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>same-blog 8 0.92114562 <a title="464-lda-8" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>9 0.87834215 <a title="464-lda-9" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>10 0.78536677 <a title="464-lda-10" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>11 0.773013 <a title="464-lda-11" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>12 0.77140313 <a title="464-lda-12" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>13 0.76668584 <a title="464-lda-13" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>14 0.76533246 <a title="464-lda-14" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>15 0.75428766 <a title="464-lda-15" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>16 0.74815422 <a title="464-lda-16" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>17 0.7461437 <a title="464-lda-17" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>18 0.74217635 <a title="464-lda-18" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>19 0.74115473 <a title="464-lda-19" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>20 0.7372759 <a title="464-lda-20" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
