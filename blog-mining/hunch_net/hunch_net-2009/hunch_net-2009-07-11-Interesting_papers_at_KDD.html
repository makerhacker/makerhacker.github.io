<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>364 hunch net-2009-07-11-Interesting papers at KDD</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-364" href="#">hunch_net-2009-364</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>364 hunch net-2009-07-11-Interesting papers at KDD</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-364-html" href="http://hunch.net/?p=835">html</a></p><p>Introduction: I attendedKDDthis year. The conference has always had a strong grounding in
what works based on theKDDcup, but it has developed a halo of workshops on
various subjects. It seems that KDD has become a place where the economy meets
machine learning in a stronger sense than many other conferences.There were
several papers that other people might like to take a look at.Yehuda
KorenCollaborative Filtering with Temporal Dynamics. This paper describes how
to incorporate temporal dynamics into a couple of collaborative filtering
approaches. This was also a best paper award.D. Sculley, Robert Malkin,Sugato
Basu,Roberto J. Bayardo,Predicting Bounce Rates in Sponsored Search
Advertisements. The basic claim of this paper is that the probability people
immediately leave ("bounce") after clicking on an advertisement is
predictable.Frank McSherryandIlya MironovDifferentially Private Recommender
Systems: Building Privacy into the Netflix Prize Contenders. The basic claim
here is that it is possible to</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bounce', 0.308), ('crowdvine', 0.308), ('temporal', 0.253), ('filtering', 0.193), ('privacy', 0.189), ('claim', 0.141), ('economy', 0.137), ('describes', 0.137), ('sponsored', 0.137), ('demonstration', 0.127), ('meets', 0.127), ('grounding', 0.127), ('popularity', 0.127), ('encouraged', 0.127), ('schedule', 0.119), ('integrated', 0.119), ('collaborative', 0.119), ('advertisement', 0.114), ('strong', 0.11), ('beat', 0.109)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="364-tfidf-1" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>Introduction: I attendedKDDthis year. The conference has always had a strong grounding in
what works based on theKDDcup, but it has developed a halo of workshops on
various subjects. It seems that KDD has become a place where the economy meets
machine learning in a stronger sense than many other conferences.There were
several papers that other people might like to take a look at.Yehuda
KorenCollaborative Filtering with Temporal Dynamics. This paper describes how
to incorporate temporal dynamics into a couple of collaborative filtering
approaches. This was also a best paper award.D. Sculley, Robert Malkin,Sugato
Basu,Roberto J. Bayardo,Predicting Bounce Rates in Sponsored Search
Advertisements. The basic claim of this paper is that the probability people
immediately leave ("bounce") after clicking on an advertisement is
predictable.Frank McSherryandIlya MironovDifferentially Private Recommender
Systems: Building Privacy into the Netflix Prize Contenders. The basic claim
here is that it is possible to</p><p>2 0.15865126 <a title="364-tfidf-2" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: Thesecond Netflix prize is canceleddue toprivacy problems. I continue to
believe my original assessment of this paper, that the privacy break was
somewhat overstated. I still haven't seen any serious privacy failures on the
scale of theAOL search log release.I expect privacy concerns to continue to be
a big issue when dealing with data releases by companies or governments. The
theory of maintaining privacy while using data is improving, but it is not yet
in a state where the limits of what's possible are clear let alone how to
achieve these limits in a manner friendly to a prediction competition.</p><p>3 0.13868663 <a title="364-tfidf-3" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>Introduction: There were several papers that seemed fairly interesting atKDD this year. The
ones that caught my attention are:Xin Jin, Mingyang Zhang,Nan Zhang, andGautam
Das,Versatile Publishing For Privacy Preservation. This paper provides a
conservative method for safely determining which data is publishable from any
complete source of information (for example, a hospital) such that it does not
violate privacy rules in a natural language. It is not differentially private,
so no external sources of join information can exist. However, it is a
mechanism forpublishingdata rather than (say) the output of a learning
algorithm.Arik FriedmanAssaf Schuster,Data Mining with Differential Privacy.
This paper shows how to create effective differentially private decision
trees. Progress in differentially private datamining is pretty impressive, as
it wasdefined in 2006.David Chan, Rong Ge, Ori Gershony,Tim Hesterberg,Diane
Lambert,Evaluating Online Ad Campaigns in a Pipeline: Causal Models At
ScaleThis paper</p><p>4 0.13213325 <a title="364-tfidf-4" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>Introduction: Slashdotpoints out theTraffic Prediction Challengewhich looks pretty fun. The
temporal aspect seems to be very common in many real-world problems and
somewhat understudied.</p><p>5 0.11532521 <a title="364-tfidf-5" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At thelast ICML,Tom Dietterichasked me to look into systems for commenting on
papers. I've been slow getting to this, but it's relevant now.The essential
observation is that we now have many tools for online collaboration, but they
are not yet much used in academic research. If we can find the right way to
use them, then perhaps great things might happen, with extra kudos to the
first conference that manages to really create an online community. Various
conferences have been poking at this. For example,UAI has setup a wiki, COLT
hasstarted usingJoomla, with some dynamic content, and AAAI has been setting
up a "student blog". Similarly,Dinoj Surendransetup a twiki for theChicago
Machine Learning Summer School, which was quite useful for coordinating events
and other things.I believe the most important thing is a willingness to
experiment. A good place to start seems to be enhancing existing conference
websites. For example, theICML 2007 papers pageis basically only useful via
grep. A mu</p><p>6 0.11423272 <a title="364-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>7 0.11268613 <a title="364-tfidf-7" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>8 0.10609218 <a title="364-tfidf-8" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>9 0.099889681 <a title="364-tfidf-9" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>10 0.094596639 <a title="364-tfidf-10" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>11 0.092898734 <a title="364-tfidf-11" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>12 0.084392905 <a title="364-tfidf-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.080525935 <a title="364-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>14 0.079528019 <a title="364-tfidf-14" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>15 0.079291508 <a title="364-tfidf-15" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>16 0.078061923 <a title="364-tfidf-16" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>17 0.075172901 <a title="364-tfidf-17" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>18 0.074021317 <a title="364-tfidf-18" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>19 0.073532328 <a title="364-tfidf-19" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>20 0.072772734 <a title="364-tfidf-20" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.077), (2, -0.01), (3, 0.059), (4, -0.016), (5, 0.054), (6, 0.009), (7, -0.013), (8, 0.03), (9, -0.012), (10, -0.132), (11, 0.025), (12, -0.096), (13, 0.091), (14, 0.122), (15, 0.019), (16, 0.031), (17, 0.055), (18, -0.083), (19, -0.03), (20, -0.022), (21, -0.086), (22, -0.132), (23, -0.104), (24, 0.059), (25, 0.048), (26, -0.012), (27, 0.085), (28, 0.047), (29, -0.008), (30, -0.068), (31, -0.024), (32, -0.087), (33, -0.013), (34, -0.096), (35, -0.031), (36, -0.114), (37, 0.112), (38, -0.001), (39, -0.024), (40, -0.009), (41, -0.042), (42, -0.023), (43, 0.02), (44, -0.017), (45, 0.027), (46, 0.035), (47, -0.05), (48, 0.042), (49, -0.114)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95969135 <a title="364-lsi-1" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>Introduction: I attendedKDDthis year. The conference has always had a strong grounding in
what works based on theKDDcup, but it has developed a halo of workshops on
various subjects. It seems that KDD has become a place where the economy meets
machine learning in a stronger sense than many other conferences.There were
several papers that other people might like to take a look at.Yehuda
KorenCollaborative Filtering with Temporal Dynamics. This paper describes how
to incorporate temporal dynamics into a couple of collaborative filtering
approaches. This was also a best paper award.D. Sculley, Robert Malkin,Sugato
Basu,Roberto J. Bayardo,Predicting Bounce Rates in Sponsored Search
Advertisements. The basic claim of this paper is that the probability people
immediately leave ("bounce") after clicking on an advertisement is
predictable.Frank McSherryandIlya MironovDifferentially Private Recommender
Systems: Building Privacy into the Netflix Prize Contenders. The basic claim
here is that it is possible to</p><p>2 0.70658946 <a title="364-lsi-2" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: Thesecond Netflix prize is canceleddue toprivacy problems. I continue to
believe my original assessment of this paper, that the privacy break was
somewhat overstated. I still haven't seen any serious privacy failures on the
scale of theAOL search log release.I expect privacy concerns to continue to be
a big issue when dealing with data releases by companies or governments. The
theory of maintaining privacy while using data is improving, but it is not yet
in a state where the limits of what's possible are clear let alone how to
achieve these limits in a manner friendly to a prediction competition.</p><p>3 0.68159199 <a title="364-lsi-3" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>Introduction: There were several papers that seemed fairly interesting atKDD this year. The
ones that caught my attention are:Xin Jin, Mingyang Zhang,Nan Zhang, andGautam
Das,Versatile Publishing For Privacy Preservation. This paper provides a
conservative method for safely determining which data is publishable from any
complete source of information (for example, a hospital) such that it does not
violate privacy rules in a natural language. It is not differentially private,
so no external sources of join information can exist. However, it is a
mechanism forpublishingdata rather than (say) the output of a learning
algorithm.Arik FriedmanAssaf Schuster,Data Mining with Differential Privacy.
This paper shows how to create effective differentially private decision
trees. Progress in differentially private datamining is pretty impressive, as
it wasdefined in 2006.David Chan, Rong Ge, Ori Gershony,Tim Hesterberg,Diane
Lambert,Evaluating Online Ad Campaigns in a Pipeline: Causal Models At
ScaleThis paper</p><p>4 0.57753366 <a title="364-lsi-4" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>Introduction: A couple security researchersclaim to have cracked the netflix dataset. The
claims of success appear somewhat overstated to me, but the method of attack
is valid and could plausibly be substantially improved so as to reveal the
movie preferences of a small fraction of Netflix users.The basic idea is to
use a heuristic similarity function between ratings in a public database (from
IMDB) and an anonymized database (Netflix) to link ratings in the private
database to public identities (in IMDB). They claim to have linked two of a
few dozen IMDB users to anonymized netflix users.The claims seem a bit
inflated to me, because (a) knowing the IMDB identity isn't equivalent to
knowing the person and (b) the claims of statistical significance are with
respect to a model of the world they created (rather than one they
created).Overall, this is another example showing that completeprivacy is
hard. It may be worth remembering that there are some substantial benefits
from the Netflix challenge as w</p><p>5 0.52906036 <a title="364-lsi-5" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><p>6 0.46563482 <a title="364-lsi-6" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>7 0.45920005 <a title="364-lsi-7" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>8 0.44896623 <a title="364-lsi-8" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>9 0.42831799 <a title="364-lsi-9" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>10 0.42191455 <a title="364-lsi-10" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>11 0.41500083 <a title="364-lsi-11" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>12 0.41052863 <a title="364-lsi-12" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>13 0.40370569 <a title="364-lsi-13" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>14 0.39965338 <a title="364-lsi-14" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>15 0.39725921 <a title="364-lsi-15" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>16 0.39441523 <a title="364-lsi-16" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>17 0.38889477 <a title="364-lsi-17" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>18 0.38174194 <a title="364-lsi-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.3783884 <a title="364-lsi-19" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>20 0.37344864 <a title="364-lsi-20" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.045), (29, 0.012), (35, 0.066), (42, 0.163), (45, 0.096), (59, 0.338), (74, 0.102), (95, 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93882591 <a title="364-lda-1" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.
The aim of the competition is to facilitate direct comparisons between various
learning methods on important and realistic domains. This year's event will
feature well-known benchmark domains as well as more challenging problems of
real-world complexity, such as helicopter control and robot soccer
keepaway.The competition begins on November 1st, 2007 when training software
is released. Results must be submitted by July 1st, 2008. The competition will
culminate in an event at ICML-08 in Helsinki, Finland, at which the winners
will be announced.For more information, visitthe competition website.</p><p>same-blog 2 0.88146508 <a title="364-lda-2" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>Introduction: I attendedKDDthis year. The conference has always had a strong grounding in
what works based on theKDDcup, but it has developed a halo of workshops on
various subjects. It seems that KDD has become a place where the economy meets
machine learning in a stronger sense than many other conferences.There were
several papers that other people might like to take a look at.Yehuda
KorenCollaborative Filtering with Temporal Dynamics. This paper describes how
to incorporate temporal dynamics into a couple of collaborative filtering
approaches. This was also a best paper award.D. Sculley, Robert Malkin,Sugato
Basu,Roberto J. Bayardo,Predicting Bounce Rates in Sponsored Search
Advertisements. The basic claim of this paper is that the probability people
immediately leave ("bounce") after clicking on an advertisement is
predictable.Frank McSherryandIlya MironovDifferentially Private Recommender
Systems: Building Privacy into the Netflix Prize Contenders. The basic claim
here is that it is possible to</p><p>3 0.70309371 <a title="364-lda-3" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for
all sorts of tasks where it either wasn't previously collected, or for tasks
that did not previously exist. While this is great for Machine Learning, it
has a downside--the massive data collection which is so useful can also lead
to substantial privacy problems.It's important to understand that this is a
much harder problem than many people appreciate. TheAOLdatareleaseis a good
example. To those doing machine learning, the following strategies might be
obvious:Just delete any names or other obviously personally identifiable
information. The logic here seems to be "if I can't easily find the person
then no one can". That doesn't work as demonstrated by the people who were
found circumstantially from the AOL data.â€¦ then just hash all the search
terms! The logic here is "if I can't read it, then no one can". It's also
trivially broken by a dictionary attack--just hash all the strings that might
be in the data an</p><p>4 0.70012212 <a title="364-lda-4" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from theatomic learning workshopis that
gradient-based optimization is pervasive. For example, at least 7 (of 12)
speakers used the word 'gradient' in their talk and several others may be
approximating a gradient. The essential useful quality of a gradient is that
it decouples local updates from global optimization. Restated: Given a
gradient, we can determine how to change individual parameters of the system
so as to improve overall performance.It's easy to feel depressed about this
and think "nothing has happened", but that appears untrue. Many of the talks
were about clever techniques for computing gradients where your calculus
textbook breaks down.Sometimes there are clever approximations of the
gradient. (Simon Osindero)Sometimes we can compute constrained gradients via
iterated gradient/project steps. (Ben Taskar)Sometimes we can compute
gradients anyways over mildly nondifferentiable functions. (Drew Bagnell)Even
given a gradient, the choice of upda</p><p>5 0.53229582 <a title="364-lda-5" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>6 0.5310986 <a title="364-lda-6" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>7 0.52266401 <a title="364-lda-7" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>8 0.51950389 <a title="364-lda-8" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>9 0.51926142 <a title="364-lda-9" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>10 0.51907408 <a title="364-lda-10" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>11 0.51685846 <a title="364-lda-11" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>12 0.51078349 <a title="364-lda-12" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>13 0.51033884 <a title="364-lda-13" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>14 0.50810683 <a title="364-lda-14" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>15 0.50786358 <a title="364-lda-15" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>16 0.50774533 <a title="364-lda-16" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>17 0.50752848 <a title="364-lda-17" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>18 0.50702226 <a title="364-lda-18" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>19 0.50518358 <a title="364-lda-19" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>20 0.50449848 <a title="364-lda-20" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
