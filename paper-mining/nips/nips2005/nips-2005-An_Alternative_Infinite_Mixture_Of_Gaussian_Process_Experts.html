<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-21" href="#">nips2005-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</h1>
<br/><p>Source: <a title="nips-2005-21-pdf" href="http://papers.nips.cc/paper/2768-an-alternative-infinite-mixture-of-gaussian-process-experts.pdf">pdf</a></p><p>Author: Edward Meeds, Simon Osindero</p><p>Abstract: We present an inÔ¨Ånite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speciÔ¨Åcation of the effective ‚Äògating network‚Äô for the different experts. 1</p><p>Reference: <a title="nips-2005-21-reference" href="../nips2005_reference/nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present an inÔ¨Ånite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. [sent-3, score-0.486]
</p><p>2 Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals. [sent-4, score-0.36]
</p><p>3 The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. [sent-5, score-0.406]
</p><p>4 This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speciÔ¨Åcation of the effective ‚Äògating network‚Äô for the different experts. [sent-6, score-0.188]
</p><p>5 1  Introduction  Gaussian process (GP) models are powerful tools for regression, function approximation, and predictive density estimation. [sent-7, score-0.192]
</p><p>6 Another problem is that it can be difÔ¨Åcult to specify priors and perform learning in GP models if we require non-stationary covariance functions, multi-modal output, or discontinuities. [sent-10, score-0.125]
</p><p>7 In particular the InÔ¨Ånite Mixture of Gaussian Process Experts (IMoGPE) model proposed by Rasmussen and Ghahramani [1] neatly addresses the aforementioned key issues. [sent-12, score-0.103]
</p><p>8 Furthermore, by combining multiple stationary GP experts, we can easily accommodate non-stationary covariance and noise levels, as well as distinctly multi-modal outputs. [sent-15, score-0.214]
</p><p>9 Finally, by placing a Dirichlet process prior over the experts we can allow the data and our prior beliefs (which may be rather vague) to automatically determine the number of components to use. [sent-16, score-0.505]
</p><p>10 In this work we present an alternative inÔ¨Ånite model that is strongly inspired by the work in [1], but which uses a different formulation for the mixture of experts that is in the style presented in, for example [3, 4]. [sent-17, score-0.575]
</p><p>11 This alternative approach effectively uses posterior re-  PSfrag replacements  PSfrag replacements  xi  zi  yi  N  zi  xi  yi  N  Figure 1: Left: Graphical model for the standard MoE model [6]. [sent-18, score-0.387]
</p><p>12 The expert indicators {z(i) } are speciÔ¨Åed by a gating network applied to the inputs {x(i) }. [sent-19, score-0.628]
</p><p>13 Right: An alternative view of MoE model using a full generative model [4]. [sent-20, score-0.294]
</p><p>14 The distribution of input locations is now given by a mixture model, with components for each expert. [sent-21, score-0.39]
</p><p>15 Conditioned on the input locations, the posterior responsibilities for each mixture component behave like a gating network. [sent-22, score-0.547]
</p><p>16 sponsibilities from a mixture distribution as the gating network. [sent-23, score-0.34]
</p><p>17 Even if the task at hand is simply output density estimation or regression, we suggest a full generative model over inputs and outputs might be preferable to a purely conditional model. [sent-24, score-0.447]
</p><p>18 The generative approach retains all the strengths of [1] and also has a number of potential advantages, such as being able to deal with partially speciÔ¨Åed data (e. [sent-25, score-0.21]
</p><p>19 missing input co-ordinates) and being able to infer inverse functional mappings (i. [sent-27, score-0.348]
</p><p>20 The generative approach also affords us a richer and more consistent way of specifying our prior beliefs about how the covariance structure of the outputs might vary as we move within input space. [sent-30, score-0.455]
</p><p>21 An example of the type of generative model which we propose is shown in Ô¨Ågure 2. [sent-31, score-0.146]
</p><p>22 We use a Dirichlet process prior over a countably inÔ¨Ånite number of experts and each expert comprises two parts: a density over input space describing the distribution of input points associated with that expert, and a Gaussian Process model over the outputs associated with that expert. [sent-32, score-1.172]
</p><p>23 In this preliminary exposition, we restrict our attention to experts whose input space densities are given a single full covariance Gaussian. [sent-33, score-0.579]
</p><p>24 However, in a more elaborate setup the input density associated with each expert might itself be an inÔ¨Ånite mixture of simpler distributions (for instance, an inÔ¨Ånite mixture of Gaussians [5]) to allow for the most Ô¨Çexible partitioning of input space amongst the experts. [sent-35, score-0.979]
</p><p>25 Then, in section 3, we give the complete speciÔ¨Åcation and graphical depiction of our generative model, and in section 4 we outline the steps required to perform Monte Carlo inference and prediction. [sent-38, score-0.174]
</p><p>26 2  Mixtures of Experts  In the standard mixture of experts (MoE) model [6], a gating network probabilistically mixes regression components. [sent-40, score-0.742]
</p><p>27 One subtlety in using GP‚Äôs in a mixture of experts model is that IID assumptions on the data no longer hold and we must specify joint distributions for each possible assignment of experts to data. [sent-41, score-0.897]
</p><p>28 Let {x(i) } be the set of d-dimensional input vectors, {y(i) } be the set of scalar outputs, and {z(i) } be the set of expert indicators which assign data points to experts. [sent-42, score-0.513]
</p><p>29 GP The likelihood of the outputs, given the inputs, is speciÔ¨Åed in equation 1, where Œ∏ r represents the GP parameters of the rth expert, Œ∏ g represents the parameters of the gating network, and the summation is over all possible conÔ¨Ågurations of indicator variables. [sent-43, score-0.213]
</p><p>30 We have used xr to represent the (i) ith data point in the set of input data whose expert label is r, and Yr to represent the set of all output data whose expert label is r. [sent-45, score-0.99]
</p><p>31 In other words, input data are IID given their expert label, whereas the sets of output data are IID given their corresponding sets of input data. [sent-46, score-0.603]
</p><p>32 The DP concentration parameter Œ±0 , the expert indicators variables, {z(i) }, the gate hyperparameters, œÜx = {¬µ0 , Œ£0 , ŒΩc , S}, the gate component parameters, x GP œàr = {¬µr , Œ£r }, and the GP expert parameters, Œ∏r = {v0r , v1r , wjr }, are all updated for all r and j. [sent-48, score-0.966]
</p><p>33 P ({z(i) }|{x(i) }, Œ∏g )  P ({y(i) }|{x(i) }, Œ∏) =  GP P ({y(i) : z(i) = r}|{x(i) : z(i) = r}, Œ∏r )  r  Z  (1) There is an alternative view of the MoE model in which the experts also generate the inputs, rather than simply being conditioned on them [3, 4] (see Ô¨Ågure 1). [sent-49, score-0.456]
</p><p>34 This alternative view employs a joint mixture model over input and output space, even though the objective is still primarily that of estimating conditional densities i. [sent-50, score-0.521]
</p><p>35 The gating network effectively gets speciÔ¨Åed by the posterior responsibilities of each of the different components in the mixture. [sent-53, score-0.283]
</p><p>36 An advantage of this perspective is that it can easily accommodate partially observed inputs and it also allows ‚Äòreverse-conditioning‚Äô, should we wish to estimate where in input space a given output value is likely to have originated. [sent-54, score-0.291]
</p><p>37 For a mixture model using Gaussian Processes experts, the likelihood is given by P ({z(i) }|Œ∏g )√ó  P ({x(i) },{y(i) }|Œ∏) = Z  GP P ({y(i) : z(i) = r}|{x(i) : z(i) = r}, Œ∏r )P ({x(i) : z(i) = r}|Œ∏ g )  (2)  r  where the description of the density over input space is encapsulated in Œ∏ g . [sent-55, score-0.425]
</p><p>38 3  InÔ¨Ånite Mixture of Gaussian Processes: A Joint Generative Model  The graphical structure for our full generative model is shown in Ô¨Ågure 2. [sent-56, score-0.224]
</p><p>39 Our generative process does not produce IID data points and is therefore most simply formulated either as  a joint distribution over a dataset of a given size, or as a set of conditionals in which we incrementally add data points. [sent-57, score-0.226]
</p><p>40 To construct a complete set of N sample points from the prior (speciÔ¨Åed by top-level hyper-parameters ‚Ñ¶) we would perform the following operations: 1. [sent-58, score-0.083]
</p><p>41 Sample the gate hyperparameters œÜx given the top-level hyperparameters. [sent-64, score-0.122]
</p><p>42 For each grouping of indicators {z(i) : z(i) = r}, sample the input space paramx x eters œàr conditioned on œÜx . [sent-66, score-0.317]
</p><p>43 œàr deÔ¨Ånes the density in input space, in our case a full-covariance Gaussian. [sent-67, score-0.177]
</p><p>44 Given the parameters œàr for each group, sample the locations of the input points Xr ‚â° {x(i) : z(i) = r}. [sent-69, score-0.272]
</p><p>45 For each group, sample the hyper-parameters for the GP expert associated with GP that group, Œ∏r . [sent-71, score-0.354]
</p><p>46 Using the input locations Xr and hyper-parameters Œ∏r for the individual groups, formulate the GP output covariance matrix and sample the set of output values, Yr ‚â° {y(i) : z(i) = r} from this joint Gaussian distribution. [sent-73, score-0.562]
</p><p>47 We write the full joint distribution of our model as follows. [sent-74, score-0.13]
</p><p>48 The notation PU refers to the Polya urn distribution [8]. [sent-77, score-0.05]
</p><p>49 In an approach similar to Rasmussen [5], we use the input data mean ¬µx and covariance Œ£x to provide an automatic normalisation of our dataset. [sent-78, score-0.197]
</p><p>50 We also incorporate additional hyperparameters f0 and fS , which allow prior beliefs about the variation in location of ¬µr and size of Œ£r , relative to the data covariance. [sent-79, score-0.205]
</p><p>51 4  Monte Carlo Updates  Almost all the integrals and summations required for inference and learning operations within our model are analytically intractable, and therefore necessitate Monte Carlo approximations. [sent-80, score-0.095]
</p><p>52 Fortunately, all the necessary updates are relatively straightforward to carry out using a Markov Chain Monte Carlo (MCMC) scheme employing Gibbs sampling and Hybrid Monte Carlo. [sent-81, score-0.096]
</p><p>53 We also note that in our model the predictive density depends on the entire set of test locations (in input space). [sent-82, score-0.444]
</p><p>54 This transductive behaviour follows from the non-IID nature of the model and the inÔ¨Çuence that test locations have on the posterior distribution over mixture parameters. [sent-83, score-0.395]
</p><p>55 Consequently, the marginal predictive distribution at a given location can depend on the other locations for which we are making simultaneous predictions. [sent-84, score-0.253]
</p><p>56 In some situations the ability to incorporate the additional information about the input density at test time may be beneÔ¨Åcial. [sent-86, score-0.177]
</p><p>57 Given a set of test locations {x‚àó }, along with training data pairs {x(i) , y(i) } and top-level (t) hyper-parameters ‚Ñ¶, we iterate through the following conditional updates to produce our ‚àó predictive distribution for unknown outputs {y(t) }. [sent-88, score-0.336]
</p><p>58 The parameter updates are all conjugate with the prior distributions, except where noted: 1. [sent-89, score-0.093]
</p><p>59 Update indicators {z(i) } by cycling through the data and sampling one indicator variable at a time. [sent-90, score-0.229]
</p><p>60 Note that ŒΩc is updated using slice sampling [11]. [sent-98, score-0.097]
</p><p>61 Resample missing output values by cycling through the experts, and jointly sampling the missing outputs associated with that GP. [sent-102, score-0.321]
</p><p>62 We perform some preliminary runs to estimate the longest auto-covariance time, œÑ max for our posterior estimates, and then use a burn-in period that is about 10 times this timescale before taking samples every œÑmax iterations. [sent-103, score-0.154]
</p><p>63 2 For our simulations the auto-covariance time was typically 40 complete update cycles, so we use a burn-in period of 500 iterations and collect samples every 50. [sent-104, score-0.144]
</p><p>64 1  Samples From The Prior  In Ô¨Ågure 3 (A) we give an example of data drawn from our model which is multi-modal and non-stationary. [sent-106, score-0.053]
</p><p>65 We also use this artiÔ¨Åcial dataset to conÔ¨Årm that our MCMC algorithm performs well and is able recover sensible posterior distributions. [sent-107, score-0.165]
</p><p>66 It would also be valid to use all the samples after the burn-in period, and although they could not be considered independent, they could be used to obtain a more accurate estimator. [sent-110, score-0.058]
</p><p>67 5  1  100  ‚àí20  count  80  ‚àí30  60 40  ‚àí40 20  ‚àí50 0  ‚àí60 ‚àí8  ‚àí6  ‚àí4  ‚àí2  0  2  4  6  8  3  4  5  6  k  10  (A)  (B)  Figure 3: (A) A set of samples from our model prior. [sent-115, score-0.111]
</p><p>68 (B) The posterior distribution of log Œ± 0 with its true value indicated by the dashed line (top) and the distribution of occupied experts (bottom). [sent-117, score-0.364]
</p><p>69 We note that the posterior mass is located in the vicinity of the true values. [sent-118, score-0.057]
</p><p>70 2  Inference On Toy Data  To illustrate some of the features of our model we constructed a toy dataset consisting of 4 continuous functions, to which we added different levels of noise. [sent-120, score-0.188]
</p><p>71 100)  Noise SD: 4 (14) Noise SD: 2 (15)  The resulting data has non-stationary noise levels, non-stationary covariance, discontinuities and signiÔ¨Åcant multi-modality. [sent-138, score-0.078]
</p><p>72 Figure 4 shows our results on this dataset along with those from a single GP for comparison. [sent-139, score-0.063]
</p><p>73 We see that in order to account for the entire data set with a single GP, we are forced to infer an unnecessarily high level of noise in the function. [sent-140, score-0.101]
</p><p>74 In contrast, our model seems much more able to deal with these challenges. [sent-142, score-0.138]
</p><p>75 Since we have a full generative model over both input and output space, we are also able to use our model to infer likely input locations given a particular output value. [sent-143, score-0.837]
</p><p>76 There are a number of applications for which this might be relevant, for example if one wanted to sample candidate locations at which to evaluate a function we are trying to optimise. [sent-144, score-0.167]
</p><p>77 We choose three output levels and conditioned on the output having these values, we sample for the input location. [sent-146, score-0.379]
</p><p>78 The inference seems plausible and our model is able to suggest locations in input space for a maximal output value (+40) that was not seen in the training data. [sent-147, score-0.487]
</p><p>79 3  Regression on a simple ‚Äúreal-world‚Äù dataset  We also apply our model and algorithm to the motorcycle dataset of [13]. [sent-149, score-0.236]
</p><p>80 This is a commonly used dataset in the GP community and therefore serves as a useful basis for comparison. [sent-150, score-0.063]
</p><p>81 In particular, it also makes it easy to see how our model compares with standard GP‚Äôs and with the work of [1]. [sent-151, score-0.053]
</p><p>82 Figure 5 compares the performance of our model with that of a single GP. [sent-152, score-0.053]
</p><p>83 (A) The training data is shown along with the predictive mean of a stationary covariance GP and the median of the predictive distribution of our model. [sent-154, score-0.373]
</p><p>84 (B) The small dots are samples from the model (160 samples per location) evaluated at 80 equally spaced locations across the range (but plotted with a small amount of jitter to aid visualisation). [sent-155, score-0.439]
</p><p>85 The circular markers at ordinates of 40, 10 and ‚àí100 show samples from ‚Äòreverse-conditioning‚Äô where we sample likely abscissa locations given the test ordinate and the set of training data. [sent-158, score-0.225]
</p><p>86 For the remainder of the dataset, the noise level modeled by our model and a single GP are very similar, although our model is better able to capture the behaviour of the data at around 30 ms. [sent-160, score-0.194]
</p><p>87 It is difÔ¨Åcult to make an exact comparison to [1], however we can speculate that our model is more realistically modeling the noise at the beginning of the dataset by not inferring an overly ‚ÄúÔ¨Çat‚Äù GP expert at that location. [sent-161, score-0.472]
</p><p>88 We can also report that our expert adjacency matrix closely resembles that of [1]. [sent-162, score-0.355]
</p><p>89 6  Discussion  We have presented an alternative framework for an inÔ¨Ånite mixture of GP experts. [sent-163, score-0.215]
</p><p>90 We feel that our proposed model carries over the strengths of [1] and augments these with the several desirable additional features. [sent-164, score-0.085]
</p><p>91 Furthermore, in our most general framework we are more naturally able to specify priors over the partitioning of space between different expert components. [sent-166, score-0.427]
</p><p>92 Also, since we have a full joint model we can infer inverse functional mappings. [sent-167, score-0.254]
</p><p>93 There should be considerable gains to be made by allowing the input density models be more powerful. [sent-168, score-0.177]
</p><p>94 This would make it easier for arbitrary regions of space to share the same covariance structures; at present the areas ‚Äòcontrolled‚Äô by a particular expert tend to be local. [sent-169, score-0.441]
</p><p>95 Consequently, a potentially undesirable aspect of the current model is that strong clustering in input space can lead us to infer several expert components even if a single GP would do a good job of modelling the data. [sent-170, score-0.565]
</p><p>96 An elegant way of extending the model in this way might be to use a separate inÔ¨Ånite mixture distribution for the input density of each expert, perhaps incorporating a hierarchical DP prior across the inÔ¨Ånite set of experts to allow information to be shared. [sent-171, score-0.768]
</p><p>97 With regard to applications, it might be interesting to further explore our model‚Äôs capability to infer inverse functional mappings; perhaps this could be useful in an optimisation or active learning context. [sent-172, score-0.124]
</p><p>98 (B) The small dots are samples from our model (160 samples per location) evaluated at 80 equally spaced locations across the range (but plotted with a small amount of jitter to aid visualisation). [sent-174, score-0.439]
</p><p>99 Acknowledgments Thanks to Ben Marlin for sharing slice sampling code and to Carl Rasmussen for making minimize. [sent-177, score-0.097]
</p><p>100 Markov chain sampling methods for Dirichlet process mixture models. [sent-241, score-0.267]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gp', 0.532), ('expert', 0.313), ('experts', 0.307), ('gating', 0.181), ('xr', 0.179), ('yr', 0.171), ('mixture', 0.159), ('locations', 0.126), ('moe', 0.114), ('sd', 0.108), ('input', 0.105), ('indicators', 0.095), ('generative', 0.093), ('covariance', 0.092), ('rasmussen', 0.09), ('predictive', 0.088), ('aimogpe', 0.085), ('wjr', 0.085), ('output', 0.08), ('gate', 0.08), ('monte', 0.079), ('dirichlet', 0.078), ('hr', 0.076), ('density', 0.072), ('outputs', 0.071), ('carlo', 0.068), ('dataset', 0.063), ('samples', 0.058), ('infer', 0.058), ('nite', 0.058), ('iid', 0.057), ('cycling', 0.057), ('motorcycle', 0.057), ('posterior', 0.057), ('median', 0.057), ('alternative', 0.056), ('model', 0.053), ('beliefs', 0.052), ('slice', 0.052), ('updates', 0.051), ('bw', 0.05), ('neatly', 0.05), ('polya', 0.05), ('psfrag', 0.05), ('qr', 0.05), ('replacements', 0.05), ('urn', 0.05), ('visualisation', 0.05), ('dp', 0.049), ('stationary', 0.048), ('update', 0.047), ('sampling', 0.045), ('able', 0.045), ('escobar', 0.045), ('fs', 0.045), ('jitter', 0.045), ('pu', 0.045), ('responsibilities', 0.045), ('mixtures', 0.044), ('toronto', 0.043), ('noise', 0.043), ('inference', 0.042), ('prior', 0.042), ('resembles', 0.042), ('hyperparameters', 0.042), ('regression', 0.042), ('sample', 0.041), ('gaussian', 0.041), ('gure', 0.041), ('deal', 0.04), ('mappings', 0.04), ('conditioned', 0.04), ('acceleration', 0.04), ('inputs', 0.039), ('location', 0.039), ('full', 0.039), ('toy', 0.039), ('period', 0.039), ('graphical', 0.039), ('joint', 0.038), ('comprises', 0.036), ('space', 0.036), ('aid', 0.035), ('discontinuities', 0.035), ('zi', 0.034), ('missing', 0.034), ('levels', 0.033), ('specify', 0.033), ('functional', 0.033), ('inverse', 0.033), ('strengths', 0.032), ('dots', 0.032), ('spaced', 0.032), ('indicator', 0.032), ('process', 0.032), ('hybrid', 0.031), ('accommodate', 0.031), ('chain', 0.031), ('primarily', 0.03), ('allow', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="21-tfidf-1" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>Author: Edward Meeds, Simon Osindero</p><p>Abstract: We present an inÔ¨Ånite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speciÔ¨Åcation of the effective ‚Äògating network‚Äô for the different experts. 1</p><p>2 0.31106418 <a title="21-tfidf-2" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani</p><p>Abstract: We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N ) training cost and O(M 2 ) prediction cost per test case. We also Ô¨Ånd hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We Ô¨Ånally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M , i.e. very sparse solutions, and it signiÔ¨Åcantly outperforms other approaches in this regime. 1</p><p>3 0.15539004 <a title="21-tfidf-3" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>Author: Cristian Sminchisescu, Atul Kanujia, Zhiguo Li, Dimitris Metaxas</p><p>Abstract: We present a conditional temporal probabilistic framework for reconstructing 3D human motion in monocular video based on descriptors encoding image silhouette observations. For computational ef√Ñ?ƒπ≈° ciency we restrict visual inference to low-dimensional kernel induced non-linear state spaces. Our methodology (kBME) combines kernel PCA-based non-linear dimensionality reduction (kPCA) and Conditional Bayesian Mixture of Experts (BME) in order to learn complex multivalued predictors between observations and model hidden states. This is necessary for accurate, inverse, visual perception inferences, where several probable, distant 3D solutions exist due to noise or the uncertainty of monocular perspective projection. Low-dimensional models are appropriate because many visual processes exhibit strong non-linear correlations in both the image observations and the target, hidden state variables. The learned predictors are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty. We study several predictors and empirically show that the proposed algorithm positively compares with techniques based on regression, Kernel Dependency Estimation (KDE) or PCA alone, and gives results competitive to those of high-dimensional mixture predictors at a fraction of their computational cost. We show that the method successfully reconstructs the complex 3D motion of humans in real monocular video sequences. 1 Introduction and Related Work We consider the problem of inferring 3D articulated human motion from monocular video. This research topic has applications for scene understanding including human-computer interfaces, markerless human motion capture, entertainment and surveillance. A monocular approach is relevant because in real-world settings the human body parts are rarely completely observed even when using multiple cameras. This is due to occlusions form other people or objects in the scene. A robust system has to necessarily deal with incomplete, ambiguous and uncertain measurements. Methods for 3D human motion reconstruction can be classi√Ñ?ƒπ≈° ed as generative and discriminative. They both require a state representation, namely a 3D human model with kinematics (joint angles) or shape (surfaces or joint positions) and they both use a set of image features as observations for state inference. The computational goal in both cases is the conditional distribution for the model state given image observations. Generative model-based approaches [6, 16, 14, 13] have been demonstrated to √Ñ?ƒπ≈°&sbquo;exibly reconstruct complex unknown human motions and to naturally handle problem constraints. However it is dif√Ñ?ƒπ≈° cult to construct reliable observation likelihoods due to the complexity of modeling human appearance. This varies widely due to different clothing and deformation, body proportions or lighting conditions. Besides being somewhat indirect, the generative approach further imposes strict conditional independence assumptions on the temporal observations given the states in order to ensure computational tractability. Due to these factors inference is expensive and produces highly multimodal state distributions [6, 16, 13]. Generative inference algorithms require complex annealing schedules [6, 13] or systematic non-linear search for local optima [16] in order to ensure continuing tracking. These dif√Ñ?ƒπ≈° culties motivate the advent of a complementary class of discriminative algorithms [10, 12, 18, 2], that approximate the state conditional directly, in order to simplify inference. However, inverse, observation-to-state multivalued mappings are dif√Ñ?ƒπ≈° cult to learn (see e.g. √Ñ?ƒπ≈° g. 1a) and a probabilistic temporal setting is necessary. In an earlier paper [15] we introduced a probabilistic discriminative framework for human motion reconstruction. Because the method operates in the originally selected state and observation spaces that can be task generic, therefore redundant and often high-dimensional, inference is more expensive and can be less robust. To summarize, reconstructing 3D human motion in a Figure 1: (a, Left) Example of 180o ambiguity in predicting 3D human poses from silhouette image features (center). It is essential that multiple plausible solutions (e.g. F 1 and F2 ) are correctly represented and tracked over time. A single state predictor will either average the distant solutions or zig-zag between them, see also tables 1 and 2. (b, Right) A conditional chain model. The local distributions p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) or p(yt |zt ) are learned as in √Ñ?ƒπ≈° g. 2. For inference, the predicted local state conditional is recursively combined with the √Ñ?ƒπ≈° ltered prior c.f . (1). conditional temporal framework poses the following dif√Ñ?ƒπ≈° culties: (i) The mapping between temporal observations and states is multivalued (i.e. the local conditional distributions to be learned are multimodal), therefore it cannot be accurately represented using global function approximations. (ii) Human models have multivariate, high-dimensional continuous states of 50 or more human joint angles. The temporal state conditionals are multimodal which makes ef√Ñ?ƒπ≈° cient Kalman √Ñ?ƒπ≈° ltering algorithms inapplicable. General inference methods (particle √Ñ?ƒπ≈° lters, mixtures) have to be used instead, but these are expensive for high-dimensional models (e.g. when reconstructing the motion of several people that operate in a joint state space). (iii) The components of the human state and of the silhouette observation vector exhibit strong correlations, because many repetitive human activities like walking or running have low intrinsic dimensionality. It appears wasteful to work with high-dimensional states of 50+ joint angles. Even if the space were truly high-dimensional, predicting correlated state dimensions independently may still be suboptimal. In this paper we present a conditional temporal estimation algorithm that restricts visual inference to low-dimensional, kernel induced state spaces. To exploit correlations among observations and among state variables, we model the local, temporal conditional distributions using ideas from Kernel PCA [11, 19] and conditional mixture modeling [7, 5], here adapted to produce multiple probabilistic predictions. The corresponding predictor is referred to as a Conditional Bayesian Mixture of Low-dimensional Kernel-Induced Experts (kBME). By integrating it within a conditional graphical model framework (√Ñ?ƒπ≈° g. 1b), we can exploit temporal constraints probabilistically. We demonstrate that this methodology is effective for reconstructing the 3D motion of multiple people in monocular video. Our contribution w.r.t. [15] is a probabilistic conditional inference framework that operates over a non-linear, kernel-induced low-dimensional state spaces, and a set of experiments (on both real and arti√Ñ?ƒπ≈° cial image sequences) that show how the proposed framework positively compares with powerful predictors based on KDE, PCA, or with the high-dimensional models of [15] at a fraction of their cost. 2 Probabilistic Inference in a Kernel Induced State Space We work with conditional graphical models with a chain structure [9], as shown in √Ñ?ƒπ≈° g. 1b, These have continuous temporal states yt , t = 1 . . . T , observations zt . For compactness, we denote joint states Yt = (y1 , y2 , . . . , yt ) or joint observations Zt = (z1 , . . . , zt ). Learning and inference are based on local conditionals: p(yt |zt ) and p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), with yt and zt being low-dimensional, kernel induced representations of some initial model having state xt and observation rt . We obtain zt , yt from rt , xt using kernel PCA [11, 19]. Inference is performed in a low-dimensional, non-linear, kernel induced latent state space (see √Ñ?ƒπ≈° g. 1b and √Ñ?ƒπ≈° g. 2 and (1)). For display or error reporting, we compute the original conditional p(x|r), or a temporally √Ñ?ƒπ≈° ltered version p(xt |Rt ), Rt = (r1 , r2 , . . . , rt ), using a learned pre-image state map [3]. 2.1 Density Propagation for Continuous Conditional Chains For online √Ñ?ƒπ≈° ltering, we compute the optimal distribution p(yt |Zt ) for the state yt , conditioned by observations Zt up to time t. The √Ñ?ƒπ≈° ltered density can be recursively derived as: p(yt |Zt ) = p(yt |ytƒÇÀò&circ;&rsquo;1 , zt )p(ytƒÇÀò&circ;&rsquo;1 |ZtƒÇÀò&circ;&rsquo;1 ) (1) ytƒÇÀò&circ;&rsquo;1 We compute using a conditional mixture for p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) (a Bayesian mixture of experts c.f . ƒÇ&sbquo;√Ç¬ß2.2) and the prior p(ytƒÇÀò&circ;&rsquo;1 |ZtƒÇÀò&circ;&rsquo;1 ), each having, say M components. We integrate M 2 pairwise products of Gaussians analytically. The means of the expanded posterior are clustered and the centers are used to initialize a reduced M -component Kullback-Leibler approximation that is re√Ñ?ƒπ≈° ned using gradient descent [15]. The propagation rule (1) is similar to the one used for discrete state labels [9], but here we work with multivariate continuous state spaces and represent the local multimodal state conditionals using kBME (√Ñ?ƒπ≈° g. 2), and not log-linear models [9] (these would require intractable normalization). This complex continuous model rules out inference based on Kalman √Ñ?ƒπ≈° ltering or dynamic programming [9]. 2.2 Learning Bayesian Mixtures over Kernel Induced State Spaces (kBME) In order to model conditional mappings between low-dimensional non-linear spaces we rely on kernel dimensionality reduction and conditional mixture predictors. The authors of KDE [19] propose a powerful structured unimodal predictor. This works by decorrelating the output using kernel PCA and learning a ridge regressor between the input and each decorrelated output dimension. Our procedure is also based on kernel PCA but takes into account the structure of the studied visual problem where both inputs and outputs are likely to be low-dimensional and the mapping between them multivalued. The output variables xi are projected onto the column vectors of the principal space in order to obtain their principal coordinates y i . A z ƒÇÀò&circ;&circ; P(Fr ) O p(y|z) kP CA ƒÇ≈Ωƒπ&scaron;r (r) ƒÇÀò&Scaron;&sbquo; Fr O / y ƒÇÀò&circ;&circ; P(Fx ) O QQQ QQQ QQQ kP CA QQQ Q( ƒÇ≈Ωƒπ&scaron;x (x) ƒÇÀò&Scaron;&sbquo; Fx x ƒÇÀò&permil;&circ; PreImage(y) O ƒÇ≈Ωƒπ&scaron;r ƒÇ≈Ωƒπ&scaron;x r ƒÇÀò&circ;&circ; R ƒÇÀò&Scaron;&sbquo; Rr x ƒÇÀò&circ;&circ; X ƒÇÀò&Scaron;&sbquo; Rx  p(x|r) ƒÇÀò&permil;&circ; p(x|y) Figure 2: The learned low-dimensional predictor, kBME, for computing p(x|r) ƒÇÀò&permil;√Ñ&bdquo; p(xt |rt ), ƒÇÀò&circ;&euro;t. (We similarly learn p(xt |xtƒÇÀò&circ;&rsquo;1 , rt ), with input (x, r) instead of r ƒÇÀò&euro;&ldquo; here we illustrate only p(x|r) for clarity.) The input r and the output x are decorrelated using Kernel PCA to obtain z and y respectively. The kernels used for the input and output are ƒÇ≈Ωƒπ&scaron; r and ƒÇ≈Ωƒπ&scaron;x , with induced feature spaces Fr and Fx , respectively. Their principal subspaces obtained by kernel PCA are denoted by P(Fr ) and P(Fx ), respectively. A conditional Bayesian mixture of experts p(y|z) is learned using the low-dimensional representation (z, y). Using learned local conditionals of the form p(yt |zt ) or p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), temporal inference can be ef√Ñ?ƒπ≈° ciently performed in a low-dimensional kernel induced state space (see e.g. (1) and √Ñ?ƒπ≈° g. 1b). For visualization and error measurement, the √Ñ?ƒπ≈° ltered density, e.g. p(yt |Zt ), can be mapped back to p(xt |Rt ) using the pre-image c.f . (3). similar procedure is performed on the inputs ri to obtain zi . In order to relate the reduced feature spaces of z and y (P(Fr ) and P(Fx )), we estimate a probability distribution over mappings from training pairs (zi , yi ). We use a conditional Bayesian mixture of experts (BME) [7, 5] in order to account for ambiguity when mapping similar, possibly identical reduced feature inputs to very different feature outputs, as common in our problem (√Ñ?ƒπ≈° g. 1a). This gives a model that is a conditional mixture of low-dimensional kernel-induced experts (kBME): M g(z|ƒÇ≈Ω√Ç¬¥ j )N (y|Wj z, ƒÇ≈Ωƒπ j ) p(y|z) = (2) j=1 where g(z|ƒÇ≈Ω√Ç¬¥ j ) is a softmax function parameterized by ƒÇ≈Ω√Ç¬¥ j and (Wj , ƒÇ≈Ωƒπ j ) are the parameters and the output covariance of expert j, here a linear regressor. As in many Bayesian settings [17, 5], the weights of the experts and of the gates, Wj and ƒÇ≈Ω√Ç¬¥ j , are controlled by hierarchical priors, typically Gaussians with 0 mean, and having inverse variance hyperparameters controlled by a second level of Gamma distributions. We learn this model using a double-loop EM and employ ML-II type approximations [8, 17] with greedy (weight) subset selection [17, 15]. Finally, the kBME algorithm requires the computation of pre-images in order to recover the state distribution x from itƒÇÀò&euro;&trade;s image y ƒÇÀò&circ;&circ; P(Fx ). This is a closed form computation for polynomial kernels of odd degree. For more general kernels optimization or learning (regression based) methods are necessary [3]. Following [3, 19], we use a sparse Bayesian kernel regressor to learn the pre-image. This is based on training data (xi , yi ): p(x|y) = N (x|AƒÇ≈Ωƒπ&scaron;y (y), ƒÇÀò&bdquo;ƒπ&scaron;) (3) with parameters and covariances (A, ƒÇÀò&bdquo;ƒπ&scaron;). Since temporal inference is performed in the low-dimensional kernel induced state space, the pre-image function needs to be calculated only for visualizing results or for the purpose of error reporting. Propagating the result from the reduced feature space P(Fx ) to the output space X pro- duces a Gaussian mixture with M elements, having coef√Ñ?ƒπ≈° cients g(z|ƒÇ≈Ω√Ç¬¥ j ) and components N (x|AƒÇ≈Ωƒπ&scaron;y (Wj z), AJƒÇ≈Ωƒπ&scaron;y ƒÇ≈Ωƒπ j JƒÇ≈Ωƒπ&scaron;y A + ƒÇÀò&bdquo;ƒπ&scaron;), where JƒÇ≈Ωƒπ&scaron;y is the Jacobian of the mapping ƒÇ≈Ωƒπ&scaron;y . 3 Experiments We run experiments on both real image sequences (√Ñ?ƒπ≈° g. 5 and √Ñ?ƒπ≈° g. 6) and on sequences where silhouettes were arti√Ñ?ƒπ≈° cially rendered. The prediction error is reported in degrees (for mixture of experts, this is w.r.t. the most probable one, but see also √Ñ?ƒπ≈° g. 4a), and normalized per joint angle, per frame. The models are learned using standard cross-validation. Pre-images are learned using kernel regressors and have average error 1.7o . Training Set and Model State Representation: For training we gather pairs of 3D human poses together with their image projections, here silhouettes, using the graphics package Maya. We use realistically rendered computer graphics human surface models which we animate using human motion capture [1]. Our original human representation (x) is based on articulated skeletons with spherical joints and has 56 skeletal d.o.f. including global translation. The database consists of 8000 samples of human activities including walking, running, turns, jumps, gestures in conversations, quarreling and pantomime. Image Descriptors: We work with image silhouettes obtained using statistical background subtraction (with foreground and background models). Silhouettes are informative for pose estimation although prone to ambiguities (e.g. the left / right limb assignment in side views) or occasional lack of observability of some of the d.o.f. (e.g. 180o ambiguities in the global azimuthal orientation for frontal views, e.g. √Ñ?ƒπ≈° g. 1a). These are multiplied by intrinsic forward / backward monocular ambiguities [16]. As observations r, we use shape contexts extracted on the silhouette [4] (5 radial, 12 angular bins, size range 1/8 to 3 on log scale). The features are computed at different scales and sizes for points sampled on the silhouette. To work in a common coordinate system, we cluster all features in the training set into K = 50 clusters. To compute the representation of a new shape feature (a point on the silhouette), we ƒÇÀò&euro;&tilde;projectƒÇÀò&euro;&trade; onto the common basis by (inverse distance) weighted voting into the cluster centers. To obtain the representation (r) for a new silhouette we regularly sample 200 points on it and add all their feature vectors into a feature histogram. Because the representation uses overlapping features of the observation the elements of the descriptor are not independent. However, a conditional temporal framework (√Ñ?ƒπ≈° g. 1b) √Ñ?ƒπ≈°&sbquo;exibly accommodates this. For experiments, we use Gaussian kernels for the joint angle feature space and dot product kernels for the observation feature space. We learn state conditionals for p(yt |zt ) and p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) using 6 dimensions for the joint angle kernel induced state space and 25 dimensions for the observation induced feature space, respectively. In √Ñ?ƒπ≈° g. 3b) we show an evaluation of the ef√Ñ?ƒπ≈° cacy of our kBME predictor for different dimensions in the joint angle kernel induced state space (the observation feature space dimension is here 50). On the analyzed dancing sequence, that involves complex motions of the arms and the legs, the non-linear model signi√Ñ?ƒπ≈° cantly outperforms alternative PCA methods and gives good predictions for compact, low-dimensional models.1 In tables 1 and 2, as well as √Ñ?ƒπ≈° g. 4, we perform quantitative experiments on arti√Ñ?ƒπ≈° cially rendered silhouettes. 3D ground truth joint angles are available and this allows a more 1 Running times: On a Pentium 4 PC (3 GHz, 2 GB RAM), a full dimensional BME model with 5 experts takes 802s to train p(xt |xtƒÇÀò&circ;&rsquo;1 , rt ), whereas a kBME (including the pre-image) takes 95s to train p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ). The prediction time is 13.7s for BME and 8.7s (including the pre-image cost 1.04s) for kBME. The integration in (1) takes 2.67s for BME and 0.31s for kBME. The speed-up for kBME is signi√Ñ?ƒπ≈° cant and likely to increase with original models having higher dimensionality. Prediction Error Number of Clusters 100 1000 100 10 1 1 2 3 4 5 6 7 8 Degree of Multimodality kBME KDE_RVM PCA_BME PCA_RVM 10 1 0 20 40 Number of Dimensions 60 Figure 3: (a, Left) Analysis of ƒÇÀò&euro;&tilde;multimodalityƒÇÀò&euro;&trade; for a training set. The input zt dimension is 25, the output yt dimension is 6, both reduced using kPCA. We cluster independently in (ytƒÇÀò&circ;&rsquo;1 , zt ) and yt using many clusters (2100) to simulate small input perturbations and we histogram the yt clusters falling within each cluster in (ytƒÇÀò&circ;&rsquo;1 , zt ). This gives intuition on the degree of ambiguity in modeling p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), for small perturbations in the input. (b, Right) Evaluation of dimensionality reduction methods for an arti√Ñ?ƒπ≈° cial dancing sequence (models trained on 300 samples). The kBME is our model ƒÇ&sbquo;√Ç¬ß2.2, whereas the KDE-RVM is a KDE model learned with a Relevance Vector Machine (RVM) [17] feature space map. PCA-BME and PCA-RVM are models where the mappings between feature spaces (obtained using PCA) is learned using a BME and a RVM. The non-linearity is signi√Ñ?ƒπ≈° cant. Kernel-based methods outperform PCA and give low prediction error for 5-6d models. systematic evaluation. Notice that the kernelized low-dimensional models generally outperform the PCA ones. At the same time, they give results competitive to the ones of high-dimensional BME predictors, while being lower-dimensional and therefore signi√Ñ?ƒπ≈° cantly less expensive for inference, e.g. the integral in (1). In √Ñ?ƒπ≈° g. 5 and √Ñ?ƒπ≈° g. 6 we show human motion reconstruction results for two real image sequences. Fig. 5 shows the good quality reconstruction of a person performing an agile jump. (Given the missing observations in a side view, 3D inference for the occluded body parts would not be possible without using prior knowledge!) For this sequence we do inference using conditionals having 5 modes and reduced 6d states. We initialize tracking using p(yt |zt ), whereas for inference we use p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) within (1). In the second sequence in √Ñ?ƒπ≈° g. 6, we simultaneously reconstruct the motion of two people mimicking domestic activities, namely washing a window and picking an object. Here we do inference over a product, 12-dimensional state space consisting of the joint 6d state of each person. We obtain good 3D reconstruction results, using only 5 hypotheses. Notice however, that the results are not perfect, there are small errors in the elbow and the bending of the knee for the subject at the l.h.s., and in the different wrist orientations for the subject at the r.h.s. This re√Ñ?ƒπ≈°&sbquo;ects the bias of our training set. Walk and turn Conversation Run and turn left KDE-RR 10.46 7.95 5.22 RVM 4.95 4.96 5.02 KDE-RVM 7.57 6.31 6.25 BME 4.27 4.15 5.01 kBME 4.69 4.79 4.92 Table 1: Comparison of average joint angle prediction error for different models. All kPCA-based models use 6 output dimensions. Testing is done on 100 video frames for each sequence, the inputs are arti√Ñ?ƒπ≈° cially generated silhouettes, not in the training set. 3D joint angle ground truth is used for evaluation. KDE-RR is a KDE model with ridge regression (RR) for the feature space mapping, KDE-RVM uses an RVM. BME uses a Bayesian mixture of experts with no dimensionality reduction. kBME is our proposed model. kPCAbased methods use kernel regressors to compute pre-images. Expert Prediction Frequency ƒÇÀò&circ;&rsquo; Closest to Ground truth Frequency ƒÇÀò&circ;&rsquo; Close to ground truth 30 25 20 15 10 5 0 1 2 3 4 Expert Number 14 10 8 6 4 2 0 5 1st Probable Prev Output 2nd Probable Prev Output 3rd Probable Prev Output 4th Probable Prev Output 5th Probable Prev Output 12 1 2 3 4 Current Expert 5 Figure 4: (a, Left) Histogram showing the accuracy of various expert predictors: how many times the expert ranked as the k-th most probable by the model (horizontal axis) is closest to the ground truth. The model is consistent (the most probable expert indeed is the most accurate most frequently), but occasionally less probable experts are better. (b, Right) Histograms show the dynamics of p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), i.e. how the probability mass is redistributed among experts between two successive time steps, in a conversation sequence. Walk and turn back Run and turn KDE-RR 7.59 17.7 RVM 6.9 16.8 KDE-RVM 7.15 16.08 BME 3.6 8.2 kBME 3.72 8.01 Table 2: Joint angle prediction error computed for two complex sequences with walks, runs and turns, thus more ambiguity (100 frames). Models have 6 state dimensions. Unimodal predictors average competing solutions. kBME has signi√Ñ?ƒπ≈° cantly lower error. Figure 5: Reconstruction of a jump (selected frames). Top: original image sequence. Middle: extracted silhouettes. Bottom: 3D reconstruction seen from a synthetic viewpoint. 4 Conclusion We have presented a probabilistic framework for conditional inference in latent kernelinduced low-dimensional state spaces. Our approach has the following properties: (a) Figure 6: Reconstructing the activities of 2 people operating in an 12-d state space (each person has its own 6d state). Top: original image sequence. Bottom: 3D reconstruction seen from a synthetic viewpoint. Accounts for non-linear correlations among input or output variables, by using kernel nonlinear dimensionality reduction (kPCA); (b) Learns probability distributions over mappings between low-dimensional state spaces using conditional Bayesian mixture of experts, as required for accurate prediction. In the resulting low-dimensional kBME predictor ambiguities and multiple solutions common in visual, inverse perception problems are accurately represented. (c) Works in a continuous, conditional temporal probabilistic setting and offers a formal management of uncertainty. We show comparisons that demonstrate how the proposed approach outperforms regression, PCA or KDE alone for reconstructing the 3D human motion in monocular video. Future work we will investigate scaling aspects for large training sets and alternative structured prediction methods. References [1] CMU Human Motion DataBase. Online at http://mocap.cs.cmu.edu/search.html, 2003. [2] A. Agarwal and B. Triggs. 3d human pose from silhouettes by Relevance Vector Regression. In CVPR, 2004. [3] G. Bakir, J. Weston, and B. Scholkopf. Learning to √Ñ?ƒπ≈° nd pre-images. In NIPS, 2004. [4] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. PAMI, 24, 2002. [5] C. Bishop and M. Svensen. Bayesian mixtures of experts. In UAI, 2003. [6] J. Deutscher, A. Blake, and I. Reid. Articulated Body Motion Capture by Annealed Particle Filtering. In CVPR, 2000. [7] M. Jordan and R. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation, (6):181ƒÇÀò&euro;&ldquo;214, 1994. [8] D. Mackay. Bayesian interpolation. Neural Computation, 4(5):720ƒÇÀò&euro;&ldquo;736, 1992. [9] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction and segmentation. In ICML, 2000. [10] R. Rosales and S. Sclaroff. Learning Body Pose Via Specialized Maps. In NIPS, 2002. [11] B. SchƒÇ&sbquo;√Ç¬® lkopf, A. Smola, and K. MƒÇ&sbquo;√Ç¬® ller. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Computation, 10:1299ƒÇÀò&euro;&ldquo;1319, 1998. [12] G. Shakhnarovich, P. Viola, and T. Darrell. Fast Pose Estimation with Parameter Sensitive Hashing. In ICCV, 2003. [13] L. Sigal, S. Bhatia, S. Roth, M. Black, and M. Isard. Tracking Loose-limbed People. In CVPR, 2004. [14] C. Sminchisescu and A. Jepson. Generative Modeling for Continuous Non-Linearly Embedded Visual Inference. In ICML, pages 759ƒÇÀò&euro;&ldquo;766, Banff, 2004. [15] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Discriminative Density Propagation for 3D Human Motion Estimation. In CVPR, 2005. [16] C. Sminchisescu and B. Triggs. Kinematic Jump Processes for Monocular 3D Human Tracking. In CVPR, volume 1, pages 69ƒÇÀò&euro;&ldquo;76, Madison, 2003. [17] M. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. JMLR, 2001. [18] C. Tomasi, S. Petrov, and A. Sastry. 3d tracking = classi√Ñ?ƒπ≈° cation + interpolation. In ICCV, 2003. [19] J. Weston, O. Chapelle, A. Elisseeff, B. Scholkopf, and V. Vapnik. Kernel dependency estimation. In NIPS, 2002.</p><p>4 0.152514 <a title="21-tfidf-4" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>Author: Sathiya Keerthi, Wei Chu</p><p>Abstract: In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efÔ¨Åciency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outperforms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions. 1</p><p>5 0.13487993 <a title="21-tfidf-5" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: Gaussian processes are attractive models for probabilistic classiÔ¨Åcation but unfortunately exact inference is analytically intractable. We compare Laplace‚Äôs method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate. In recent years models based on Gaussian process (GP) priors have attracted much attention in the machine learning community. Whereas inference in the GP regression model with Gaussian noise can be done analytically, probabilistic classiÔ¨Åcation using GPs is analytically intractable. Several approaches to approximate Bayesian inference have been suggested, including Laplace‚Äôs approximation, Expectation Propagation (EP), variational approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in conjunction with generalisation bounds, online learning schemes and sparse approximations. Despite the abundance of recent work on probabilistic GP classiÔ¨Åers, most experimental studies provide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which algorithm should be preferred. Thus, from a practitioners point of view probabilistic GP classiÔ¨Åcation remains a jungle. In this paper, we set out to understand and compare two of the most wide-spread approximations: Laplace‚Äôs method and Expectation Propagation (EP). We also compare to a sophisticated, but computationally demanding MCMC scheme to examine how close the approximations are to ground truth. We examine two aspects of the approximation schemes: Firstly the accuracy of approximations to the marginal likelihood which is of central importance for model selection and model comparison. In any practical application of GPs in classiÔ¨Åcation (usually multiple) parameters of the covariance function (hyperparameters) have to be handled. Bayesian model selection provides a consistent framework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the marginal likelihood approximations as a function of the hyperparameters, in order to assess the practical usefulness of the approach Secondly, we need to assess the quality of the approximate probabilistic predictions. In the past, the probabilistic nature of the GP predictions have not received much attention, the focus being mostly on classiÔ¨Åcation error rates. This unfortunate state of affairs is caused primarily by typical benchmarking problems being considered outside of a realistic context. The ability of a classiÔ¨Åer to produce class probabilities or conÔ¨Ådences, have obvious relevance in most areas of application, eg. medical diagnosis. We evaluate the predictive distributions of the approximate methods, and compare to the MCMC gold standard. 1 The Gaussian Process Model for Binary ClassiÔ¨Åcation Let y ‚àà {‚àí1, 1} denote the class label of an input x. Gaussian process classiÔ¨Åcation (GPC) is discriminative in modelling p(y|x) for given x by a Bernoulli distribution. The probability of success p(y = 1|x) is related to an unconstrained latent function f (x) which is mapped to the unit interval by a sigmoid transformation, eg. the logit or the probit. For reasons of analytic convenience we exclusively use the probit model p(y = 1|x) = Œ¶(f (x)), where Œ¶ denotes the cumulative density function of the standard Normal distribution. In the GPC model Bayesian inference is performed about the latent function f in the light of observed data D = {(yi , xi )|i = 1, . . . , m}. Let fi = f (xi ) and f = [f1 , . . . , fm ] be shorthand for the values of the latent function and y = [y1 , . . . , ym ] and X = [x1 , . . . , xm ] collect the class labels and inputs respectively. Given the latent function the class labels are independent Bernoulli variables, so the joint likelihood factories: m m p(yi |fi ) = p(y|f ) = i=1 Œ¶(yi fi ), i=1 and depends on f only through its value at the observed inputs. We use a zero-mean Gaussian process prior over the latent function f with a covariance function k(x, x |Œ∏), which may depend on hyperparameters Œ∏ [1]. The functional form and parameters of the covariance function encodes assumptions about the latent function, and adaptation of these is part of the inference. The posterior distribution over latent function values f at the observed X for given hyperparameters Œ∏ becomes: m p(f |D, Œ∏) = N (f |0, K) Œ¶(yi fi ), p(D|Œ∏) i=1 where p(D|Œ∏) = p(y|f )p(f |X, Œ∏)df , denotes the marginal likelihood. Unfortunately neither the marginal likelihood, nor the posterior itself, or predictions can be computed analytically, so approximations are needed. 2 Approximate Bayesian Inference For the GPC model approximations are either based on a Gaussian approximation to the posterior p(f |D, Œ∏) ‚âà q(f |D, Œ∏) = N (f |m, A) or involve Markov chain Monte Carlo (MCMC) sampling [2]. We compare Laplace‚Äôs method and Expectation Propagation (EP) which are two alternative approaches to Ô¨Ånding parameters m and A of the Gaussian q(f |D, Œ∏). Both methods also allow approximate evaluation of the marginal likelihood, which is useful for ML-II hyperparameter optimisation. Laplace‚Äôs approximation (LA) is found by making a second order Taylor approximation of the (un-normalised) log posterior [3]. The mean m is placed at the mode (MAP) and the covariance A equals the negative inverse Hessian of the log posterior density at m. The EP approximation [4] also gives a Gaussian approximation to the posterior. The parameters m and A are found in an iterative scheme by matching the approximate marginal moments of p(fi |D, Œ∏) by the marginals of the approximation N (fi |mi , Aii ). Although we cannot prove the convergence of EP, we conjecture that it always converges for GPC with probit likelihood, and have never encountered an exception. A key insight is that a Gaussian approximation to the GPC posterior is equivalent to a GP approximation to the posterior distribution over latent functions. For a test input x‚àó the fi 1 0.16 0.14 0.8 0.6 0.1 fj p(y|f) p(f|y) 0.12 Likelihood p(y|f) Prior p(f) Posterior p(f|y) Laplace q(f|y) EP q(f|y) 0.08 0.4 0.06 0.04 0.2 0.02 0 ‚àí4 0 4 8 0 f . (a) (b) Figure 1: Panel (a) provides a one-dimensional illustration of the approximations. The prior N (f |0, 52 ) combined with the probit likelihood (y = 1) results in a skewed posterior. The likelihood uses the right axis, all other curves use the left axis. Laplace‚Äôs approximation peaks at the posterior mode, but places far too much mass over negative values of f and too little at large positive values. The EP approximation matches the Ô¨Årst two posterior moments, which results in a larger mean and a more accurate placement of probability mass compared to Laplace‚Äôs approximation. In Panel (b) we caricature a high dimensional zeromean Gaussian prior as an ellipse. The gray shadow indicates that for a high dimensional Gaussian most of the mass lies in a thin shell. For large latent signals (large entries in K), the likelihood essentially cuts off regions which are incompatible with the training labels (hatched area), leaving the upper right orthant as the posterior. The dot represents the mode of the posterior, which remains close to the origin. approximate predictive latent and class probabilities are: 2 q(f‚àó |D, Œ∏, x‚àó ) = N (¬µ‚àó , œÉ‚àó ), and 2 q(y‚àó = 1|D, x‚àó ) = Œ¶(¬µ‚àó / 1 + œÉ‚àó ), 2 where ¬µ‚àó = k‚àó K‚àí1 m and œÉ‚àó = k(x‚àó , x‚àó )‚àík‚àó (K‚àí1 ‚àí K‚àí1 AK‚àí1 )k‚àó , where the vector k‚àó = [k(x1 , x‚àó ), . . . , k(xm , x‚àó )] collects covariances between x‚àó and training inputs X. MCMC sampling has the advantage that it becomes exact in the limit of long runs and so provides a gold standard by which to measure the two analytic methods described above. Although MCMC methods can in principle be used to do inference over f and Œ∏ jointly [5], we compare to methods using ML-II optimisation over Œ∏, thus we use MCMC to integrate over f only. Good marginal likelihood estimates are notoriously difÔ¨Åcult to obtain; in our experiments we use Annealed Importance Sampling (AIS) [6], combining several Thermodynamic Integration runs into a single (unbiased) estimate of the marginal likelihood. Both analytic approximations have a computational complexity which is cubic O(m3 ) as common among non-sparse GP models due to inversions m √ó m matrices. In our implementations LA and EP need similar running times, on the order of a few minutes for several hundred data-points. Making AIS work efÔ¨Åciently requires some Ô¨Åne-tuning and a single estimate of p(D|Œ∏) can take several hours for data sets of a few hundred examples, but this could conceivably be improved upon. 3 Structural Properties of the Posterior and its Approximations Structural properties of the posterior can best be understood by examining its construction. The prior is a correlated m-dimensional Gaussian N (f |0, K) centred at the origin. Each likelihood term p(yi |fi ) softly truncates the half-space from the prior that is incompatible with the observed label, see Figure 1. The resulting posterior is unimodal and skewed, similar to a multivariate Gaussian truncated to the orthant containing y. The mode of the posterior remains close to the origin, while the mass is placed in accordance with the observed class labels. Additionally, high dimensional Gaussian distributions exhibit the property that most probability mass is contained in a thin ellipsoidal shell ‚Äì depending on the covariance structure ‚Äì away from the mean [7, ch. 29.2]. Intuitively this occurs since in high dimensions the volume grows extremely rapidly with the radius. As an effect the mode becomes less representative (typical) for the prior distribution as the dimension increases. For the GPC posterior this property persists: the mode of the posterior distribution stays relatively close to the origin, still being unrepresentative for the posterior distribution, while the mean moves to the mass of the posterior making mean and mode differ signiÔ¨Åcantly. We cannot generally assume the posterior to be close to Gaussian, as in the often studied limit of low-dimensional parametric models with large amounts of data. Therefore in GPC we must be aware of making a Gaussian approximation to a non-Gaussian posterior. From the properties of the posterior it can be expected that Laplace‚Äôs method places m in the right orthant but too close to the origin, such that the approximation will overlap with regions having practically zero posterior mass. As an effect the amplitude of the approximate latent posterior GP will be underestimated systematically, leading to overly cautious predictive distributions. The EP approximation does not rely on a local expansion, but assumes that the marginal distributions can be well approximated by Gaussians. This assumption will be examined empirically below. 4 Experiments In this section we compare and inspect approximations for GPC using various benchmark data sets. The primary focus is not to optimise the absolute performance of GPC models but to compare the relative accuracy of approximations and to validate the arguments given in the previous section. In all experiments we use a covariance function of the form: k(x, x |Œ∏) = œÉ 2 exp ‚àí 1 x ‚àí x 2 2 / 2 , (1) such that Œ∏ = [œÉ, ]. We refer to œÉ 2 as the signal variance and to as the characteristic length-scale. Note that for many classiÔ¨Åcation tasks it may be reasonable to use an individual length scale parameter for every input dimension (ARD) or a different kind of covariance function. Nevertheless, for the sake of presentability we use the above covariance function and we believe the conclusions about the accuracy of approximations to be independent of this choice, since it relies on arguments which are independent of the form of the covariance function. As measure of the accuracy of predictive probabilities we use the average information in bits of the predictions about the test targets in excess of that of random guessing. Let p‚àó = p(y‚àó = 1|D, Œ∏, x‚àó ) be the model‚Äôs prediction, then we average: I(p‚àó , yi ) = i yi +1 2 log2 (p‚àó ) + i 1‚àíyi 2 log2 (1 ‚àí p‚àó ) + H i (2) over all test cases, where H is the entropy of the training labels. The error rate E is equal to the percentage of erroneous class assignments if prediction is understood as a decision problem with symmetric costs. For the Ô¨Årst set of experiments presented here the well-known USPS digits and the Ionosphere data set were used. A binary sub-problem from the USPS digits is deÔ¨Åned by only considering 3‚Äôs vs. 5‚Äôs (which is probably the hardest of the binary sub-problems) and dividing the data into 767 cases for training and 773 for testing. The Ionosphere data is split into 200 training and 151 test cases. We do an exhaustive investigation on a Ô¨Åne regular grid of values for the log hyperparameters. For each Œ∏ on the grid we compute the approximated log marginal likelihood by LA, EP and AIS. Additionally we compute the respective predictive performance (2) on the test set. Results are shown in Figure 2. Log marginal likelihood ‚àí150 ‚àí130 ‚àí200 Log marginal likelihood 5 ‚àí115 ‚àí105 ‚àí95 4 ‚àí115 ‚àí105 3 ‚àí130 ‚àí100 ‚àí150 2 1 log magnitude, log(œÉf) log magnitude, log(œÉf) 4 Log marginal likelihood 5 ‚àí160 4 ‚àí100 3 ‚àí130 ‚àí92 ‚àí160 2 ‚àí105 ‚àí160 ‚àí105 ‚àí200 ‚àí115 1 log magnitude, log(œÉf) 5 ‚àí92 ‚àí95 3 ‚àí100 ‚àí105 2‚àí200 ‚àí115 ‚àí160 ‚àí130 ‚àí200 1 ‚àí200 0 0 0 ‚àí200 3 4 log lengthscale, log(l) 5 2 3 4 log lengthscale, log(l) (1a) 4 0.84 4 0.8 0.8 0.25 3 0.8 0.84 2 0.7 0.7 1 0.5 log magnitude, log(œÉf) 0.86 5 0.86 0.8 0.89 0.88 0.7 1 0.5 3 4 log lengthscale, log(l) 2 3 4 log lengthscale, log(l) (2a) Log marginal likelihood ‚àí90 ‚àí70 ‚àí100 ‚àí120 ‚àí120 0 ‚àí70 ‚àí75 ‚àí120 1 ‚àí100 1 2 3 log lengthscale, log(l) 4 0 ‚àí70 ‚àí90 ‚àí65 2 ‚àí100 ‚àí100 1 ‚àí120 ‚àí80 1 2 3 log lengthscale, log(l) 4 ‚àí1 ‚àí1 5 5 f 0.1 0.2 0.55 0 1 0.4 1 2 3 log lengthscale, log(l) 5 0.5 0.1 0 0.3 0.4 0.6 0.55 0.3 0.2 0.2 0.1 1 0 0.2 4 5 ‚àí1 ‚àí1 0.4 0.2 0.6 2 0.3 10 0 0.1 0.2 0.1 0 0 0.5 1 2 3 log lengthscale, log(l) 0.5 0.5 0.55 3 0 0.1 0 1 2 3 log lengthscale, log(l) 0.5 0.3 0.5 4 2 5 (3c) 0.5 3 4 Information about test targets in bits 4 log magnitude, log(œÉf) 4 2 0 (3b) Information about test targets in bits 0.3 log magnitude, log(œÉ ) ‚àí75 0 ‚àí1 ‚àí1 5 5 0 ‚àí120 3 ‚àí120 (3a) ‚àí1 ‚àí1 ‚àí90 ‚àí80 ‚àí65 ‚àí100 2 Information about test targets in bits 0 ‚àí75 4 0 3 5 Log marginal likelihood ‚àí90 3 ‚àí100 0 0.25 3 4 log lengthscale, log(l) 5 log magnitude, log(œÉf) log magnitude, log(œÉf) f log magnitude, log(œÉ ) ‚àí80 3 0.5 (2c) ‚àí75 ‚àí90 0.7 0.8 2 4 ‚àí75 ‚àí1 ‚àí1 0.86 0.84 Log marginal likelihood 4 1 0.7 1 5 5 ‚àí150 2 (2b) 5 2 0.88 3 0 5 0.84 0.89 0.25 0 0.7 0.25 0 0.86 4 0.84 3 2 5 Information about test targets in bits log magnitude, log(œÉf) log magnitude, log(œÉf) 5 ‚àí200 3 4 log lengthscale, log(l) (1c) Information about test targets in bits 5 2 2 (1b) Information about test targets in bits 0.5 5 log magnitude, log(œÉf) 2 4 5 ‚àí1 ‚àí1 0 1 2 3 log lengthscale, log(l) 4 5 (4a) (4b) (4c) Figure 2: Comparison of marginal likelihood approximations and predictive performances of different approximation techniques for USPS 3s vs. 5s (upper half) and the Ionosphere data (lower half). The columns correspond to LA (a), EP (b), and MCMC (c). The rows show estimates of the log marginal likelihood (rows 1 & 3) and the corresponding predictive performance (2) on the test set (rows 2 & 4) respectively. MCMC samples Laplace p(f|D) EP p(f|D) 0.2 0.15 0.45 0.1 0.4 0.05 0.3 ‚àí16 ‚àí14 ‚àí12 ‚àí10 ‚àí8 ‚àí6 f ‚àí4 ‚àí2 0 2 4 p(xi) 0 0.35 (a) 0.06 0.25 0.2 0.15 MCMC samples Laplace p(f|D) EP p(f|D) 0.1 0.05 0.04 0 0 2 0.02 xi 4 6 (c) 0 ‚àí40 ‚àí35 ‚àí30 ‚àí25 ‚àí20 ‚àí15 ‚àí10 ‚àí5 0 5 10 15 f (b) Figure 3: Panel (a) and (b) show two marginal distributions p(fi |D, Œ∏) from a GPC posterior and its approximations. The true posterior is approximated by a normalised histogram of 9000 samples of fi obtained by MCMC sampling. Panel (c) shows a histogram of samples of a marginal distribution of a truncated high-dimensional Gaussian. The line describes a Gaussian with mean and variance estimated from the samples. For all three approximation techniques we see an agreement between marginal likelihood estimates and test performance, which justiÔ¨Åes the use of ML-II parameter estimation. But the shape of the contours and the values differ between the methods. The contours for Laplace‚Äôs method appear to be slanted compared to EP. The marginal likelihood estimates of EP and AIS agree surprisingly well1 , given that the marginal likelihood comes as a 767 respectively 200 dimensional integral. The EP predictions contain as much information about the test cases as the MCMC predictions and signiÔ¨Åcantly more than for LA. Note that for small signal variances (roughly ln(œÉ 2 ) < 1) LA and EP give very similar results. A possible explanation is that for small signal variances the likelihood does not truncate the prior but only down-weights the tail that disagrees with the observation. As an effect the posterior will be less skewed and both approximations will lead to similar results. For the USPS 3‚Äôs vs. 5‚Äôs we now inspect the marginal distributions p(fi |D, Œ∏) of single latent function values under the posterior approximations for a given value of Œ∏. We have chosen the values ln(œÉ) = 3.35 and ln( ) = 2.85 which are between the ML-II estimates of EP and LA. Hybrid MCMC was used to generate 9000 samples from the posterior p(f |D, Œ∏). For LA and EP the approximate marginals are q(fi |D, Œ∏) = N (fi |mi , Aii ) where m and A are found by the respective approximation techniques. In general we observe that the marginal distributions of MCMC samples agree very well with the respective marginal distributions of the EP approximation. For Laplace‚Äôs approximation we Ô¨Ånd the mean to be underestimated and the marginal distributions to overlap with zero far more than the EP approximations. Figure (3a) displays the marginal distribution and its approximations for which the MCMC samples show maximal skewness. Figure (3b) shows a typical example where the EP approximation agrees very well with the MCMC samples. We show this particular example because under the EP approximation p(yi = 1|D, Œ∏) < 0.1% but LA gives a wrong p(yi = 1|D, Œ∏) ‚âà 18%. In the experiment we saw that the marginal distributions of the posterior often agree very 1 Note that the agreement between the two seems to be limited by the accuracy of the MCMC runs, as judged by the regularity of the contour lines; the tolerance is less than one unit on a (natural) log scale. well with a Gaussian approximation. This seems to contradict the description given in the previous section were we argued that the posterior is skewed by construction. In order to inspect the marginals of a truncated high-dimensional multivariate Gaussian distribution we made an additional synthetic experiment. We constructed a 767 dimensional Gaussian N (x|0, C) with a covariance matrix having one eigenvalue of 100 with eigenvector 1, and all other eigenvalues are 1. We then truncate this distribution such that all xi ‚â• 0. Note that the mode of the truncated Gaussian is still at zero, whereas the mean moves towards the remaining mass. Figure (3c) shows a normalised histogram of samples from a marginal distribution of one xi . The samples agree very well with a Gaussian approximation. In the previous section we described the somewhat surprising property, that for a truncated high-dimensional Gaussian, resembling the posterior, the mode (used by LA) may not be particularly representative of the distribution. Although the marginal is also truncated, it is still exceptionally well modelled by a Gaussian ‚Äì however, the Laplace approximation centred on the origin would be completely inappropriate. In a second set of experiments we compare the predictive performance of LA and EP for GPC on several well known benchmark problems. Each data set is randomly split into 10 folds of which one at a time is left out as a test set to measure the predictive performance of a model trained (or selected) on the remaining nine folds. All performance measures are averages over the 10 folds. For GPC we implement model selection by ML-II hyperparameter estimation, reporting results given the Œ∏ that maximised the respective approximate marginal likelihoods p(D|Œ∏). In order to get a better picture of the absolute performance we also compare to results obtained by C-SVM classiÔ¨Åcation. The kernel we used is equivalent to the covariance function (1) without the signal variance parameter. For each fold the parameters C and are found in an inner loop of 5-fold cross-validation, in which the parameter grids are reÔ¨Åned until the performance stabilises. Predictive probabilities for test cases are obtained by mapping the unthresholded output of the SVM to [0, 1] using a sigmoid function [8]. Results are summarised in Table 1. Comparing Laplace‚Äôs method to EP the latter shows to be more accurate both in terms of error rate and information. While the error rates are relatively similar the predictive distribution obtained by EP shows to be more informative about the test targets. Note that for GPC the error rate only depends of the sign of the mean ¬µ‚àó of the approximated posterior over latent functions and not the entire posterior predictive distribution. As to be expected, the length of the mean vector m shows much larger values for the EP approximations. Comparing EP and SVMs the results are mixed. For the Crabs data set all methods show the same error rate but the information content of the predictive distributions differs dramatically. For some test cases the SVM predicts the wrong class with large certainty. 5 Summary & Conclusions Our experiments reveal serious differences between Laplace‚Äôs method and EP when used in GPC models. From the structural properties of the posterior we described why LA systematically underestimates the mean m. The resulting posterior GP over latent functions will have too small amplitude, although the sign of the mean function will be mostly correct. As an effect LA gives over-conservative predictive probabilities, and diminished information about the test labels. This effect has been show empirically on several real world examples. Large resulting discrepancies in the actual posterior probabilities were found, even at the training locations, which renders the predictive class probabilities produced under this approximation grossly inaccurate. Note, the difference becomes less dramatic if we only consider the classiÔ¨Åcation error rates obtained by thresholding p‚àó at 1/2. For this particular task, we‚Äôve seen the the sign of the latent function tends to be correct (at least at the training locations). Laplace EP SVM Data Set m n E% I m E% I m E% I Ionosphere 351 34 8.84 0.591 49.96 7.99 0.661 124.94 5.69 0.681 Wisconsin 683 9 3.21 0.804 62.62 3.21 0.805 84.95 3.21 0.795 Pima Indians 768 8 22.77 0.252 29.05 22.63 0.253 47.49 23.01 0.232 Crabs 200 7 2.0 0.682 112.34 2.0 0.908 2552.97 2.0 0.047 Sonar 208 60 15.36 0.439 26.86 13.85 0.537 15678.55 11.14 0.567 USPS 3 vs 5 1540 256 2.27 0.849 163.05 2.21 0.902 22011.70 2.01 0.918 Table 1: Results for benchmark data sets. The Ô¨Årst three columns give the name of the data set, number of observations m and dimension of inputs n. For Laplace‚Äôs method and EP the table reports the average error rate E%, the average information I (2) and the average length m of the mean vector of the Gaussian approximation. For SVMs the error rate and the average information about the test targets are reported. Note that for the Crabs data set we use the sex (not the colour) of the crabs as class label. The EP approximation has shown to give results very close to MCMC both in terms of predictive distributions and marginal likelihood estimates. We have shown and explained why the marginal distributions of the posterior can be well approximated by Gaussians. Further, the marginal likelihood values obtained by LA and EP differ systematically which will lead to different results of ML-II hyperparameter estimation. The discrepancies are similar for different tasks. Using AIS we were able to show the accuracy of marginal likelihood estimates, which to the best of our knowledge has never been done before. In summary, we found that EP is the method of choice for approximate inference in binary GPC models, when the computational cost of MCMC is prohibitive. In contrast, the Laplace approximation is so inaccurate that we advise against its use, especially when predictive probabilities are to be taken seriously. Further experiments and a detailed description of the approximation schemes can be found in [2]. Acknowledgements Both authors acknowledge support by the German Research Foundation (DFG) through grant RA 1030/1. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST2002-506778. This publication only reÔ¨Çects the authors‚Äô views. References [1] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, NIPS 8, pages 514‚Äì520. MIT Press, 1996. [2] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiÔ¨Åcation. Journal of Machine Learning Research, 6:1679‚Äì1704, 2005. [3] C. K. I. Williams and D. Barber. Bayesian classiÔ¨Åcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342‚Äì1351, 1998. [4] T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Department of Electrical Engineering and Computer Science, MIT, 2001. [5] R. M. Neal. Regression and classiÔ¨Åcation using Gaussian process priors. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics 6, pages 475‚Äì501. Oxford University Press, 1998. [6] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125‚Äì139, 2001. [7] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. CUP, 2003. [8] J. C. Platt. Probabilities for SV machines. In Advances in Large Margin ClassiÔ¨Åers, pages 61‚Äì73. The MIT Press, 2000.</p><p>6 0.11963617 <a title="21-tfidf-6" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>7 0.094517969 <a title="21-tfidf-7" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>8 0.090659007 <a title="21-tfidf-8" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>9 0.089534268 <a title="21-tfidf-9" href="./nips-2005-Dual-Tree_Fast_Gauss_Transforms.html">59 nips-2005-Dual-Tree Fast Gauss Transforms</a></p>
<p>10 0.088996358 <a title="21-tfidf-10" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>11 0.087134719 <a title="21-tfidf-11" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>12 0.079900064 <a title="21-tfidf-12" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>13 0.075562991 <a title="21-tfidf-13" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>14 0.074483678 <a title="21-tfidf-14" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>15 0.071017735 <a title="21-tfidf-15" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>16 0.070173755 <a title="21-tfidf-16" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>17 0.06950549 <a title="21-tfidf-17" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>18 0.068976067 <a title="21-tfidf-18" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>19 0.068782821 <a title="21-tfidf-19" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>20 0.065036021 <a title="21-tfidf-20" href="./nips-2005-Learning_Shared_Latent_Structure_for_Image_Synthesis_and_Robotic_Imitation.html">115 nips-2005-Learning Shared Latent Structure for Image Synthesis and Robotic Imitation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.229), (1, 0.021), (2, -0.014), (3, 0.093), (4, 0.135), (5, -0.243), (6, 0.07), (7, -0.045), (8, 0.048), (9, 0.179), (10, 0.025), (11, 0.116), (12, 0.209), (13, 0.185), (14, -0.047), (15, 0.069), (16, 0.099), (17, -0.161), (18, -0.065), (19, -0.052), (20, -0.171), (21, 0.004), (22, -0.151), (23, 0.097), (24, -0.046), (25, -0.01), (26, 0.042), (27, 0.132), (28, -0.076), (29, 0.034), (30, -0.016), (31, -0.05), (32, -0.014), (33, -0.072), (34, 0.074), (35, 0.109), (36, 0.042), (37, 0.099), (38, -0.041), (39, 0.094), (40, 0.026), (41, -0.048), (42, -0.036), (43, 0.035), (44, 0.108), (45, -0.012), (46, 0.04), (47, 0.005), (48, -0.111), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95178276 <a title="21-lsi-1" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>Author: Edward Meeds, Simon Osindero</p><p>Abstract: We present an inÔ¨Ånite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speciÔ¨Åcation of the effective ‚Äògating network‚Äô for the different experts. 1</p><p>2 0.8675034 <a title="21-lsi-2" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani</p><p>Abstract: We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N ) training cost and O(M 2 ) prediction cost per test case. We also Ô¨Ånd hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We Ô¨Ånally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M , i.e. very sparse solutions, and it signiÔ¨Åcantly outperforms other approaches in this regime. 1</p><p>3 0.6817081 <a title="21-lsi-3" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>Author: Sathiya Keerthi, Wei Chu</p><p>Abstract: In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efÔ¨Åciency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outperforms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions. 1</p><p>4 0.55957007 <a title="21-lsi-4" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: Gaussian processes are attractive models for probabilistic classiÔ¨Åcation but unfortunately exact inference is analytically intractable. We compare Laplace‚Äôs method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate. In recent years models based on Gaussian process (GP) priors have attracted much attention in the machine learning community. Whereas inference in the GP regression model with Gaussian noise can be done analytically, probabilistic classiÔ¨Åcation using GPs is analytically intractable. Several approaches to approximate Bayesian inference have been suggested, including Laplace‚Äôs approximation, Expectation Propagation (EP), variational approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in conjunction with generalisation bounds, online learning schemes and sparse approximations. Despite the abundance of recent work on probabilistic GP classiÔ¨Åers, most experimental studies provide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which algorithm should be preferred. Thus, from a practitioners point of view probabilistic GP classiÔ¨Åcation remains a jungle. In this paper, we set out to understand and compare two of the most wide-spread approximations: Laplace‚Äôs method and Expectation Propagation (EP). We also compare to a sophisticated, but computationally demanding MCMC scheme to examine how close the approximations are to ground truth. We examine two aspects of the approximation schemes: Firstly the accuracy of approximations to the marginal likelihood which is of central importance for model selection and model comparison. In any practical application of GPs in classiÔ¨Åcation (usually multiple) parameters of the covariance function (hyperparameters) have to be handled. Bayesian model selection provides a consistent framework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the marginal likelihood approximations as a function of the hyperparameters, in order to assess the practical usefulness of the approach Secondly, we need to assess the quality of the approximate probabilistic predictions. In the past, the probabilistic nature of the GP predictions have not received much attention, the focus being mostly on classiÔ¨Åcation error rates. This unfortunate state of affairs is caused primarily by typical benchmarking problems being considered outside of a realistic context. The ability of a classiÔ¨Åer to produce class probabilities or conÔ¨Ådences, have obvious relevance in most areas of application, eg. medical diagnosis. We evaluate the predictive distributions of the approximate methods, and compare to the MCMC gold standard. 1 The Gaussian Process Model for Binary ClassiÔ¨Åcation Let y ‚àà {‚àí1, 1} denote the class label of an input x. Gaussian process classiÔ¨Åcation (GPC) is discriminative in modelling p(y|x) for given x by a Bernoulli distribution. The probability of success p(y = 1|x) is related to an unconstrained latent function f (x) which is mapped to the unit interval by a sigmoid transformation, eg. the logit or the probit. For reasons of analytic convenience we exclusively use the probit model p(y = 1|x) = Œ¶(f (x)), where Œ¶ denotes the cumulative density function of the standard Normal distribution. In the GPC model Bayesian inference is performed about the latent function f in the light of observed data D = {(yi , xi )|i = 1, . . . , m}. Let fi = f (xi ) and f = [f1 , . . . , fm ] be shorthand for the values of the latent function and y = [y1 , . . . , ym ] and X = [x1 , . . . , xm ] collect the class labels and inputs respectively. Given the latent function the class labels are independent Bernoulli variables, so the joint likelihood factories: m m p(yi |fi ) = p(y|f ) = i=1 Œ¶(yi fi ), i=1 and depends on f only through its value at the observed inputs. We use a zero-mean Gaussian process prior over the latent function f with a covariance function k(x, x |Œ∏), which may depend on hyperparameters Œ∏ [1]. The functional form and parameters of the covariance function encodes assumptions about the latent function, and adaptation of these is part of the inference. The posterior distribution over latent function values f at the observed X for given hyperparameters Œ∏ becomes: m p(f |D, Œ∏) = N (f |0, K) Œ¶(yi fi ), p(D|Œ∏) i=1 where p(D|Œ∏) = p(y|f )p(f |X, Œ∏)df , denotes the marginal likelihood. Unfortunately neither the marginal likelihood, nor the posterior itself, or predictions can be computed analytically, so approximations are needed. 2 Approximate Bayesian Inference For the GPC model approximations are either based on a Gaussian approximation to the posterior p(f |D, Œ∏) ‚âà q(f |D, Œ∏) = N (f |m, A) or involve Markov chain Monte Carlo (MCMC) sampling [2]. We compare Laplace‚Äôs method and Expectation Propagation (EP) which are two alternative approaches to Ô¨Ånding parameters m and A of the Gaussian q(f |D, Œ∏). Both methods also allow approximate evaluation of the marginal likelihood, which is useful for ML-II hyperparameter optimisation. Laplace‚Äôs approximation (LA) is found by making a second order Taylor approximation of the (un-normalised) log posterior [3]. The mean m is placed at the mode (MAP) and the covariance A equals the negative inverse Hessian of the log posterior density at m. The EP approximation [4] also gives a Gaussian approximation to the posterior. The parameters m and A are found in an iterative scheme by matching the approximate marginal moments of p(fi |D, Œ∏) by the marginals of the approximation N (fi |mi , Aii ). Although we cannot prove the convergence of EP, we conjecture that it always converges for GPC with probit likelihood, and have never encountered an exception. A key insight is that a Gaussian approximation to the GPC posterior is equivalent to a GP approximation to the posterior distribution over latent functions. For a test input x‚àó the fi 1 0.16 0.14 0.8 0.6 0.1 fj p(y|f) p(f|y) 0.12 Likelihood p(y|f) Prior p(f) Posterior p(f|y) Laplace q(f|y) EP q(f|y) 0.08 0.4 0.06 0.04 0.2 0.02 0 ‚àí4 0 4 8 0 f . (a) (b) Figure 1: Panel (a) provides a one-dimensional illustration of the approximations. The prior N (f |0, 52 ) combined with the probit likelihood (y = 1) results in a skewed posterior. The likelihood uses the right axis, all other curves use the left axis. Laplace‚Äôs approximation peaks at the posterior mode, but places far too much mass over negative values of f and too little at large positive values. The EP approximation matches the Ô¨Årst two posterior moments, which results in a larger mean and a more accurate placement of probability mass compared to Laplace‚Äôs approximation. In Panel (b) we caricature a high dimensional zeromean Gaussian prior as an ellipse. The gray shadow indicates that for a high dimensional Gaussian most of the mass lies in a thin shell. For large latent signals (large entries in K), the likelihood essentially cuts off regions which are incompatible with the training labels (hatched area), leaving the upper right orthant as the posterior. The dot represents the mode of the posterior, which remains close to the origin. approximate predictive latent and class probabilities are: 2 q(f‚àó |D, Œ∏, x‚àó ) = N (¬µ‚àó , œÉ‚àó ), and 2 q(y‚àó = 1|D, x‚àó ) = Œ¶(¬µ‚àó / 1 + œÉ‚àó ), 2 where ¬µ‚àó = k‚àó K‚àí1 m and œÉ‚àó = k(x‚àó , x‚àó )‚àík‚àó (K‚àí1 ‚àí K‚àí1 AK‚àí1 )k‚àó , where the vector k‚àó = [k(x1 , x‚àó ), . . . , k(xm , x‚àó )] collects covariances between x‚àó and training inputs X. MCMC sampling has the advantage that it becomes exact in the limit of long runs and so provides a gold standard by which to measure the two analytic methods described above. Although MCMC methods can in principle be used to do inference over f and Œ∏ jointly [5], we compare to methods using ML-II optimisation over Œ∏, thus we use MCMC to integrate over f only. Good marginal likelihood estimates are notoriously difÔ¨Åcult to obtain; in our experiments we use Annealed Importance Sampling (AIS) [6], combining several Thermodynamic Integration runs into a single (unbiased) estimate of the marginal likelihood. Both analytic approximations have a computational complexity which is cubic O(m3 ) as common among non-sparse GP models due to inversions m √ó m matrices. In our implementations LA and EP need similar running times, on the order of a few minutes for several hundred data-points. Making AIS work efÔ¨Åciently requires some Ô¨Åne-tuning and a single estimate of p(D|Œ∏) can take several hours for data sets of a few hundred examples, but this could conceivably be improved upon. 3 Structural Properties of the Posterior and its Approximations Structural properties of the posterior can best be understood by examining its construction. The prior is a correlated m-dimensional Gaussian N (f |0, K) centred at the origin. Each likelihood term p(yi |fi ) softly truncates the half-space from the prior that is incompatible with the observed label, see Figure 1. The resulting posterior is unimodal and skewed, similar to a multivariate Gaussian truncated to the orthant containing y. The mode of the posterior remains close to the origin, while the mass is placed in accordance with the observed class labels. Additionally, high dimensional Gaussian distributions exhibit the property that most probability mass is contained in a thin ellipsoidal shell ‚Äì depending on the covariance structure ‚Äì away from the mean [7, ch. 29.2]. Intuitively this occurs since in high dimensions the volume grows extremely rapidly with the radius. As an effect the mode becomes less representative (typical) for the prior distribution as the dimension increases. For the GPC posterior this property persists: the mode of the posterior distribution stays relatively close to the origin, still being unrepresentative for the posterior distribution, while the mean moves to the mass of the posterior making mean and mode differ signiÔ¨Åcantly. We cannot generally assume the posterior to be close to Gaussian, as in the often studied limit of low-dimensional parametric models with large amounts of data. Therefore in GPC we must be aware of making a Gaussian approximation to a non-Gaussian posterior. From the properties of the posterior it can be expected that Laplace‚Äôs method places m in the right orthant but too close to the origin, such that the approximation will overlap with regions having practically zero posterior mass. As an effect the amplitude of the approximate latent posterior GP will be underestimated systematically, leading to overly cautious predictive distributions. The EP approximation does not rely on a local expansion, but assumes that the marginal distributions can be well approximated by Gaussians. This assumption will be examined empirically below. 4 Experiments In this section we compare and inspect approximations for GPC using various benchmark data sets. The primary focus is not to optimise the absolute performance of GPC models but to compare the relative accuracy of approximations and to validate the arguments given in the previous section. In all experiments we use a covariance function of the form: k(x, x |Œ∏) = œÉ 2 exp ‚àí 1 x ‚àí x 2 2 / 2 , (1) such that Œ∏ = [œÉ, ]. We refer to œÉ 2 as the signal variance and to as the characteristic length-scale. Note that for many classiÔ¨Åcation tasks it may be reasonable to use an individual length scale parameter for every input dimension (ARD) or a different kind of covariance function. Nevertheless, for the sake of presentability we use the above covariance function and we believe the conclusions about the accuracy of approximations to be independent of this choice, since it relies on arguments which are independent of the form of the covariance function. As measure of the accuracy of predictive probabilities we use the average information in bits of the predictions about the test targets in excess of that of random guessing. Let p‚àó = p(y‚àó = 1|D, Œ∏, x‚àó ) be the model‚Äôs prediction, then we average: I(p‚àó , yi ) = i yi +1 2 log2 (p‚àó ) + i 1‚àíyi 2 log2 (1 ‚àí p‚àó ) + H i (2) over all test cases, where H is the entropy of the training labels. The error rate E is equal to the percentage of erroneous class assignments if prediction is understood as a decision problem with symmetric costs. For the Ô¨Årst set of experiments presented here the well-known USPS digits and the Ionosphere data set were used. A binary sub-problem from the USPS digits is deÔ¨Åned by only considering 3‚Äôs vs. 5‚Äôs (which is probably the hardest of the binary sub-problems) and dividing the data into 767 cases for training and 773 for testing. The Ionosphere data is split into 200 training and 151 test cases. We do an exhaustive investigation on a Ô¨Åne regular grid of values for the log hyperparameters. For each Œ∏ on the grid we compute the approximated log marginal likelihood by LA, EP and AIS. Additionally we compute the respective predictive performance (2) on the test set. Results are shown in Figure 2. Log marginal likelihood ‚àí150 ‚àí130 ‚àí200 Log marginal likelihood 5 ‚àí115 ‚àí105 ‚àí95 4 ‚àí115 ‚àí105 3 ‚àí130 ‚àí100 ‚àí150 2 1 log magnitude, log(œÉf) log magnitude, log(œÉf) 4 Log marginal likelihood 5 ‚àí160 4 ‚àí100 3 ‚àí130 ‚àí92 ‚àí160 2 ‚àí105 ‚àí160 ‚àí105 ‚àí200 ‚àí115 1 log magnitude, log(œÉf) 5 ‚àí92 ‚àí95 3 ‚àí100 ‚àí105 2‚àí200 ‚àí115 ‚àí160 ‚àí130 ‚àí200 1 ‚àí200 0 0 0 ‚àí200 3 4 log lengthscale, log(l) 5 2 3 4 log lengthscale, log(l) (1a) 4 0.84 4 0.8 0.8 0.25 3 0.8 0.84 2 0.7 0.7 1 0.5 log magnitude, log(œÉf) 0.86 5 0.86 0.8 0.89 0.88 0.7 1 0.5 3 4 log lengthscale, log(l) 2 3 4 log lengthscale, log(l) (2a) Log marginal likelihood ‚àí90 ‚àí70 ‚àí100 ‚àí120 ‚àí120 0 ‚àí70 ‚àí75 ‚àí120 1 ‚àí100 1 2 3 log lengthscale, log(l) 4 0 ‚àí70 ‚àí90 ‚àí65 2 ‚àí100 ‚àí100 1 ‚àí120 ‚àí80 1 2 3 log lengthscale, log(l) 4 ‚àí1 ‚àí1 5 5 f 0.1 0.2 0.55 0 1 0.4 1 2 3 log lengthscale, log(l) 5 0.5 0.1 0 0.3 0.4 0.6 0.55 0.3 0.2 0.2 0.1 1 0 0.2 4 5 ‚àí1 ‚àí1 0.4 0.2 0.6 2 0.3 10 0 0.1 0.2 0.1 0 0 0.5 1 2 3 log lengthscale, log(l) 0.5 0.5 0.55 3 0 0.1 0 1 2 3 log lengthscale, log(l) 0.5 0.3 0.5 4 2 5 (3c) 0.5 3 4 Information about test targets in bits 4 log magnitude, log(œÉf) 4 2 0 (3b) Information about test targets in bits 0.3 log magnitude, log(œÉ ) ‚àí75 0 ‚àí1 ‚àí1 5 5 0 ‚àí120 3 ‚àí120 (3a) ‚àí1 ‚àí1 ‚àí90 ‚àí80 ‚àí65 ‚àí100 2 Information about test targets in bits 0 ‚àí75 4 0 3 5 Log marginal likelihood ‚àí90 3 ‚àí100 0 0.25 3 4 log lengthscale, log(l) 5 log magnitude, log(œÉf) log magnitude, log(œÉf) f log magnitude, log(œÉ ) ‚àí80 3 0.5 (2c) ‚àí75 ‚àí90 0.7 0.8 2 4 ‚àí75 ‚àí1 ‚àí1 0.86 0.84 Log marginal likelihood 4 1 0.7 1 5 5 ‚àí150 2 (2b) 5 2 0.88 3 0 5 0.84 0.89 0.25 0 0.7 0.25 0 0.86 4 0.84 3 2 5 Information about test targets in bits log magnitude, log(œÉf) log magnitude, log(œÉf) 5 ‚àí200 3 4 log lengthscale, log(l) (1c) Information about test targets in bits 5 2 2 (1b) Information about test targets in bits 0.5 5 log magnitude, log(œÉf) 2 4 5 ‚àí1 ‚àí1 0 1 2 3 log lengthscale, log(l) 4 5 (4a) (4b) (4c) Figure 2: Comparison of marginal likelihood approximations and predictive performances of different approximation techniques for USPS 3s vs. 5s (upper half) and the Ionosphere data (lower half). The columns correspond to LA (a), EP (b), and MCMC (c). The rows show estimates of the log marginal likelihood (rows 1 & 3) and the corresponding predictive performance (2) on the test set (rows 2 & 4) respectively. MCMC samples Laplace p(f|D) EP p(f|D) 0.2 0.15 0.45 0.1 0.4 0.05 0.3 ‚àí16 ‚àí14 ‚àí12 ‚àí10 ‚àí8 ‚àí6 f ‚àí4 ‚àí2 0 2 4 p(xi) 0 0.35 (a) 0.06 0.25 0.2 0.15 MCMC samples Laplace p(f|D) EP p(f|D) 0.1 0.05 0.04 0 0 2 0.02 xi 4 6 (c) 0 ‚àí40 ‚àí35 ‚àí30 ‚àí25 ‚àí20 ‚àí15 ‚àí10 ‚àí5 0 5 10 15 f (b) Figure 3: Panel (a) and (b) show two marginal distributions p(fi |D, Œ∏) from a GPC posterior and its approximations. The true posterior is approximated by a normalised histogram of 9000 samples of fi obtained by MCMC sampling. Panel (c) shows a histogram of samples of a marginal distribution of a truncated high-dimensional Gaussian. The line describes a Gaussian with mean and variance estimated from the samples. For all three approximation techniques we see an agreement between marginal likelihood estimates and test performance, which justiÔ¨Åes the use of ML-II parameter estimation. But the shape of the contours and the values differ between the methods. The contours for Laplace‚Äôs method appear to be slanted compared to EP. The marginal likelihood estimates of EP and AIS agree surprisingly well1 , given that the marginal likelihood comes as a 767 respectively 200 dimensional integral. The EP predictions contain as much information about the test cases as the MCMC predictions and signiÔ¨Åcantly more than for LA. Note that for small signal variances (roughly ln(œÉ 2 ) < 1) LA and EP give very similar results. A possible explanation is that for small signal variances the likelihood does not truncate the prior but only down-weights the tail that disagrees with the observation. As an effect the posterior will be less skewed and both approximations will lead to similar results. For the USPS 3‚Äôs vs. 5‚Äôs we now inspect the marginal distributions p(fi |D, Œ∏) of single latent function values under the posterior approximations for a given value of Œ∏. We have chosen the values ln(œÉ) = 3.35 and ln( ) = 2.85 which are between the ML-II estimates of EP and LA. Hybrid MCMC was used to generate 9000 samples from the posterior p(f |D, Œ∏). For LA and EP the approximate marginals are q(fi |D, Œ∏) = N (fi |mi , Aii ) where m and A are found by the respective approximation techniques. In general we observe that the marginal distributions of MCMC samples agree very well with the respective marginal distributions of the EP approximation. For Laplace‚Äôs approximation we Ô¨Ånd the mean to be underestimated and the marginal distributions to overlap with zero far more than the EP approximations. Figure (3a) displays the marginal distribution and its approximations for which the MCMC samples show maximal skewness. Figure (3b) shows a typical example where the EP approximation agrees very well with the MCMC samples. We show this particular example because under the EP approximation p(yi = 1|D, Œ∏) < 0.1% but LA gives a wrong p(yi = 1|D, Œ∏) ‚âà 18%. In the experiment we saw that the marginal distributions of the posterior often agree very 1 Note that the agreement between the two seems to be limited by the accuracy of the MCMC runs, as judged by the regularity of the contour lines; the tolerance is less than one unit on a (natural) log scale. well with a Gaussian approximation. This seems to contradict the description given in the previous section were we argued that the posterior is skewed by construction. In order to inspect the marginals of a truncated high-dimensional multivariate Gaussian distribution we made an additional synthetic experiment. We constructed a 767 dimensional Gaussian N (x|0, C) with a covariance matrix having one eigenvalue of 100 with eigenvector 1, and all other eigenvalues are 1. We then truncate this distribution such that all xi ‚â• 0. Note that the mode of the truncated Gaussian is still at zero, whereas the mean moves towards the remaining mass. Figure (3c) shows a normalised histogram of samples from a marginal distribution of one xi . The samples agree very well with a Gaussian approximation. In the previous section we described the somewhat surprising property, that for a truncated high-dimensional Gaussian, resembling the posterior, the mode (used by LA) may not be particularly representative of the distribution. Although the marginal is also truncated, it is still exceptionally well modelled by a Gaussian ‚Äì however, the Laplace approximation centred on the origin would be completely inappropriate. In a second set of experiments we compare the predictive performance of LA and EP for GPC on several well known benchmark problems. Each data set is randomly split into 10 folds of which one at a time is left out as a test set to measure the predictive performance of a model trained (or selected) on the remaining nine folds. All performance measures are averages over the 10 folds. For GPC we implement model selection by ML-II hyperparameter estimation, reporting results given the Œ∏ that maximised the respective approximate marginal likelihoods p(D|Œ∏). In order to get a better picture of the absolute performance we also compare to results obtained by C-SVM classiÔ¨Åcation. The kernel we used is equivalent to the covariance function (1) without the signal variance parameter. For each fold the parameters C and are found in an inner loop of 5-fold cross-validation, in which the parameter grids are reÔ¨Åned until the performance stabilises. Predictive probabilities for test cases are obtained by mapping the unthresholded output of the SVM to [0, 1] using a sigmoid function [8]. Results are summarised in Table 1. Comparing Laplace‚Äôs method to EP the latter shows to be more accurate both in terms of error rate and information. While the error rates are relatively similar the predictive distribution obtained by EP shows to be more informative about the test targets. Note that for GPC the error rate only depends of the sign of the mean ¬µ‚àó of the approximated posterior over latent functions and not the entire posterior predictive distribution. As to be expected, the length of the mean vector m shows much larger values for the EP approximations. Comparing EP and SVMs the results are mixed. For the Crabs data set all methods show the same error rate but the information content of the predictive distributions differs dramatically. For some test cases the SVM predicts the wrong class with large certainty. 5 Summary & Conclusions Our experiments reveal serious differences between Laplace‚Äôs method and EP when used in GPC models. From the structural properties of the posterior we described why LA systematically underestimates the mean m. The resulting posterior GP over latent functions will have too small amplitude, although the sign of the mean function will be mostly correct. As an effect LA gives over-conservative predictive probabilities, and diminished information about the test labels. This effect has been show empirically on several real world examples. Large resulting discrepancies in the actual posterior probabilities were found, even at the training locations, which renders the predictive class probabilities produced under this approximation grossly inaccurate. Note, the difference becomes less dramatic if we only consider the classiÔ¨Åcation error rates obtained by thresholding p‚àó at 1/2. For this particular task, we‚Äôve seen the the sign of the latent function tends to be correct (at least at the training locations). Laplace EP SVM Data Set m n E% I m E% I m E% I Ionosphere 351 34 8.84 0.591 49.96 7.99 0.661 124.94 5.69 0.681 Wisconsin 683 9 3.21 0.804 62.62 3.21 0.805 84.95 3.21 0.795 Pima Indians 768 8 22.77 0.252 29.05 22.63 0.253 47.49 23.01 0.232 Crabs 200 7 2.0 0.682 112.34 2.0 0.908 2552.97 2.0 0.047 Sonar 208 60 15.36 0.439 26.86 13.85 0.537 15678.55 11.14 0.567 USPS 3 vs 5 1540 256 2.27 0.849 163.05 2.21 0.902 22011.70 2.01 0.918 Table 1: Results for benchmark data sets. The Ô¨Årst three columns give the name of the data set, number of observations m and dimension of inputs n. For Laplace‚Äôs method and EP the table reports the average error rate E%, the average information I (2) and the average length m of the mean vector of the Gaussian approximation. For SVMs the error rate and the average information about the test targets are reported. Note that for the Crabs data set we use the sex (not the colour) of the crabs as class label. The EP approximation has shown to give results very close to MCMC both in terms of predictive distributions and marginal likelihood estimates. We have shown and explained why the marginal distributions of the posterior can be well approximated by Gaussians. Further, the marginal likelihood values obtained by LA and EP differ systematically which will lead to different results of ML-II hyperparameter estimation. The discrepancies are similar for different tasks. Using AIS we were able to show the accuracy of marginal likelihood estimates, which to the best of our knowledge has never been done before. In summary, we found that EP is the method of choice for approximate inference in binary GPC models, when the computational cost of MCMC is prohibitive. In contrast, the Laplace approximation is so inaccurate that we advise against its use, especially when predictive probabilities are to be taken seriously. Further experiments and a detailed description of the approximation schemes can be found in [2]. Acknowledgements Both authors acknowledge support by the German Research Foundation (DFG) through grant RA 1030/1. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST2002-506778. This publication only reÔ¨Çects the authors‚Äô views. References [1] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, NIPS 8, pages 514‚Äì520. MIT Press, 1996. [2] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiÔ¨Åcation. Journal of Machine Learning Research, 6:1679‚Äì1704, 2005. [3] C. K. I. Williams and D. Barber. Bayesian classiÔ¨Åcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342‚Äì1351, 1998. [4] T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Department of Electrical Engineering and Computer Science, MIT, 2001. [5] R. M. Neal. Regression and classiÔ¨Åcation using Gaussian process priors. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics 6, pages 475‚Äì501. Oxford University Press, 1998. [6] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125‚Äì139, 2001. [7] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. CUP, 2003. [8] J. C. Platt. Probabilities for SV machines. In Advances in Large Margin ClassiÔ¨Åers, pages 61‚Äì73. The MIT Press, 2000.</p><p>5 0.46529692 <a title="21-lsi-5" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>Author: Frank Wood, Stefan Roth, Michael J. Black</p><p>Abstract: Probabilistic modeling of correlated neural population Ô¨Åring activity is central to understanding the neural code and building practical decoding algorithms. No parametric models currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data makes fully non-parametric methods impractical. To address these problems we propose an energy-based model in which the joint probability of neural activity is represented using learned functions of the 1D marginal histograms of the data. The parameters of the model are learned using contrastive divergence and an optimization procedure for Ô¨Ånding appropriate marginal directions. We evaluate the method using real data recorded from a population of motor cortical neurons. In particular, we model the joint probability of population spiking times and 2D hand position and show that the likelihood of test data under our model is signiÔ¨Åcantly higher than under other models. These results suggest that our model captures correlations in the Ô¨Åring activity. Our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity. 1</p><p>6 0.40336552 <a title="21-lsi-6" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>7 0.38673025 <a title="21-lsi-7" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>8 0.38361064 <a title="21-lsi-8" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>9 0.37945321 <a title="21-lsi-9" href="./nips-2005-Active_Learning_for_Misspecified_Models.html">19 nips-2005-Active Learning for Misspecified Models</a></p>
<p>10 0.36437199 <a title="21-lsi-10" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>11 0.35150102 <a title="21-lsi-11" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>12 0.33532193 <a title="21-lsi-12" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>13 0.325297 <a title="21-lsi-13" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>14 0.32410717 <a title="21-lsi-14" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>15 0.32298669 <a title="21-lsi-15" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>16 0.3216278 <a title="21-lsi-16" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>17 0.31403908 <a title="21-lsi-17" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>18 0.31385267 <a title="21-lsi-18" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>19 0.31277195 <a title="21-lsi-19" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>20 0.31276372 <a title="21-lsi-20" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.047), (10, 0.045), (11, 0.013), (18, 0.015), (23, 0.248), (27, 0.032), (31, 0.059), (34, 0.062), (41, 0.013), (50, 0.016), (55, 0.038), (69, 0.076), (73, 0.045), (88, 0.147), (91, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78620452 <a title="21-lda-1" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>Author: Edward Meeds, Simon Osindero</p><p>Abstract: We present an inÔ¨Ånite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speciÔ¨Åcation of the effective ‚Äògating network‚Äô for the different experts. 1</p><p>2 0.63088751 <a title="21-lda-2" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani</p><p>Abstract: We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N ) training cost and O(M 2 ) prediction cost per test case. We also Ô¨Ånd hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We Ô¨Ånally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M , i.e. very sparse solutions, and it signiÔ¨Åcantly outperforms other approaches in this regime. 1</p><p>3 0.61909461 <a title="21-lda-3" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>Author: Uri Maoz, Elon Portugaly, Tamar Flash, Yair Weiss</p><p>Abstract: The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system. 1</p><p>4 0.61844146 <a title="21-lda-4" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: Gaussian processes are attractive models for probabilistic classiÔ¨Åcation but unfortunately exact inference is analytically intractable. We compare Laplace‚Äôs method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate. In recent years models based on Gaussian process (GP) priors have attracted much attention in the machine learning community. Whereas inference in the GP regression model with Gaussian noise can be done analytically, probabilistic classiÔ¨Åcation using GPs is analytically intractable. Several approaches to approximate Bayesian inference have been suggested, including Laplace‚Äôs approximation, Expectation Propagation (EP), variational approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in conjunction with generalisation bounds, online learning schemes and sparse approximations. Despite the abundance of recent work on probabilistic GP classiÔ¨Åers, most experimental studies provide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which algorithm should be preferred. Thus, from a practitioners point of view probabilistic GP classiÔ¨Åcation remains a jungle. In this paper, we set out to understand and compare two of the most wide-spread approximations: Laplace‚Äôs method and Expectation Propagation (EP). We also compare to a sophisticated, but computationally demanding MCMC scheme to examine how close the approximations are to ground truth. We examine two aspects of the approximation schemes: Firstly the accuracy of approximations to the marginal likelihood which is of central importance for model selection and model comparison. In any practical application of GPs in classiÔ¨Åcation (usually multiple) parameters of the covariance function (hyperparameters) have to be handled. Bayesian model selection provides a consistent framework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the marginal likelihood approximations as a function of the hyperparameters, in order to assess the practical usefulness of the approach Secondly, we need to assess the quality of the approximate probabilistic predictions. In the past, the probabilistic nature of the GP predictions have not received much attention, the focus being mostly on classiÔ¨Åcation error rates. This unfortunate state of affairs is caused primarily by typical benchmarking problems being considered outside of a realistic context. The ability of a classiÔ¨Åer to produce class probabilities or conÔ¨Ådences, have obvious relevance in most areas of application, eg. medical diagnosis. We evaluate the predictive distributions of the approximate methods, and compare to the MCMC gold standard. 1 The Gaussian Process Model for Binary ClassiÔ¨Åcation Let y ‚àà {‚àí1, 1} denote the class label of an input x. Gaussian process classiÔ¨Åcation (GPC) is discriminative in modelling p(y|x) for given x by a Bernoulli distribution. The probability of success p(y = 1|x) is related to an unconstrained latent function f (x) which is mapped to the unit interval by a sigmoid transformation, eg. the logit or the probit. For reasons of analytic convenience we exclusively use the probit model p(y = 1|x) = Œ¶(f (x)), where Œ¶ denotes the cumulative density function of the standard Normal distribution. In the GPC model Bayesian inference is performed about the latent function f in the light of observed data D = {(yi , xi )|i = 1, . . . , m}. Let fi = f (xi ) and f = [f1 , . . . , fm ] be shorthand for the values of the latent function and y = [y1 , . . . , ym ] and X = [x1 , . . . , xm ] collect the class labels and inputs respectively. Given the latent function the class labels are independent Bernoulli variables, so the joint likelihood factories: m m p(yi |fi ) = p(y|f ) = i=1 Œ¶(yi fi ), i=1 and depends on f only through its value at the observed inputs. We use a zero-mean Gaussian process prior over the latent function f with a covariance function k(x, x |Œ∏), which may depend on hyperparameters Œ∏ [1]. The functional form and parameters of the covariance function encodes assumptions about the latent function, and adaptation of these is part of the inference. The posterior distribution over latent function values f at the observed X for given hyperparameters Œ∏ becomes: m p(f |D, Œ∏) = N (f |0, K) Œ¶(yi fi ), p(D|Œ∏) i=1 where p(D|Œ∏) = p(y|f )p(f |X, Œ∏)df , denotes the marginal likelihood. Unfortunately neither the marginal likelihood, nor the posterior itself, or predictions can be computed analytically, so approximations are needed. 2 Approximate Bayesian Inference For the GPC model approximations are either based on a Gaussian approximation to the posterior p(f |D, Œ∏) ‚âà q(f |D, Œ∏) = N (f |m, A) or involve Markov chain Monte Carlo (MCMC) sampling [2]. We compare Laplace‚Äôs method and Expectation Propagation (EP) which are two alternative approaches to Ô¨Ånding parameters m and A of the Gaussian q(f |D, Œ∏). Both methods also allow approximate evaluation of the marginal likelihood, which is useful for ML-II hyperparameter optimisation. Laplace‚Äôs approximation (LA) is found by making a second order Taylor approximation of the (un-normalised) log posterior [3]. The mean m is placed at the mode (MAP) and the covariance A equals the negative inverse Hessian of the log posterior density at m. The EP approximation [4] also gives a Gaussian approximation to the posterior. The parameters m and A are found in an iterative scheme by matching the approximate marginal moments of p(fi |D, Œ∏) by the marginals of the approximation N (fi |mi , Aii ). Although we cannot prove the convergence of EP, we conjecture that it always converges for GPC with probit likelihood, and have never encountered an exception. A key insight is that a Gaussian approximation to the GPC posterior is equivalent to a GP approximation to the posterior distribution over latent functions. For a test input x‚àó the fi 1 0.16 0.14 0.8 0.6 0.1 fj p(y|f) p(f|y) 0.12 Likelihood p(y|f) Prior p(f) Posterior p(f|y) Laplace q(f|y) EP q(f|y) 0.08 0.4 0.06 0.04 0.2 0.02 0 ‚àí4 0 4 8 0 f . (a) (b) Figure 1: Panel (a) provides a one-dimensional illustration of the approximations. The prior N (f |0, 52 ) combined with the probit likelihood (y = 1) results in a skewed posterior. The likelihood uses the right axis, all other curves use the left axis. Laplace‚Äôs approximation peaks at the posterior mode, but places far too much mass over negative values of f and too little at large positive values. The EP approximation matches the Ô¨Årst two posterior moments, which results in a larger mean and a more accurate placement of probability mass compared to Laplace‚Äôs approximation. In Panel (b) we caricature a high dimensional zeromean Gaussian prior as an ellipse. The gray shadow indicates that for a high dimensional Gaussian most of the mass lies in a thin shell. For large latent signals (large entries in K), the likelihood essentially cuts off regions which are incompatible with the training labels (hatched area), leaving the upper right orthant as the posterior. The dot represents the mode of the posterior, which remains close to the origin. approximate predictive latent and class probabilities are: 2 q(f‚àó |D, Œ∏, x‚àó ) = N (¬µ‚àó , œÉ‚àó ), and 2 q(y‚àó = 1|D, x‚àó ) = Œ¶(¬µ‚àó / 1 + œÉ‚àó ), 2 where ¬µ‚àó = k‚àó K‚àí1 m and œÉ‚àó = k(x‚àó , x‚àó )‚àík‚àó (K‚àí1 ‚àí K‚àí1 AK‚àí1 )k‚àó , where the vector k‚àó = [k(x1 , x‚àó ), . . . , k(xm , x‚àó )] collects covariances between x‚àó and training inputs X. MCMC sampling has the advantage that it becomes exact in the limit of long runs and so provides a gold standard by which to measure the two analytic methods described above. Although MCMC methods can in principle be used to do inference over f and Œ∏ jointly [5], we compare to methods using ML-II optimisation over Œ∏, thus we use MCMC to integrate over f only. Good marginal likelihood estimates are notoriously difÔ¨Åcult to obtain; in our experiments we use Annealed Importance Sampling (AIS) [6], combining several Thermodynamic Integration runs into a single (unbiased) estimate of the marginal likelihood. Both analytic approximations have a computational complexity which is cubic O(m3 ) as common among non-sparse GP models due to inversions m √ó m matrices. In our implementations LA and EP need similar running times, on the order of a few minutes for several hundred data-points. Making AIS work efÔ¨Åciently requires some Ô¨Åne-tuning and a single estimate of p(D|Œ∏) can take several hours for data sets of a few hundred examples, but this could conceivably be improved upon. 3 Structural Properties of the Posterior and its Approximations Structural properties of the posterior can best be understood by examining its construction. The prior is a correlated m-dimensional Gaussian N (f |0, K) centred at the origin. Each likelihood term p(yi |fi ) softly truncates the half-space from the prior that is incompatible with the observed label, see Figure 1. The resulting posterior is unimodal and skewed, similar to a multivariate Gaussian truncated to the orthant containing y. The mode of the posterior remains close to the origin, while the mass is placed in accordance with the observed class labels. Additionally, high dimensional Gaussian distributions exhibit the property that most probability mass is contained in a thin ellipsoidal shell ‚Äì depending on the covariance structure ‚Äì away from the mean [7, ch. 29.2]. Intuitively this occurs since in high dimensions the volume grows extremely rapidly with the radius. As an effect the mode becomes less representative (typical) for the prior distribution as the dimension increases. For the GPC posterior this property persists: the mode of the posterior distribution stays relatively close to the origin, still being unrepresentative for the posterior distribution, while the mean moves to the mass of the posterior making mean and mode differ signiÔ¨Åcantly. We cannot generally assume the posterior to be close to Gaussian, as in the often studied limit of low-dimensional parametric models with large amounts of data. Therefore in GPC we must be aware of making a Gaussian approximation to a non-Gaussian posterior. From the properties of the posterior it can be expected that Laplace‚Äôs method places m in the right orthant but too close to the origin, such that the approximation will overlap with regions having practically zero posterior mass. As an effect the amplitude of the approximate latent posterior GP will be underestimated systematically, leading to overly cautious predictive distributions. The EP approximation does not rely on a local expansion, but assumes that the marginal distributions can be well approximated by Gaussians. This assumption will be examined empirically below. 4 Experiments In this section we compare and inspect approximations for GPC using various benchmark data sets. The primary focus is not to optimise the absolute performance of GPC models but to compare the relative accuracy of approximations and to validate the arguments given in the previous section. In all experiments we use a covariance function of the form: k(x, x |Œ∏) = œÉ 2 exp ‚àí 1 x ‚àí x 2 2 / 2 , (1) such that Œ∏ = [œÉ, ]. We refer to œÉ 2 as the signal variance and to as the characteristic length-scale. Note that for many classiÔ¨Åcation tasks it may be reasonable to use an individual length scale parameter for every input dimension (ARD) or a different kind of covariance function. Nevertheless, for the sake of presentability we use the above covariance function and we believe the conclusions about the accuracy of approximations to be independent of this choice, since it relies on arguments which are independent of the form of the covariance function. As measure of the accuracy of predictive probabilities we use the average information in bits of the predictions about the test targets in excess of that of random guessing. Let p‚àó = p(y‚àó = 1|D, Œ∏, x‚àó ) be the model‚Äôs prediction, then we average: I(p‚àó , yi ) = i yi +1 2 log2 (p‚àó ) + i 1‚àíyi 2 log2 (1 ‚àí p‚àó ) + H i (2) over all test cases, where H is the entropy of the training labels. The error rate E is equal to the percentage of erroneous class assignments if prediction is understood as a decision problem with symmetric costs. For the Ô¨Årst set of experiments presented here the well-known USPS digits and the Ionosphere data set were used. A binary sub-problem from the USPS digits is deÔ¨Åned by only considering 3‚Äôs vs. 5‚Äôs (which is probably the hardest of the binary sub-problems) and dividing the data into 767 cases for training and 773 for testing. The Ionosphere data is split into 200 training and 151 test cases. We do an exhaustive investigation on a Ô¨Åne regular grid of values for the log hyperparameters. For each Œ∏ on the grid we compute the approximated log marginal likelihood by LA, EP and AIS. Additionally we compute the respective predictive performance (2) on the test set. Results are shown in Figure 2. Log marginal likelihood ‚àí150 ‚àí130 ‚àí200 Log marginal likelihood 5 ‚àí115 ‚àí105 ‚àí95 4 ‚àí115 ‚àí105 3 ‚àí130 ‚àí100 ‚àí150 2 1 log magnitude, log(œÉf) log magnitude, log(œÉf) 4 Log marginal likelihood 5 ‚àí160 4 ‚àí100 3 ‚àí130 ‚àí92 ‚àí160 2 ‚àí105 ‚àí160 ‚àí105 ‚àí200 ‚àí115 1 log magnitude, log(œÉf) 5 ‚àí92 ‚àí95 3 ‚àí100 ‚àí105 2‚àí200 ‚àí115 ‚àí160 ‚àí130 ‚àí200 1 ‚àí200 0 0 0 ‚àí200 3 4 log lengthscale, log(l) 5 2 3 4 log lengthscale, log(l) (1a) 4 0.84 4 0.8 0.8 0.25 3 0.8 0.84 2 0.7 0.7 1 0.5 log magnitude, log(œÉf) 0.86 5 0.86 0.8 0.89 0.88 0.7 1 0.5 3 4 log lengthscale, log(l) 2 3 4 log lengthscale, log(l) (2a) Log marginal likelihood ‚àí90 ‚àí70 ‚àí100 ‚àí120 ‚àí120 0 ‚àí70 ‚àí75 ‚àí120 1 ‚àí100 1 2 3 log lengthscale, log(l) 4 0 ‚àí70 ‚àí90 ‚àí65 2 ‚àí100 ‚àí100 1 ‚àí120 ‚àí80 1 2 3 log lengthscale, log(l) 4 ‚àí1 ‚àí1 5 5 f 0.1 0.2 0.55 0 1 0.4 1 2 3 log lengthscale, log(l) 5 0.5 0.1 0 0.3 0.4 0.6 0.55 0.3 0.2 0.2 0.1 1 0 0.2 4 5 ‚àí1 ‚àí1 0.4 0.2 0.6 2 0.3 10 0 0.1 0.2 0.1 0 0 0.5 1 2 3 log lengthscale, log(l) 0.5 0.5 0.55 3 0 0.1 0 1 2 3 log lengthscale, log(l) 0.5 0.3 0.5 4 2 5 (3c) 0.5 3 4 Information about test targets in bits 4 log magnitude, log(œÉf) 4 2 0 (3b) Information about test targets in bits 0.3 log magnitude, log(œÉ ) ‚àí75 0 ‚àí1 ‚àí1 5 5 0 ‚àí120 3 ‚àí120 (3a) ‚àí1 ‚àí1 ‚àí90 ‚àí80 ‚àí65 ‚àí100 2 Information about test targets in bits 0 ‚àí75 4 0 3 5 Log marginal likelihood ‚àí90 3 ‚àí100 0 0.25 3 4 log lengthscale, log(l) 5 log magnitude, log(œÉf) log magnitude, log(œÉf) f log magnitude, log(œÉ ) ‚àí80 3 0.5 (2c) ‚àí75 ‚àí90 0.7 0.8 2 4 ‚àí75 ‚àí1 ‚àí1 0.86 0.84 Log marginal likelihood 4 1 0.7 1 5 5 ‚àí150 2 (2b) 5 2 0.88 3 0 5 0.84 0.89 0.25 0 0.7 0.25 0 0.86 4 0.84 3 2 5 Information about test targets in bits log magnitude, log(œÉf) log magnitude, log(œÉf) 5 ‚àí200 3 4 log lengthscale, log(l) (1c) Information about test targets in bits 5 2 2 (1b) Information about test targets in bits 0.5 5 log magnitude, log(œÉf) 2 4 5 ‚àí1 ‚àí1 0 1 2 3 log lengthscale, log(l) 4 5 (4a) (4b) (4c) Figure 2: Comparison of marginal likelihood approximations and predictive performances of different approximation techniques for USPS 3s vs. 5s (upper half) and the Ionosphere data (lower half). The columns correspond to LA (a), EP (b), and MCMC (c). The rows show estimates of the log marginal likelihood (rows 1 & 3) and the corresponding predictive performance (2) on the test set (rows 2 & 4) respectively. MCMC samples Laplace p(f|D) EP p(f|D) 0.2 0.15 0.45 0.1 0.4 0.05 0.3 ‚àí16 ‚àí14 ‚àí12 ‚àí10 ‚àí8 ‚àí6 f ‚àí4 ‚àí2 0 2 4 p(xi) 0 0.35 (a) 0.06 0.25 0.2 0.15 MCMC samples Laplace p(f|D) EP p(f|D) 0.1 0.05 0.04 0 0 2 0.02 xi 4 6 (c) 0 ‚àí40 ‚àí35 ‚àí30 ‚àí25 ‚àí20 ‚àí15 ‚àí10 ‚àí5 0 5 10 15 f (b) Figure 3: Panel (a) and (b) show two marginal distributions p(fi |D, Œ∏) from a GPC posterior and its approximations. The true posterior is approximated by a normalised histogram of 9000 samples of fi obtained by MCMC sampling. Panel (c) shows a histogram of samples of a marginal distribution of a truncated high-dimensional Gaussian. The line describes a Gaussian with mean and variance estimated from the samples. For all three approximation techniques we see an agreement between marginal likelihood estimates and test performance, which justiÔ¨Åes the use of ML-II parameter estimation. But the shape of the contours and the values differ between the methods. The contours for Laplace‚Äôs method appear to be slanted compared to EP. The marginal likelihood estimates of EP and AIS agree surprisingly well1 , given that the marginal likelihood comes as a 767 respectively 200 dimensional integral. The EP predictions contain as much information about the test cases as the MCMC predictions and signiÔ¨Åcantly more than for LA. Note that for small signal variances (roughly ln(œÉ 2 ) < 1) LA and EP give very similar results. A possible explanation is that for small signal variances the likelihood does not truncate the prior but only down-weights the tail that disagrees with the observation. As an effect the posterior will be less skewed and both approximations will lead to similar results. For the USPS 3‚Äôs vs. 5‚Äôs we now inspect the marginal distributions p(fi |D, Œ∏) of single latent function values under the posterior approximations for a given value of Œ∏. We have chosen the values ln(œÉ) = 3.35 and ln( ) = 2.85 which are between the ML-II estimates of EP and LA. Hybrid MCMC was used to generate 9000 samples from the posterior p(f |D, Œ∏). For LA and EP the approximate marginals are q(fi |D, Œ∏) = N (fi |mi , Aii ) where m and A are found by the respective approximation techniques. In general we observe that the marginal distributions of MCMC samples agree very well with the respective marginal distributions of the EP approximation. For Laplace‚Äôs approximation we Ô¨Ånd the mean to be underestimated and the marginal distributions to overlap with zero far more than the EP approximations. Figure (3a) displays the marginal distribution and its approximations for which the MCMC samples show maximal skewness. Figure (3b) shows a typical example where the EP approximation agrees very well with the MCMC samples. We show this particular example because under the EP approximation p(yi = 1|D, Œ∏) < 0.1% but LA gives a wrong p(yi = 1|D, Œ∏) ‚âà 18%. In the experiment we saw that the marginal distributions of the posterior often agree very 1 Note that the agreement between the two seems to be limited by the accuracy of the MCMC runs, as judged by the regularity of the contour lines; the tolerance is less than one unit on a (natural) log scale. well with a Gaussian approximation. This seems to contradict the description given in the previous section were we argued that the posterior is skewed by construction. In order to inspect the marginals of a truncated high-dimensional multivariate Gaussian distribution we made an additional synthetic experiment. We constructed a 767 dimensional Gaussian N (x|0, C) with a covariance matrix having one eigenvalue of 100 with eigenvector 1, and all other eigenvalues are 1. We then truncate this distribution such that all xi ‚â• 0. Note that the mode of the truncated Gaussian is still at zero, whereas the mean moves towards the remaining mass. Figure (3c) shows a normalised histogram of samples from a marginal distribution of one xi . The samples agree very well with a Gaussian approximation. In the previous section we described the somewhat surprising property, that for a truncated high-dimensional Gaussian, resembling the posterior, the mode (used by LA) may not be particularly representative of the distribution. Although the marginal is also truncated, it is still exceptionally well modelled by a Gaussian ‚Äì however, the Laplace approximation centred on the origin would be completely inappropriate. In a second set of experiments we compare the predictive performance of LA and EP for GPC on several well known benchmark problems. Each data set is randomly split into 10 folds of which one at a time is left out as a test set to measure the predictive performance of a model trained (or selected) on the remaining nine folds. All performance measures are averages over the 10 folds. For GPC we implement model selection by ML-II hyperparameter estimation, reporting results given the Œ∏ that maximised the respective approximate marginal likelihoods p(D|Œ∏). In order to get a better picture of the absolute performance we also compare to results obtained by C-SVM classiÔ¨Åcation. The kernel we used is equivalent to the covariance function (1) without the signal variance parameter. For each fold the parameters C and are found in an inner loop of 5-fold cross-validation, in which the parameter grids are reÔ¨Åned until the performance stabilises. Predictive probabilities for test cases are obtained by mapping the unthresholded output of the SVM to [0, 1] using a sigmoid function [8]. Results are summarised in Table 1. Comparing Laplace‚Äôs method to EP the latter shows to be more accurate both in terms of error rate and information. While the error rates are relatively similar the predictive distribution obtained by EP shows to be more informative about the test targets. Note that for GPC the error rate only depends of the sign of the mean ¬µ‚àó of the approximated posterior over latent functions and not the entire posterior predictive distribution. As to be expected, the length of the mean vector m shows much larger values for the EP approximations. Comparing EP and SVMs the results are mixed. For the Crabs data set all methods show the same error rate but the information content of the predictive distributions differs dramatically. For some test cases the SVM predicts the wrong class with large certainty. 5 Summary & Conclusions Our experiments reveal serious differences between Laplace‚Äôs method and EP when used in GPC models. From the structural properties of the posterior we described why LA systematically underestimates the mean m. The resulting posterior GP over latent functions will have too small amplitude, although the sign of the mean function will be mostly correct. As an effect LA gives over-conservative predictive probabilities, and diminished information about the test labels. This effect has been show empirically on several real world examples. Large resulting discrepancies in the actual posterior probabilities were found, even at the training locations, which renders the predictive class probabilities produced under this approximation grossly inaccurate. Note, the difference becomes less dramatic if we only consider the classiÔ¨Åcation error rates obtained by thresholding p‚àó at 1/2. For this particular task, we‚Äôve seen the the sign of the latent function tends to be correct (at least at the training locations). Laplace EP SVM Data Set m n E% I m E% I m E% I Ionosphere 351 34 8.84 0.591 49.96 7.99 0.661 124.94 5.69 0.681 Wisconsin 683 9 3.21 0.804 62.62 3.21 0.805 84.95 3.21 0.795 Pima Indians 768 8 22.77 0.252 29.05 22.63 0.253 47.49 23.01 0.232 Crabs 200 7 2.0 0.682 112.34 2.0 0.908 2552.97 2.0 0.047 Sonar 208 60 15.36 0.439 26.86 13.85 0.537 15678.55 11.14 0.567 USPS 3 vs 5 1540 256 2.27 0.849 163.05 2.21 0.902 22011.70 2.01 0.918 Table 1: Results for benchmark data sets. The Ô¨Årst three columns give the name of the data set, number of observations m and dimension of inputs n. For Laplace‚Äôs method and EP the table reports the average error rate E%, the average information I (2) and the average length m of the mean vector of the Gaussian approximation. For SVMs the error rate and the average information about the test targets are reported. Note that for the Crabs data set we use the sex (not the colour) of the crabs as class label. The EP approximation has shown to give results very close to MCMC both in terms of predictive distributions and marginal likelihood estimates. We have shown and explained why the marginal distributions of the posterior can be well approximated by Gaussians. Further, the marginal likelihood values obtained by LA and EP differ systematically which will lead to different results of ML-II hyperparameter estimation. The discrepancies are similar for different tasks. Using AIS we were able to show the accuracy of marginal likelihood estimates, which to the best of our knowledge has never been done before. In summary, we found that EP is the method of choice for approximate inference in binary GPC models, when the computational cost of MCMC is prohibitive. In contrast, the Laplace approximation is so inaccurate that we advise against its use, especially when predictive probabilities are to be taken seriously. Further experiments and a detailed description of the approximation schemes can be found in [2]. Acknowledgements Both authors acknowledge support by the German Research Foundation (DFG) through grant RA 1030/1. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST2002-506778. This publication only reÔ¨Çects the authors‚Äô views. References [1] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, NIPS 8, pages 514‚Äì520. MIT Press, 1996. [2] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiÔ¨Åcation. Journal of Machine Learning Research, 6:1679‚Äì1704, 2005. [3] C. K. I. Williams and D. Barber. Bayesian classiÔ¨Åcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342‚Äì1351, 1998. [4] T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Department of Electrical Engineering and Computer Science, MIT, 2001. [5] R. M. Neal. Regression and classiÔ¨Åcation using Gaussian process priors. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics 6, pages 475‚Äì501. Oxford University Press, 1998. [6] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125‚Äì139, 2001. [7] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. CUP, 2003. [8] J. C. Platt. Probabilities for SV machines. In Advances in Large Margin ClassiÔ¨Åers, pages 61‚Äì73. The MIT Press, 2000.</p><p>5 0.61688244 <a title="21-lda-5" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>Author: Cristian Sminchisescu, Atul Kanujia, Zhiguo Li, Dimitris Metaxas</p><p>Abstract: We present a conditional temporal probabilistic framework for reconstructing 3D human motion in monocular video based on descriptors encoding image silhouette observations. For computational ef√Ñ?ƒπ≈° ciency we restrict visual inference to low-dimensional kernel induced non-linear state spaces. Our methodology (kBME) combines kernel PCA-based non-linear dimensionality reduction (kPCA) and Conditional Bayesian Mixture of Experts (BME) in order to learn complex multivalued predictors between observations and model hidden states. This is necessary for accurate, inverse, visual perception inferences, where several probable, distant 3D solutions exist due to noise or the uncertainty of monocular perspective projection. Low-dimensional models are appropriate because many visual processes exhibit strong non-linear correlations in both the image observations and the target, hidden state variables. The learned predictors are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty. We study several predictors and empirically show that the proposed algorithm positively compares with techniques based on regression, Kernel Dependency Estimation (KDE) or PCA alone, and gives results competitive to those of high-dimensional mixture predictors at a fraction of their computational cost. We show that the method successfully reconstructs the complex 3D motion of humans in real monocular video sequences. 1 Introduction and Related Work We consider the problem of inferring 3D articulated human motion from monocular video. This research topic has applications for scene understanding including human-computer interfaces, markerless human motion capture, entertainment and surveillance. A monocular approach is relevant because in real-world settings the human body parts are rarely completely observed even when using multiple cameras. This is due to occlusions form other people or objects in the scene. A robust system has to necessarily deal with incomplete, ambiguous and uncertain measurements. Methods for 3D human motion reconstruction can be classi√Ñ?ƒπ≈° ed as generative and discriminative. They both require a state representation, namely a 3D human model with kinematics (joint angles) or shape (surfaces or joint positions) and they both use a set of image features as observations for state inference. The computational goal in both cases is the conditional distribution for the model state given image observations. Generative model-based approaches [6, 16, 14, 13] have been demonstrated to √Ñ?ƒπ≈°&sbquo;exibly reconstruct complex unknown human motions and to naturally handle problem constraints. However it is dif√Ñ?ƒπ≈° cult to construct reliable observation likelihoods due to the complexity of modeling human appearance. This varies widely due to different clothing and deformation, body proportions or lighting conditions. Besides being somewhat indirect, the generative approach further imposes strict conditional independence assumptions on the temporal observations given the states in order to ensure computational tractability. Due to these factors inference is expensive and produces highly multimodal state distributions [6, 16, 13]. Generative inference algorithms require complex annealing schedules [6, 13] or systematic non-linear search for local optima [16] in order to ensure continuing tracking. These dif√Ñ?ƒπ≈° culties motivate the advent of a complementary class of discriminative algorithms [10, 12, 18, 2], that approximate the state conditional directly, in order to simplify inference. However, inverse, observation-to-state multivalued mappings are dif√Ñ?ƒπ≈° cult to learn (see e.g. √Ñ?ƒπ≈° g. 1a) and a probabilistic temporal setting is necessary. In an earlier paper [15] we introduced a probabilistic discriminative framework for human motion reconstruction. Because the method operates in the originally selected state and observation spaces that can be task generic, therefore redundant and often high-dimensional, inference is more expensive and can be less robust. To summarize, reconstructing 3D human motion in a Figure 1: (a, Left) Example of 180o ambiguity in predicting 3D human poses from silhouette image features (center). It is essential that multiple plausible solutions (e.g. F 1 and F2 ) are correctly represented and tracked over time. A single state predictor will either average the distant solutions or zig-zag between them, see also tables 1 and 2. (b, Right) A conditional chain model. The local distributions p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) or p(yt |zt ) are learned as in √Ñ?ƒπ≈° g. 2. For inference, the predicted local state conditional is recursively combined with the √Ñ?ƒπ≈° ltered prior c.f . (1). conditional temporal framework poses the following dif√Ñ?ƒπ≈° culties: (i) The mapping between temporal observations and states is multivalued (i.e. the local conditional distributions to be learned are multimodal), therefore it cannot be accurately represented using global function approximations. (ii) Human models have multivariate, high-dimensional continuous states of 50 or more human joint angles. The temporal state conditionals are multimodal which makes ef√Ñ?ƒπ≈° cient Kalman √Ñ?ƒπ≈° ltering algorithms inapplicable. General inference methods (particle √Ñ?ƒπ≈° lters, mixtures) have to be used instead, but these are expensive for high-dimensional models (e.g. when reconstructing the motion of several people that operate in a joint state space). (iii) The components of the human state and of the silhouette observation vector exhibit strong correlations, because many repetitive human activities like walking or running have low intrinsic dimensionality. It appears wasteful to work with high-dimensional states of 50+ joint angles. Even if the space were truly high-dimensional, predicting correlated state dimensions independently may still be suboptimal. In this paper we present a conditional temporal estimation algorithm that restricts visual inference to low-dimensional, kernel induced state spaces. To exploit correlations among observations and among state variables, we model the local, temporal conditional distributions using ideas from Kernel PCA [11, 19] and conditional mixture modeling [7, 5], here adapted to produce multiple probabilistic predictions. The corresponding predictor is referred to as a Conditional Bayesian Mixture of Low-dimensional Kernel-Induced Experts (kBME). By integrating it within a conditional graphical model framework (√Ñ?ƒπ≈° g. 1b), we can exploit temporal constraints probabilistically. We demonstrate that this methodology is effective for reconstructing the 3D motion of multiple people in monocular video. Our contribution w.r.t. [15] is a probabilistic conditional inference framework that operates over a non-linear, kernel-induced low-dimensional state spaces, and a set of experiments (on both real and arti√Ñ?ƒπ≈° cial image sequences) that show how the proposed framework positively compares with powerful predictors based on KDE, PCA, or with the high-dimensional models of [15] at a fraction of their cost. 2 Probabilistic Inference in a Kernel Induced State Space We work with conditional graphical models with a chain structure [9], as shown in √Ñ?ƒπ≈° g. 1b, These have continuous temporal states yt , t = 1 . . . T , observations zt . For compactness, we denote joint states Yt = (y1 , y2 , . . . , yt ) or joint observations Zt = (z1 , . . . , zt ). Learning and inference are based on local conditionals: p(yt |zt ) and p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), with yt and zt being low-dimensional, kernel induced representations of some initial model having state xt and observation rt . We obtain zt , yt from rt , xt using kernel PCA [11, 19]. Inference is performed in a low-dimensional, non-linear, kernel induced latent state space (see √Ñ?ƒπ≈° g. 1b and √Ñ?ƒπ≈° g. 2 and (1)). For display or error reporting, we compute the original conditional p(x|r), or a temporally √Ñ?ƒπ≈° ltered version p(xt |Rt ), Rt = (r1 , r2 , . . . , rt ), using a learned pre-image state map [3]. 2.1 Density Propagation for Continuous Conditional Chains For online √Ñ?ƒπ≈° ltering, we compute the optimal distribution p(yt |Zt ) for the state yt , conditioned by observations Zt up to time t. The √Ñ?ƒπ≈° ltered density can be recursively derived as: p(yt |Zt ) = p(yt |ytƒÇÀò&circ;&rsquo;1 , zt )p(ytƒÇÀò&circ;&rsquo;1 |ZtƒÇÀò&circ;&rsquo;1 ) (1) ytƒÇÀò&circ;&rsquo;1 We compute using a conditional mixture for p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) (a Bayesian mixture of experts c.f . ƒÇ&sbquo;√Ç¬ß2.2) and the prior p(ytƒÇÀò&circ;&rsquo;1 |ZtƒÇÀò&circ;&rsquo;1 ), each having, say M components. We integrate M 2 pairwise products of Gaussians analytically. The means of the expanded posterior are clustered and the centers are used to initialize a reduced M -component Kullback-Leibler approximation that is re√Ñ?ƒπ≈° ned using gradient descent [15]. The propagation rule (1) is similar to the one used for discrete state labels [9], but here we work with multivariate continuous state spaces and represent the local multimodal state conditionals using kBME (√Ñ?ƒπ≈° g. 2), and not log-linear models [9] (these would require intractable normalization). This complex continuous model rules out inference based on Kalman √Ñ?ƒπ≈° ltering or dynamic programming [9]. 2.2 Learning Bayesian Mixtures over Kernel Induced State Spaces (kBME) In order to model conditional mappings between low-dimensional non-linear spaces we rely on kernel dimensionality reduction and conditional mixture predictors. The authors of KDE [19] propose a powerful structured unimodal predictor. This works by decorrelating the output using kernel PCA and learning a ridge regressor between the input and each decorrelated output dimension. Our procedure is also based on kernel PCA but takes into account the structure of the studied visual problem where both inputs and outputs are likely to be low-dimensional and the mapping between them multivalued. The output variables xi are projected onto the column vectors of the principal space in order to obtain their principal coordinates y i . A z ƒÇÀò&circ;&circ; P(Fr ) O p(y|z) kP CA ƒÇ≈Ωƒπ&scaron;r (r) ƒÇÀò&Scaron;&sbquo; Fr O / y ƒÇÀò&circ;&circ; P(Fx ) O QQQ QQQ QQQ kP CA QQQ Q( ƒÇ≈Ωƒπ&scaron;x (x) ƒÇÀò&Scaron;&sbquo; Fx x ƒÇÀò&permil;&circ; PreImage(y) O ƒÇ≈Ωƒπ&scaron;r ƒÇ≈Ωƒπ&scaron;x r ƒÇÀò&circ;&circ; R ƒÇÀò&Scaron;&sbquo; Rr x ƒÇÀò&circ;&circ; X ƒÇÀò&Scaron;&sbquo; Rx  p(x|r) ƒÇÀò&permil;&circ; p(x|y) Figure 2: The learned low-dimensional predictor, kBME, for computing p(x|r) ƒÇÀò&permil;√Ñ&bdquo; p(xt |rt ), ƒÇÀò&circ;&euro;t. (We similarly learn p(xt |xtƒÇÀò&circ;&rsquo;1 , rt ), with input (x, r) instead of r ƒÇÀò&euro;&ldquo; here we illustrate only p(x|r) for clarity.) The input r and the output x are decorrelated using Kernel PCA to obtain z and y respectively. The kernels used for the input and output are ƒÇ≈Ωƒπ&scaron; r and ƒÇ≈Ωƒπ&scaron;x , with induced feature spaces Fr and Fx , respectively. Their principal subspaces obtained by kernel PCA are denoted by P(Fr ) and P(Fx ), respectively. A conditional Bayesian mixture of experts p(y|z) is learned using the low-dimensional representation (z, y). Using learned local conditionals of the form p(yt |zt ) or p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), temporal inference can be ef√Ñ?ƒπ≈° ciently performed in a low-dimensional kernel induced state space (see e.g. (1) and √Ñ?ƒπ≈° g. 1b). For visualization and error measurement, the √Ñ?ƒπ≈° ltered density, e.g. p(yt |Zt ), can be mapped back to p(xt |Rt ) using the pre-image c.f . (3). similar procedure is performed on the inputs ri to obtain zi . In order to relate the reduced feature spaces of z and y (P(Fr ) and P(Fx )), we estimate a probability distribution over mappings from training pairs (zi , yi ). We use a conditional Bayesian mixture of experts (BME) [7, 5] in order to account for ambiguity when mapping similar, possibly identical reduced feature inputs to very different feature outputs, as common in our problem (√Ñ?ƒπ≈° g. 1a). This gives a model that is a conditional mixture of low-dimensional kernel-induced experts (kBME): M g(z|ƒÇ≈Ω√Ç¬¥ j )N (y|Wj z, ƒÇ≈Ωƒπ j ) p(y|z) = (2) j=1 where g(z|ƒÇ≈Ω√Ç¬¥ j ) is a softmax function parameterized by ƒÇ≈Ω√Ç¬¥ j and (Wj , ƒÇ≈Ωƒπ j ) are the parameters and the output covariance of expert j, here a linear regressor. As in many Bayesian settings [17, 5], the weights of the experts and of the gates, Wj and ƒÇ≈Ω√Ç¬¥ j , are controlled by hierarchical priors, typically Gaussians with 0 mean, and having inverse variance hyperparameters controlled by a second level of Gamma distributions. We learn this model using a double-loop EM and employ ML-II type approximations [8, 17] with greedy (weight) subset selection [17, 15]. Finally, the kBME algorithm requires the computation of pre-images in order to recover the state distribution x from itƒÇÀò&euro;&trade;s image y ƒÇÀò&circ;&circ; P(Fx ). This is a closed form computation for polynomial kernels of odd degree. For more general kernels optimization or learning (regression based) methods are necessary [3]. Following [3, 19], we use a sparse Bayesian kernel regressor to learn the pre-image. This is based on training data (xi , yi ): p(x|y) = N (x|AƒÇ≈Ωƒπ&scaron;y (y), ƒÇÀò&bdquo;ƒπ&scaron;) (3) with parameters and covariances (A, ƒÇÀò&bdquo;ƒπ&scaron;). Since temporal inference is performed in the low-dimensional kernel induced state space, the pre-image function needs to be calculated only for visualizing results or for the purpose of error reporting. Propagating the result from the reduced feature space P(Fx ) to the output space X pro- duces a Gaussian mixture with M elements, having coef√Ñ?ƒπ≈° cients g(z|ƒÇ≈Ω√Ç¬¥ j ) and components N (x|AƒÇ≈Ωƒπ&scaron;y (Wj z), AJƒÇ≈Ωƒπ&scaron;y ƒÇ≈Ωƒπ j JƒÇ≈Ωƒπ&scaron;y A + ƒÇÀò&bdquo;ƒπ&scaron;), where JƒÇ≈Ωƒπ&scaron;y is the Jacobian of the mapping ƒÇ≈Ωƒπ&scaron;y . 3 Experiments We run experiments on both real image sequences (√Ñ?ƒπ≈° g. 5 and √Ñ?ƒπ≈° g. 6) and on sequences where silhouettes were arti√Ñ?ƒπ≈° cially rendered. The prediction error is reported in degrees (for mixture of experts, this is w.r.t. the most probable one, but see also √Ñ?ƒπ≈° g. 4a), and normalized per joint angle, per frame. The models are learned using standard cross-validation. Pre-images are learned using kernel regressors and have average error 1.7o . Training Set and Model State Representation: For training we gather pairs of 3D human poses together with their image projections, here silhouettes, using the graphics package Maya. We use realistically rendered computer graphics human surface models which we animate using human motion capture [1]. Our original human representation (x) is based on articulated skeletons with spherical joints and has 56 skeletal d.o.f. including global translation. The database consists of 8000 samples of human activities including walking, running, turns, jumps, gestures in conversations, quarreling and pantomime. Image Descriptors: We work with image silhouettes obtained using statistical background subtraction (with foreground and background models). Silhouettes are informative for pose estimation although prone to ambiguities (e.g. the left / right limb assignment in side views) or occasional lack of observability of some of the d.o.f. (e.g. 180o ambiguities in the global azimuthal orientation for frontal views, e.g. √Ñ?ƒπ≈° g. 1a). These are multiplied by intrinsic forward / backward monocular ambiguities [16]. As observations r, we use shape contexts extracted on the silhouette [4] (5 radial, 12 angular bins, size range 1/8 to 3 on log scale). The features are computed at different scales and sizes for points sampled on the silhouette. To work in a common coordinate system, we cluster all features in the training set into K = 50 clusters. To compute the representation of a new shape feature (a point on the silhouette), we ƒÇÀò&euro;&tilde;projectƒÇÀò&euro;&trade; onto the common basis by (inverse distance) weighted voting into the cluster centers. To obtain the representation (r) for a new silhouette we regularly sample 200 points on it and add all their feature vectors into a feature histogram. Because the representation uses overlapping features of the observation the elements of the descriptor are not independent. However, a conditional temporal framework (√Ñ?ƒπ≈° g. 1b) √Ñ?ƒπ≈°&sbquo;exibly accommodates this. For experiments, we use Gaussian kernels for the joint angle feature space and dot product kernels for the observation feature space. We learn state conditionals for p(yt |zt ) and p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) using 6 dimensions for the joint angle kernel induced state space and 25 dimensions for the observation induced feature space, respectively. In √Ñ?ƒπ≈° g. 3b) we show an evaluation of the ef√Ñ?ƒπ≈° cacy of our kBME predictor for different dimensions in the joint angle kernel induced state space (the observation feature space dimension is here 50). On the analyzed dancing sequence, that involves complex motions of the arms and the legs, the non-linear model signi√Ñ?ƒπ≈° cantly outperforms alternative PCA methods and gives good predictions for compact, low-dimensional models.1 In tables 1 and 2, as well as √Ñ?ƒπ≈° g. 4, we perform quantitative experiments on arti√Ñ?ƒπ≈° cially rendered silhouettes. 3D ground truth joint angles are available and this allows a more 1 Running times: On a Pentium 4 PC (3 GHz, 2 GB RAM), a full dimensional BME model with 5 experts takes 802s to train p(xt |xtƒÇÀò&circ;&rsquo;1 , rt ), whereas a kBME (including the pre-image) takes 95s to train p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ). The prediction time is 13.7s for BME and 8.7s (including the pre-image cost 1.04s) for kBME. The integration in (1) takes 2.67s for BME and 0.31s for kBME. The speed-up for kBME is signi√Ñ?ƒπ≈° cant and likely to increase with original models having higher dimensionality. Prediction Error Number of Clusters 100 1000 100 10 1 1 2 3 4 5 6 7 8 Degree of Multimodality kBME KDE_RVM PCA_BME PCA_RVM 10 1 0 20 40 Number of Dimensions 60 Figure 3: (a, Left) Analysis of ƒÇÀò&euro;&tilde;multimodalityƒÇÀò&euro;&trade; for a training set. The input zt dimension is 25, the output yt dimension is 6, both reduced using kPCA. We cluster independently in (ytƒÇÀò&circ;&rsquo;1 , zt ) and yt using many clusters (2100) to simulate small input perturbations and we histogram the yt clusters falling within each cluster in (ytƒÇÀò&circ;&rsquo;1 , zt ). This gives intuition on the degree of ambiguity in modeling p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), for small perturbations in the input. (b, Right) Evaluation of dimensionality reduction methods for an arti√Ñ?ƒπ≈° cial dancing sequence (models trained on 300 samples). The kBME is our model ƒÇ&sbquo;√Ç¬ß2.2, whereas the KDE-RVM is a KDE model learned with a Relevance Vector Machine (RVM) [17] feature space map. PCA-BME and PCA-RVM are models where the mappings between feature spaces (obtained using PCA) is learned using a BME and a RVM. The non-linearity is signi√Ñ?ƒπ≈° cant. Kernel-based methods outperform PCA and give low prediction error for 5-6d models. systematic evaluation. Notice that the kernelized low-dimensional models generally outperform the PCA ones. At the same time, they give results competitive to the ones of high-dimensional BME predictors, while being lower-dimensional and therefore signi√Ñ?ƒπ≈° cantly less expensive for inference, e.g. the integral in (1). In √Ñ?ƒπ≈° g. 5 and √Ñ?ƒπ≈° g. 6 we show human motion reconstruction results for two real image sequences. Fig. 5 shows the good quality reconstruction of a person performing an agile jump. (Given the missing observations in a side view, 3D inference for the occluded body parts would not be possible without using prior knowledge!) For this sequence we do inference using conditionals having 5 modes and reduced 6d states. We initialize tracking using p(yt |zt ), whereas for inference we use p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ) within (1). In the second sequence in √Ñ?ƒπ≈° g. 6, we simultaneously reconstruct the motion of two people mimicking domestic activities, namely washing a window and picking an object. Here we do inference over a product, 12-dimensional state space consisting of the joint 6d state of each person. We obtain good 3D reconstruction results, using only 5 hypotheses. Notice however, that the results are not perfect, there are small errors in the elbow and the bending of the knee for the subject at the l.h.s., and in the different wrist orientations for the subject at the r.h.s. This re√Ñ?ƒπ≈°&sbquo;ects the bias of our training set. Walk and turn Conversation Run and turn left KDE-RR 10.46 7.95 5.22 RVM 4.95 4.96 5.02 KDE-RVM 7.57 6.31 6.25 BME 4.27 4.15 5.01 kBME 4.69 4.79 4.92 Table 1: Comparison of average joint angle prediction error for different models. All kPCA-based models use 6 output dimensions. Testing is done on 100 video frames for each sequence, the inputs are arti√Ñ?ƒπ≈° cially generated silhouettes, not in the training set. 3D joint angle ground truth is used for evaluation. KDE-RR is a KDE model with ridge regression (RR) for the feature space mapping, KDE-RVM uses an RVM. BME uses a Bayesian mixture of experts with no dimensionality reduction. kBME is our proposed model. kPCAbased methods use kernel regressors to compute pre-images. Expert Prediction Frequency ƒÇÀò&circ;&rsquo; Closest to Ground truth Frequency ƒÇÀò&circ;&rsquo; Close to ground truth 30 25 20 15 10 5 0 1 2 3 4 Expert Number 14 10 8 6 4 2 0 5 1st Probable Prev Output 2nd Probable Prev Output 3rd Probable Prev Output 4th Probable Prev Output 5th Probable Prev Output 12 1 2 3 4 Current Expert 5 Figure 4: (a, Left) Histogram showing the accuracy of various expert predictors: how many times the expert ranked as the k-th most probable by the model (horizontal axis) is closest to the ground truth. The model is consistent (the most probable expert indeed is the most accurate most frequently), but occasionally less probable experts are better. (b, Right) Histograms show the dynamics of p(yt |ytƒÇÀò&circ;&rsquo;1 , zt ), i.e. how the probability mass is redistributed among experts between two successive time steps, in a conversation sequence. Walk and turn back Run and turn KDE-RR 7.59 17.7 RVM 6.9 16.8 KDE-RVM 7.15 16.08 BME 3.6 8.2 kBME 3.72 8.01 Table 2: Joint angle prediction error computed for two complex sequences with walks, runs and turns, thus more ambiguity (100 frames). Models have 6 state dimensions. Unimodal predictors average competing solutions. kBME has signi√Ñ?ƒπ≈° cantly lower error. Figure 5: Reconstruction of a jump (selected frames). Top: original image sequence. Middle: extracted silhouettes. Bottom: 3D reconstruction seen from a synthetic viewpoint. 4 Conclusion We have presented a probabilistic framework for conditional inference in latent kernelinduced low-dimensional state spaces. Our approach has the following properties: (a) Figure 6: Reconstructing the activities of 2 people operating in an 12-d state space (each person has its own 6d state). Top: original image sequence. Bottom: 3D reconstruction seen from a synthetic viewpoint. Accounts for non-linear correlations among input or output variables, by using kernel nonlinear dimensionality reduction (kPCA); (b) Learns probability distributions over mappings between low-dimensional state spaces using conditional Bayesian mixture of experts, as required for accurate prediction. In the resulting low-dimensional kBME predictor ambiguities and multiple solutions common in visual, inverse perception problems are accurately represented. (c) Works in a continuous, conditional temporal probabilistic setting and offers a formal management of uncertainty. We show comparisons that demonstrate how the proposed approach outperforms regression, PCA or KDE alone for reconstructing the 3D human motion in monocular video. Future work we will investigate scaling aspects for large training sets and alternative structured prediction methods. References [1] CMU Human Motion DataBase. Online at http://mocap.cs.cmu.edu/search.html, 2003. [2] A. Agarwal and B. Triggs. 3d human pose from silhouettes by Relevance Vector Regression. In CVPR, 2004. [3] G. Bakir, J. Weston, and B. Scholkopf. Learning to √Ñ?ƒπ≈° nd pre-images. In NIPS, 2004. [4] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. PAMI, 24, 2002. [5] C. Bishop and M. Svensen. Bayesian mixtures of experts. In UAI, 2003. [6] J. Deutscher, A. Blake, and I. Reid. Articulated Body Motion Capture by Annealed Particle Filtering. In CVPR, 2000. [7] M. Jordan and R. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation, (6):181ƒÇÀò&euro;&ldquo;214, 1994. [8] D. Mackay. Bayesian interpolation. Neural Computation, 4(5):720ƒÇÀò&euro;&ldquo;736, 1992. [9] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction and segmentation. In ICML, 2000. [10] R. Rosales and S. Sclaroff. Learning Body Pose Via Specialized Maps. In NIPS, 2002. [11] B. SchƒÇ&sbquo;√Ç¬® lkopf, A. Smola, and K. MƒÇ&sbquo;√Ç¬® ller. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Computation, 10:1299ƒÇÀò&euro;&ldquo;1319, 1998. [12] G. Shakhnarovich, P. Viola, and T. Darrell. Fast Pose Estimation with Parameter Sensitive Hashing. In ICCV, 2003. [13] L. Sigal, S. Bhatia, S. Roth, M. Black, and M. Isard. Tracking Loose-limbed People. In CVPR, 2004. [14] C. Sminchisescu and A. Jepson. Generative Modeling for Continuous Non-Linearly Embedded Visual Inference. In ICML, pages 759ƒÇÀò&euro;&ldquo;766, Banff, 2004. [15] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Discriminative Density Propagation for 3D Human Motion Estimation. In CVPR, 2005. [16] C. Sminchisescu and B. Triggs. Kinematic Jump Processes for Monocular 3D Human Tracking. In CVPR, volume 1, pages 69ƒÇÀò&euro;&ldquo;76, Madison, 2003. [17] M. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. JMLR, 2001. [18] C. Tomasi, S. Petrov, and A. Sastry. 3d tracking = classi√Ñ?ƒπ≈° cation + interpolation. In ICCV, 2003. [19] J. Weston, O. Chapelle, A. Elisseeff, B. Scholkopf, and V. Vapnik. Kernel dependency estimation. In NIPS, 2002.</p><p>6 0.61429137 <a title="21-lda-6" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>7 0.61075616 <a title="21-lda-7" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>8 0.60909408 <a title="21-lda-8" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>9 0.60679436 <a title="21-lda-9" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>10 0.60477853 <a title="21-lda-10" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>11 0.60414618 <a title="21-lda-11" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>12 0.60394591 <a title="21-lda-12" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>13 0.60113919 <a title="21-lda-13" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>14 0.59949774 <a title="21-lda-14" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>15 0.59927422 <a title="21-lda-15" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>16 0.59923565 <a title="21-lda-16" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>17 0.59770519 <a title="21-lda-17" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>18 0.59698504 <a title="21-lda-18" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>19 0.59667343 <a title="21-lda-19" href="./nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">197 nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<p>20 0.59633338 <a title="21-lda-20" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
