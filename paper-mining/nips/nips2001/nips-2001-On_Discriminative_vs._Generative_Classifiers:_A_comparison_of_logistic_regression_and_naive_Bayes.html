<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-133" href="#">nips2001-133</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</h1>
<br/><p>Source: <a title="nips-2001-133-pdf" href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf">pdf</a></p><p>Author: Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster. 1</p><p>Reference: <a title="nips-2001-133-reference" href="../nips2001_reference/nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Generative classifiers: A comparison of logistic regression and naive Bayes  Andrew Y. [sent-2, score-0.727]
</p><p>2 University of California, Berkeley University of California, Berkeley Berkeley, CA 94720 Berkeley, CA 94720  Abstract We compare discriminative and generative learning as typified by logistic regression and naive Bayes. [sent-9, score-1.1]
</p><p>3 We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. [sent-10, score-0.438]
</p><p>4 This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster. [sent-11, score-0.887]
</p><p>5 1  Introduction  Generative classifiers learn a model of the joint probability, p( x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(ylx), and then picking the most likely label y. [sent-12, score-0.34]
</p><p>6 Discriminative classifiers model the posterior p(ylx) directly, or learn a direct map from inputs x to the class labels. [sent-13, score-0.231]
</p><p>7 " Indeed, leaving aside computational issues and matters such as handling missing data, the prevailing consensus seems to be that discriminative classifiers are almost always to be preferred to generative ones. [sent-15, score-0.568]
</p><p>8 Another piece of prevailing folk wisdom is that the number of examples needed to fit a model is often roughly linear in the number of free parameters of a model. [sent-16, score-0.176]
</p><p>9 , [1, 3]), and it is known that sample complexity in the discriminative setting is linear in the VC dimension [6]. [sent-19, score-0.263]
</p><p>10 A parametric family of probabilistic models p(x, y) can be fit either to optimize the joint likelihood of the inputs and the labels, or fit to optimize the conditional likelihood p(ylx), or even fit to minimize the 0-1 training error obtained  by thresholding p(ylx) to make predictions. [sent-21, score-0.542]
</p><p>11 Given a classifier hGen fit according to the first criterion, and a model h Dis fit according to either the second or the third criterion (using the same parametric family of models) , we call hGen and h Dis a Generative-Discriminative pair. [sent-22, score-0.418]
</p><p>12 For example, if p(xly) is Gaussian and p(y) is multinomial, then the corresponding Generative-Discriminative pair is Normal Discriminant Analysis and logistic regression. [sent-23, score-0.277]
</p><p>13 Similarly, for the case of discrete inputs it is also well known that the naive Bayes classifier and logistic regression form a Generative-Discriminative pair [4, 5]. [sent-24, score-1.052]
</p><p>14 To compare generative and discriminative learning, it seems natural to focus on such pairs. [sent-25, score-0.395]
</p><p>15 2  Preliminaries  We consider a binary classification task, and begin with the case of discrete data. [sent-28, score-0.104]
</p><p>16 Let X = {O, l}n be the n-dimensional input space, where we have assumed binary inputs for simplicity (the generalization offering no difficulties). [sent-29, score-0.093]
</p><p>17 Let the output labels be Y = {T, F}, and let there be a joint distribution V over X x Y from which a training set S = {x(i) , y(i) }~1 of m iid examples is drawn. [sent-30, score-0.111]
</p><p>18 The generative naive Bayes classifier uses S to calculate estimates p(xiIY) and p(y) of the probabilities p(xi IY) and p(y), as follows: ' (1) P(x-, = 11Y = b) = #s{xi=l ,y=b}+1 #s{y-b}+21 (and similarly for p(y = b),) where #s{-} counts the number of occurrences of an event in the training set S. [sent-31, score-0.702]
</p><p>19 Here, setting l = corresponds to taking the empirical estimates of the probabilities, and l is more traditionally set to a positive value such as 1, which corresponds to using Laplace smoothing of the probabilities. [sent-32, score-0.028]
</p><p>20 To classify a test example x, the naive Bayes classifier hGen : X r-+ Y predicts hGen(x) = T if and only if the following quantity is positive:  °  IGen(x ) = log  (rr~-d) (x i ly  = T))p(y = T)  (rrn P(X,_ - F)) P Y ' IY _ '( _ i=1  ~ p(xilY = T) F) = L. [sent-33, score-0.671]
</p><p>21 The parameters are fit via maximum likelihood, so for example {ti ly=b is the empirical mean of the i-th coordinate of all the examples in the training set with label y = b. [sent-36, score-0.213]
</p><p>22 In both the discrete and the continuous cases, it is well known that the discriminative analog of naive Bayes is logistic regression. [sent-40, score-0.996]
</p><p>23 Given a test example x,  a;  the discriminative logistic regression classifier ho is : X and only if the linear discriminant function  I-t  Y predicts hOis (x)  = T if  lDis(x) = L~=l (3ixi + () (3) is positive. [sent-42, score-0.917]
</p><p>24 Being a discriminative model, the parameters [(3, ()] can be fit either to maximize the conditionallikelikood on the training set L~= llogp(y(i) Ix(i); (3, ()), or to minimize 0-1 training error L~= ll{hois(x(i)) 1- y(i)}, where 1{-} is the indicator function (I{True} = 1, I{False} = 0) . [sent-43, score-0.463]
</p><p>25 Insofar as the error metric is 0-1 classification error, we view the latter alternative as being more truly in the "spirit" of discriminative learning, though the former is also frequently used as a computationally efficient approximation to the latter. [sent-44, score-0.365]
</p><p>26 In this paper, we will largely ignore the difference between these two versions of discriminative learning and, with some abuse of terminology, will loosely use the term "logistic regression" to refer to either, though our formal analyses will focus on the latter method. [sent-45, score-0.309]
</p><p>27 Finally, let 1i be the family of all linear classifiers (maps from X to Y); and given a classifier h : X I-t y, define its generalization error to be c(h) = Pr(x,y)~v [h(x) 1- y]. [sent-46, score-0.409]
</p><p>28 3  Analysis of algorithms  When V is such that the two classes are far from linearly separable, neither logistic regression nor naive Bayes can possibly do well, since both are linear classifiers. [sent-47, score-0.727]
</p><p>29 Thus, to obtain non-trivial results, it is most interesting to compare the performance of these algorithms to their asymptotic errors (cf. [sent-48, score-0.136]
</p><p>30 More precisely, let hGen,oo be the population version of the naive Bayes classifier; i. [sent-50, score-0.343]
</p><p>31 hGen,oo is the naive Bayes classifier with parameters p(xly) = p(xly),p(y) = p(y). [sent-52, score-0.478]
</p><p>32 Similarly, let hOis ,oo be the population version of logistic regression. [sent-53, score-0.308]
</p><p>33 Proposition 1 Let  hGen and h Dis be any generative-discriminative pair of classifiers, and hGen,oo and hois, oo be their asymptotic/population versions. [sent-55, score-0.048]
</p><p>34 Proposition 2 Let h Dis be logistic regression in n-dimensions. [sent-57, score-0.415]
</p><p>35 Then with high  probability c(hois ) :S c(hois,oo) + 0 (J ~ log ~) Thus, for c(hOis ) :S c(hOis,oo) + EO to hold with high probability (here, fixed constant), it suffices to pick m = O(n). [sent-58, score-0.177]
</p><p>36 EO  > 0 is some  Proposition 1 states that aymptotically, the error of the discriminative logistic regression is smaller than that of the generative naive Bayes. [sent-59, score-1.138]
</p><p>37 This is easily shown by observing that, since c(hDis) converges to infhE1-l c(h) (where 1i is the class of all linear classifiers), it must therefore be asymptotically no worse than the linear classifier picked by naive Bayes. [sent-60, score-0.507]
</p><p>38 This proposition also provides a basis for what seems to be the widely held belief that discriminative classifiers are better than generative ones. [sent-61, score-0.664]
</p><p>39 Proposition 2 is another standard result, and is a straightforward application of Vapnik's uniform convergence bounds to logistic regression, and using the fact that 1i has VC dimension n. [sent-62, score-0.277]
</p><p>40 The second part of the proposition states that the sample complexity of discriminative learning- that is, the number of examples needed to approach the asymptotic error- is at most on the order of n. [sent-63, score-0.547]
</p><p>41 Note that the worst case sample complexity is also lower-bounded by order n [6]. [sent-64, score-0.025]
</p><p>42 lUnder a technical assumption (that is true for most classifiers, including logistic regression) that the family of possible classifiers hOis (in the case of logistic regression, this is 1l) has finite VC dimension. [sent-65, score-0.749]
</p><p>43 The picture for discriminative learning is thus fairly well-understood: The error converges to that of the best linear classifier, and convergence occurs after on the order of n examples. [sent-66, score-0.302]
</p><p>44 How about generative learning, specifically the case of the naive Bayes classifier? [sent-67, score-0.45]
</p><p>45 Assume that for some fixed Po > 0, we have that Po :s: p(y = T) :s: 1 - Po. [sent-70, score-0.024]
</p><p>46 In case of discrete inputs, IjJ(XiIY = b) - p(xilY b) - p(y = b) I :s: 101, for all i = 1, . [sent-73, score-0.066]
</p><p>47 In the case of continuous inputs, IPi ly=b - f-li ly=b I :s: 101, laT IjJ(y = b) - p(y = b) I :s: 101 for all i = 1, . [sent-78, score-0.085]
</p><p>48 By the Chernoff bound, with probability at least 1 - 81 = 1- 2exp(-2Eim) , the fraction of positive examples will be within 101 of p(y = T) , which implies IjJ(y = b) - p(y = b)1 :s: 101, and we have at least 1 m positive and 1m negative examples, where I = Po - 101 = 0(1). [sent-85, score-0.153]
</p><p>49 So by the Chernoff bound again , for specific i, b, the chance that IjJ(XiIY = b) - p(xilY = b)1 > 101 is at most 82 = 2exp(-2Ehm). [sent-86, score-0.087]
</p><p>50 Since there are 2n such probabilities, the overall chance of error, by the Union bound, is at most 81 + 2n82 . [sent-87, score-0.048]
</p><p>51 Substituting in 81 and 8/s definitions , we see that to guarantee 81 + 2n82 :s: 8, it suffices that m is as stated. [sent-88, score-0.048]
</p><p>52 Lastly, smoothing (l > 0) adds at most a small, O(l/m) perturbation to these probabilities , and using the same argument as above with (say) 101/2 instead of 101, and arguing that this O(l/m) perturbation is at most 101/2 (which it is as m is at least order l/Ei) , again gives the result. [sent-89, score-0.092]
</p><p>53 The result for the continuous case is proved similarly using a Chernoff-bounds based argument (and the assumption that Xi E [0,1]). [sent-90, score-0.13]
</p><p>54 D Thus, with a number of samples that is only logarithmic, rather than linear, in n, the parameters of the generative classifier hGen are uniformly close to their asymptotic values in hGen ,oo . [sent-91, score-0.44]
</p><p>55 Is is tempting to conclude therefore that c(hGen), the error of the generative naive Bayes classifier, also converges to its asymptotic value of c(hGen,oo) after this many examples, implying only 0 (log n) examples are required to fit a naive Bayes model. [sent-92, score-1.106]
</p><p>56 We will shortly establish some simple conditions under which this intuition is indeed correct. [sent-93, score-0.032]
</p><p>57 Note that this implies that, even though naive Bayes converges to a higher asymptotic error of c(hGen,oo) compared to logistic regression's c: (hDis, oo ), it may also approach it significantly faster-after O(log n), rather than O(n), training examples. [sent-94, score-0.933]
</p><p>58 One way of showing c(hGen) approaches c(hGen,oo) is by showing that the parameters' convergence implies that hGen is very likely to make the same predictions as hGen,oo . [sent-95, score-0.065]
</p><p>59 Recall hGen makes its predictions by thresholding the discriminant function lGen defined in (2). [sent-96, score-0.164]
</p><p>60 Let lGen,oo be the corresponding discriminant function used by hGen,oo. [sent-97, score-0.101]
</p><p>61 Moreover, as long as lGen,oo (x) is, with fairly high probability, far from zero, then lGen (x), being a small perturbation of lGen ,oo(x), will also be usually on the same side ofzero as lGen ,oo (x). [sent-99, score-0.058]
</p><p>62 Assume that for some fixed Po > 0, we have Po :s: p(y = T) :s: 1 - Po, and that either Po :s: P(Xi = 11Y = b) :s: 1 - Po for all i, b (in the case of discrete inputs), or O"T 2: Po (in the continuous case). [sent-101, score-0.199]
</p><p>63 c(hGen) - c(hGen,oo) is upperbounded by the chance that hGen,oo correctly classifies a randomly chosen example, but hGen misclassifies it. [sent-104, score-0.048]
</p><p>64 This in turn implies that everyone of the n + 1 terms in the sum in lGen (as in Equation 2) is within O( j(1ogn)/m) of the corresponding term in lGen ,oo , and hence that IlGen(x) -lGen,oo(x)1 :S O(nj(1ogn)/m). [sent-106, score-0.03]
</p><p>65 D The key quantity in the Theorem is the G(T) , which must be small when T is small in order for the bound to be non-trivial. [sent-109, score-0.039]
</p><p>66 Note G(T) is upper-bounded by Prx[lGen,oo(x) E [-Tn, Tn]]-the chance that lGen, oo(X) (a random variable whose distribution is induced by x ""' V) falls near zero. [sent-110, score-0.048]
</p><p>67 To gain intuition about the scaling of these random variables, consider the following: Proposition 5 Suppose that, for at least an 0(1) fraction of the features i (i = 1, . [sent-111, score-0.056]
</p><p>68 ,n), it holds true that IP(Xi = 11Y = T) - P(Xi = 11Y = F)I ::::: 'Y for some fixed'Y > 0 (or IJLi ly=T - JLi ly=FI ::::: 'Y in the case of continuous inputs). [sent-114, score-0.111]
</p><p>69 Thus, as long as the class label gives information about an 0(1) fraction of the features (or less formally, as long as most of the features are "relevant" to the class label), the expected value of IlGen, oo(X) I will be O(n). [sent-116, score-0.117]
</p><p>70 Proposition 5 guarantees that IlGen,oo (x)1 has large expectation, though what we want in order to bound G is actually slightly stronger, namely that the random variable IlGen,oo (x)1 further be large/far from zero with high probability. [sent-118, score-0.067]
</p><p>71 One way of obtaining a loose bound is via the Chebyshev inequality. [sent-120, score-0.039]
</p><p>72 For the rest of this discussion, let us for simplicity implicitly condition on the event that a test example x has label T. [sent-121, score-0.095]
</p><p>73 The Chebyshev inequality implies that Pr[lGen ,oo(x) :S E[lGen ,oo(X)] - t] :S Var(lGen,oo(x))/t2 . [sent-122, score-0.03]
</p><p>74 So, if E[lGen,oo (x)ly = T] = an (as would be guaranteed by Proposition 5) for some a > 0, by setting t = (a - T)n, Chebyshev's inequality gives Pr[lGen,oo(x) :S Tn] :S O(l/(a - T)2n1/) (T < a), where 1} = 0 in the worst case, and 1} = 1 in the independent case. [sent-127, score-0.053]
</p><p>75 This thus gives a bound for G(T), but note that it will frequently be very loose. [sent-128, score-0.039]
</p><p>76 Indeed, in the unrealistic case in which the naive Bayes assumption really holds , we can obtain the much stronger (via the Chernoff bound) G(T):S exp(-O((a - T)2n)) , which is exponentially small in n. [sent-129, score-0.381]
</p><p>77 In the continuous case, if lGen,oo (x) has a density that, within some small interval [-m,mJ, is uniformly bounded by O(l/n), then we also have G(T) = O(T). [sent-130, score-0.109]
</p><p>78 Corollary 6 Let the conditions of Theorem 4 hold, and suppose that G(T) :S Eo/2+ F(T) for some function F(T) (independent of n) that satisfies F(T) -+ 0 as T -+ 0, and some fixed EO > O. [sent-132, score-0.024]
</p><p>79 Then for €(hGen) :S c(hGen,oo) + EO to hold with high  pima (continuous)  adult (continuous)  0. [sent-133, score-0.069]
</p><p>80 25 0"0  60  optdigits (O's and 1 's, continuous)  10  20  02Q- - -"""' 20- - -40-----"cJ "' 60  30  optdigits (2's and 3's, continuous)  ionosphere (continuous)  o. [sent-150, score-0.088]
</p><p>81 2  50  100  150  200  adult (discrete)  liver disorders (continuous)  0. [sent-159, score-0.035]
</p><p>82 150  00  20  40  60  80  Figure 1: Results of 15 experiments on datasets from the VCI Machine Learning repository. [sent-252, score-0.045]
</p><p>83 Dashed line is logistic regression; solid line is naive Bayes. [sent-255, score-0.589]
</p><p>84 This also means that either ofthese (the latter also requiring T) > 0) is a sufficient condition for the asymptotic sample complexity to be 0 (log n). [sent-258, score-0.204]
</p><p>85 4  Experiments  The results of the previous section imply that even though the discriminative logistic regression algorithm has a lower asymptotic error, the generative naive Bayes classifier may also converge more quickly to its (higher) asymptotic error. [sent-259, score-1.566]
</p><p>86 Thus, as the number of training examples m is increased, one would expect generative naive Bayes to initially do better, but for discriminative logistic regression to eventually catch up to, and quite likely overtake, the performance of naive Bayes. [sent-260, score-1.53]
</p><p>87 To test these predictions, we performed experiments on 15 datasets, 8 with continuous inputs, 7 with discrete inputs, from the VCI Machine Learning repository. [sent-261, score-0.151]
</p><p>88 2 The results ofthese experiments are shown in Figure 1. [sent-262, score-0.044]
</p><p>89 We find that the theoretical predictions are borne out surprisingly well. [sent-263, score-0.073]
</p><p>90 5  Discussion  Efron [2] also analyzed logistic regression and Normal Discriminant Analysis (for continuous inputs) , and concluded that the former was only asymptotically very slightly (1/3- 1/2 times) less statistically efficient. [sent-265, score-0.5]
</p><p>91 In this setting, the estimated covariance matrix is singular if we have fewer than linear in n training examples, so it is no surprise that Normal Discriminant Analysis cannot learn much faster than logistic regression here. [sent-267, score-0.472]
</p><p>92 A second important difference is that Efron considered only the special case in which the P(xly) is truly Gaussian. [sent-268, score-0.026]
</p><p>93 Such an asymptotic comparison is not very useful in the general case, since the only possible conclusion, if €(hDis,oo) < €(hGen,oo), is that logistic regression is the superior algorithm. [sent-269, score-0.551]
</p><p>94 Train/test splits were random subject to there being at least one example of each class in the training set, and continuous-valued inputs were also rescaled to [0 , 1] if necessary. [sent-272, score-0.175]
</p><p>95 In the case of linearly separable datasets, logistic regression makes no distinction between the many possible separating planes. [sent-273, score-0.449]
</p><p>96 In this setting we used an MCMC sampler to pick a classifier randomly from them (i. [sent-274, score-0.224]
</p><p>97 Our implementation of Normal Discriminant Analysis also used the (standard) trick of adding € to the diagonal of the covariance matrix to ensure invertibility, and for naive Bayes we used I = 1. [sent-277, score-0.334]
</p><p>98 such as shrinking the parameters via an L1 constraint, imposing a margin constraint in the separable case, or various forms of averaging. [sent-278, score-0.034]
</p><p>99 By developing a clearer understanding of the conditions under which pure generative and discriminative approaches are most successful, we should be better able to design hybrid classifiers that enjoy the best properties of either across a wider range of conditions. [sent-280, score-0.535]
</p><p>100 The efficiency of logistic regression compared to Normal Discriminant Analysis. [sent-293, score-0.415]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hgen', 0.437), ('lgen', 0.349), ('naive', 0.312), ('logistic', 0.277), ('discriminative', 0.235), ('classifier', 0.166), ('hois', 0.153), ('xly', 0.152), ('ly', 0.152), ('bayes', 0.142), ('classifiers', 0.138), ('regression', 0.138), ('generative', 0.138), ('asymptotic', 0.136), ('proposition', 0.131), ('ijj', 0.109), ('po', 0.104), ('discriminant', 0.101), ('fit', 0.096), ('inputs', 0.093), ('xily', 0.087), ('continuous', 0.085), ('dis', 0.069), ('ylx', 0.069), ('discrete', 0.066), ('xiiy', 0.065), ('eo', 0.064), ('vc', 0.062), ('efron', 0.057), ('chebyshev', 0.057), ('xi', 0.057), ('oo', 0.048), ('suffices', 0.048), ('chance', 0.048), ('iy', 0.046), ('datasets', 0.045), ('berkeley', 0.045), ('examples', 0.045), ('hdis', 0.044), ('ilgen', 0.044), ('ofthese', 0.044), ('optdigits', 0.044), ('chernoff', 0.043), ('var', 0.043), ('tn', 0.041), ('log', 0.041), ('normal', 0.04), ('bound', 0.039), ('corollary', 0.038), ('error', 0.038), ('borne', 0.038), ('logn', 0.038), ('catch', 0.038), ('classification', 0.038), ('pr', 0.037), ('fa', 0.037), ('label', 0.037), ('family', 0.036), ('training', 0.035), ('perturbation', 0.035), ('adult', 0.035), ('prevailing', 0.035), ('predictions', 0.035), ('hold', 0.034), ('fraction', 0.034), ('separable', 0.034), ('indeed', 0.032), ('vci', 0.032), ('let', 0.031), ('regimes', 0.03), ('implies', 0.03), ('pick', 0.03), ('logarithmic', 0.029), ('converges', 0.029), ('though', 0.028), ('setting', 0.028), ('thresholding', 0.028), ('event', 0.027), ('truly', 0.026), ('holds', 0.026), ('splits', 0.025), ('worst', 0.025), ('ng', 0.025), ('analyses', 0.025), ('fixed', 0.024), ('similarly', 0.024), ('bounded', 0.024), ('either', 0.024), ('sketch', 0.024), ('long', 0.023), ('seems', 0.022), ('least', 0.022), ('covariance', 0.022), ('stronger', 0.022), ('ti', 0.022), ('increased', 0.022), ('largely', 0.021), ('assumption', 0.021), ('andrew', 0.021), ('analog', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="133-tfidf-1" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster. 1</p><p>2 0.12574734 <a title="133-tfidf-2" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>3 0.11038642 <a title="133-tfidf-3" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>4 0.10735427 <a title="133-tfidf-4" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>Author: Polina Golland</p><p>Abstract: In many scientiﬁc and engineering applications, detecting and understanding differences between two groups of examples can be reduced to a classical problem of training a classiﬁer for labeling new examples while making as few mistakes as possible. In the traditional classiﬁcation setting, the resulting classiﬁer is rarely analyzed in terms of the properties of the input data captured by the discriminative model. However, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the statistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we deﬁne a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classiﬁer function. We derive the discriminative direction for kernel-based classiﬁers, demonstrate the technique on several examples and brieﬂy discuss its use in the statistical shape analysis, an application that originally motivated this work.</p><p>5 0.096953481 <a title="133-tfidf-5" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>Author: Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We investigate a learning algorithm for the classiﬁcation of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classiﬁers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm—its guarantee of monotonic improvement, and its absence of tuning parameters—with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classiﬁcation than comparably sized models trained by EM. 1</p><p>6 0.093199342 <a title="133-tfidf-6" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>7 0.087107129 <a title="133-tfidf-7" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>8 0.084144339 <a title="133-tfidf-8" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>9 0.078541346 <a title="133-tfidf-9" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>10 0.073578224 <a title="133-tfidf-10" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>11 0.071796104 <a title="133-tfidf-11" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>12 0.070626147 <a title="133-tfidf-12" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>13 0.069433987 <a title="133-tfidf-13" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>14 0.066213645 <a title="133-tfidf-14" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>15 0.066137068 <a title="133-tfidf-15" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>16 0.065387644 <a title="133-tfidf-16" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>17 0.064513981 <a title="133-tfidf-17" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>18 0.063134052 <a title="133-tfidf-18" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>19 0.062384132 <a title="133-tfidf-19" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>20 0.060274128 <a title="133-tfidf-20" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.193), (1, 0.081), (2, -0.001), (3, 0.06), (4, 0.028), (5, -0.037), (6, 0.065), (7, -0.034), (8, -0.029), (9, -0.017), (10, 0.054), (11, 0.086), (12, 0.018), (13, -0.01), (14, 0.03), (15, -0.017), (16, -0.0), (17, -0.053), (18, 0.057), (19, 0.024), (20, 0.032), (21, 0.083), (22, -0.102), (23, 0.026), (24, -0.003), (25, -0.002), (26, -0.059), (27, 0.096), (28, -0.133), (29, 0.142), (30, -0.019), (31, -0.004), (32, -0.035), (33, -0.231), (34, 0.087), (35, 0.047), (36, 0.041), (37, -0.016), (38, -0.109), (39, -0.01), (40, -0.045), (41, 0.106), (42, -0.07), (43, 0.001), (44, -0.109), (45, 0.13), (46, -0.028), (47, -0.056), (48, 0.017), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94427347 <a title="133-lsi-1" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster. 1</p><p>2 0.52266407 <a title="133-lsi-2" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>3 0.49013141 <a title="133-lsi-3" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>4 0.4897505 <a title="133-lsi-4" href="./nips-2001-EM-DD%3A_An_Improved_Multiple-Instance_Learning_Technique.html">64 nips-2001-EM-DD: An Improved Multiple-Instance Learning Technique</a></p>
<p>Author: Qi Zhang, Sally A. Goldman</p><p>Abstract: We present a new multiple-inst ance (MI) learning technique (EMDD) that combines EM with the diverse density (DD) algorithm. EM-DD is a general-purpose MI algorithm that can be applied with boolean or real-value labels and makes real-value predictions. On the boolean Musk benchmarks, the EM-DD algorithm without any tuning significantly outperforms all previous algorithms. EM-DD is relatively insensitive to the number of relevant attributes in the data set and scales up well to large bag sizes. Furthermore, EMDD provides a new framework for MI learning, in which the MI problem is converted to a single-instance setting by using EM to estimate the instance responsible for the label of the bag. 1</p><p>5 0.46998638 <a title="133-lsi-5" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>Author: Ralf Herbrich, Robert C. Williamson</p><p>Abstract: In contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specific learning algorithm used. Motivated by the luckiness framework [8] we are also able to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits both the margin and the distribution of the data in feature space. 1</p><p>6 0.46760154 <a title="133-lsi-6" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>7 0.4539392 <a title="133-lsi-7" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>8 0.44674116 <a title="133-lsi-8" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>9 0.44296256 <a title="133-lsi-9" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>10 0.42391741 <a title="133-lsi-10" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>11 0.42233565 <a title="133-lsi-11" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>12 0.40622902 <a title="133-lsi-12" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>13 0.40335751 <a title="133-lsi-13" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>14 0.400103 <a title="133-lsi-14" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>15 0.39984411 <a title="133-lsi-15" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>16 0.38827276 <a title="133-lsi-16" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>17 0.37658313 <a title="133-lsi-17" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>18 0.36529395 <a title="133-lsi-18" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>19 0.35863572 <a title="133-lsi-19" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>20 0.35585371 <a title="133-lsi-20" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.033), (17, 0.027), (19, 0.019), (27, 0.597), (30, 0.03), (36, 0.014), (38, 0.013), (59, 0.02), (72, 0.042), (79, 0.025), (83, 0.011), (91, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99541533 <a title="133-lda-1" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>2 0.99132353 <a title="133-lda-2" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>Author: Magnus Rattray, Gleb Basalyga</p><p>Abstract: We study the dynamics of a Hebbian ICA algorithm extracting a single non-Gaussian component from a high-dimensional Gaussian background. For both on-line and batch learning we ﬁnd that a surprisingly large number of examples are required to avoid trapping in a sub-optimal state close to the initial conditions. To extract a skewed signal at least examples are required for -dimensional data and examples are required to extract a symmetrical signal with non-zero kurtosis. § ¡ ©£¢  £ §¥ ¡ ¨¦¤£¢</p><p>same-paper 3 0.98960656 <a title="133-lda-3" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster. 1</p><p>4 0.98936963 <a title="133-lda-4" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>Author: Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We investigate a learning algorithm for the classiﬁcation of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classiﬁers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm—its guarantee of monotonic improvement, and its absence of tuning parameters—with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classiﬁcation than comparably sized models trained by EM. 1</p><p>5 0.98816293 <a title="133-lda-5" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi</p><p>Abstract: Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami op erator on a manifold , and the connections to the heat equation , we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered. In many areas of artificial intelligence, information retrieval and data mining, one is often confronted with intrinsically low dimensional data lying in a very high dimensional space. For example, gray scale n x n images of a fixed object taken with a moving camera yield data points in rn: n2 . However , the intrinsic dimensionality of the space of all images of t he same object is the number of degrees of freedom of the camera - in fact the space has the natural structure of a manifold embedded in rn: n2 . While there is a large body of work on dimensionality reduction in general, most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside. Recently, there has been some interest (Tenenbaum et aI, 2000 ; Roweis and Saul, 2000) in the problem of developing low dimensional representations of data in this particular context. In this paper , we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction. The core algorithm is very simple, has a few local computations and one sparse eigenvalu e problem. The solution reflects th e intrinsic geom etric structure of the manifold. Th e justification comes from the role of the Laplacian op erator in providing an optimal emb edding. Th e Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace-Beltrami operator defined on the manifold. The emb edding maps for the data come from approximations to a natural map that is defined on the entire manifold. The framework of analysis presented here makes this connection explicit. While this connection is known to geometers and specialists in sp ectral graph theory (for example , see [1, 2]) to the best of our knowledge we do not know of any application to data representation yet. The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner. The locality preserving character of the Laplacian Eigenmap algorithm makes it relatively insensitive to outliers and noise. A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data. Connections to spectral clustering algorithms developed in learning and computer vision (see Shi and Malik , 1997) become very clear. Following the discussion of Roweis and Saul (2000) , and Tenenbaum et al (2000), we note that the biological perceptual apparatus is confronted with high dimensional stimuli from which it must recover low dimensional structure. One might argue that if the approach to recovering such low-dimensional structure is inherently local , then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception. 1 The Algorithm Given k points Xl , ... , Xk in ]]{ I, we construct a weighted graph with k nodes, one for each point , and the set of edges connecting neighboring points to each other. 1. Step 1. [Constru cting th e Graph] We put an edge between nodes i and j if Xi and Xj are</p><p>6 0.98425418 <a title="133-lda-6" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>7 0.97687352 <a title="133-lda-7" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>8 0.89888912 <a title="133-lda-8" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>9 0.87160993 <a title="133-lda-9" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>10 0.8417272 <a title="133-lda-10" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>11 0.78801841 <a title="133-lda-11" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>12 0.78655881 <a title="133-lda-12" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>13 0.78562909 <a title="133-lda-13" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>14 0.78491336 <a title="133-lda-14" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>15 0.77671033 <a title="133-lda-15" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>16 0.77516973 <a title="133-lda-16" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>17 0.77087873 <a title="133-lda-17" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>18 0.76592481 <a title="133-lda-18" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>19 0.76483691 <a title="133-lda-19" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>20 0.76315033 <a title="133-lda-20" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
