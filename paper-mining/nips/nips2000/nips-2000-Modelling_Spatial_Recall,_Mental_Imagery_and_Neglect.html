<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-87" href="#">nips2000-87</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</h1>
<br/><p>Source: <a title="nips-2000-87-pdf" href="http://papers.nips.cc/paper/1916-modelling-spatial-recall-mental-imagery-and-neglect.pdf">pdf</a></p><p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>Reference: <a title="nips-2000-87-reference" href="../nips2000_reference/nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Modelling spatial recall, mental imagery and neglect  Suzanna Becker Department of Psychology McMaster University 1280 Main Street West Hamilton,Ont. [sent-1, score-0.389]
</p><p>2 uk  Abstract We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. [sent-6, score-0.723]
</p><p>3 Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. [sent-7, score-0.418]
</p><p>4 Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. [sent-8, score-0.549]
</p><p>5 The model thereby simulates recall and imagery of locations and objects in complex environments. [sent-9, score-0.327]
</p><p>6 After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. [sent-10, score-0.82]
</p><p>7 Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients. [sent-11, score-0.736]
</p><p>8 These representations, and the ability to translate between them, have been accounted for in several computational models of the parietal cortex e. [sent-14, score-0.513]
</p><p>9 In other situations such as route planning, recall and imagery for scenes or events one must also reply upon representations of spatial layouts from long-term memory. [sent-17, score-0.383]
</p><p>10 Neuropsychological and neuroimaging studies implicate both the parietal and hippocampal regions in such tasks [4, 5], with the long-term memory component associated with the hippocampus. [sent-18, score-0.664]
</p><p>11 The discovery of "place cells" in the hippocampus [6] provides evidence that hippocampal representations are ailocentric, in that absolute locations in open spaces are encoded irrespective of viewing direction. [sent-19, score-0.418]
</p><p>12 This paper addresses the nature and source of the spatial representations in the hippocampal and parietal regions, and how they interact during recall and navigation. [sent-20, score-0.835]
</p><p>13 We assume that in the hippocampus proper, long-term spatial memories are stored allocentrically, whereas in the parietal cortex view-based images are created on-the-fly during perception or recall. [sent-21, score-0.697]
</p><p>14 Intuitively it makes sense to use an allocentric representation for long-term storage as the  position of the body will have changed before recall. [sent-22, score-0.318]
</p><p>15 reach with the hand) or to imagine a scene, an egocentric representation (e. [sent-25, score-0.429]
</p><p>16 A study of hemispatial neglect patients throws some light on the interaction of long-term memory with mental imagery. [sent-28, score-0.325]
</p><p>17 Bisiach and Luzatti [1] asked two patients to recall the buildings from the familiar Cathedral Square in Milan, after being asked to imagine (i) facing the cathedral, and (ii) facing in the opposite direction. [sent-29, score-0.389]
</p><p>18 Both patients, in both (i) and (ii), predominantly recalled buildings that would have appeared on their right from the specified viewpoint. [sent-30, score-0.235]
</p><p>19 Since the buildings recalled in (i) were located physically on the opposite side of the square to those recalled in (ii), the patients' long-term memory for all of the buildings in the square was apparently intact. [sent-31, score-0.597]
</p><p>20 Further, the area neglected rotated according to the patient's imagined viewpoint, suggesting that their impairment relates to the generation of egocentric mental images from a non-egocentric long-term store. [sent-32, score-0.526]
</p><p>21 Object information from the ventral visual processing stream enters the hippocampal formation (medial entorhinal cortex) via the perirhinal cortex, while vi suo spatial information from the dorsal pathways enters lateral entorhinal cortex primarily via the parahippocampal cortex [9]. [sent-36, score-0.962]
</p><p>22 We extend the O'Keefe & Burgess [10] hippocampal model to include object-place associations by encoding object features in perirhinal cortex (we refer to these features as texture, but they could also be attributes such as colour, shape or size). [sent-37, score-0.571]
</p><p>23 Reciprocal connections to the parahippocampus allow object features to cue the hippocampus to activate a remembered location in an environment, and conversely, a remembered location can be used to reactivate the feature information of objects at that location. [sent-38, score-0.93]
</p><p>24 The connections from parietal to parahippocampal areas allow the remembered location to be specified in egocentric imagery. [sent-39, score-1.265]
</p><p>25 parietal ego <·>allo /ransla/um Medial parietal egocentric locations  "'-~~~-:. [sent-41, score-1.319]
</p><p>26 Note the allocentric encoding of direction (NSEW) in parahippocampus, and the egocentric encoding of directions (LR) in medial parietal cortex. [sent-60, score-1.274]
</p><p>27 An allocentric representation of object location is extracted from the ventral visual stream in the parahippocampus, and feeds into the hippocampus. [sent-62, score-0.501]
</p><p>28 The dorsal visual stream provides an egocentric representation of object location in medial parietal areas and makes bi-directional contact with the  parahippocampus via posterior parietal area 7a. [sent-63, score-1.741]
</p><p>29 Inputs carrying allocentric heading direction information [11] project to both parietal and parahippocampal regions, allowing bidirectional translation from allocentric to egocentric directions. [sent-64, score-1.745]
</p><p>30 Recurrent connections in the hippocampus allow recall from long-term memory via the parahippocampus, and egocentric imagery in the medial parietal areas. [sent-65, score-1.356]
</p><p>31 The hippocampal formation (HF) consists of several regions - the entorhinal cortex, dentate gyrus, CA3, and CAl, each of which appears to code for space with varying degrees of sparseness. [sent-69, score-0.285]
</p><p>32 To simplify, in our model the HF is represented by a single layer of "place cells", each tuned to random, fixed configurations of spatial features as in [10, 12]. [sent-70, score-0.265]
</p><p>33 It receives these inputs from the parahippocampal cortex (PH) and perirhinal cortex (PR), respectively. [sent-72, score-0.496]
</p><p>34 The parahippocampal representation of object locations is simulated as a layer of neurons, each of which is tuned to respond whenever there is a landmark at a given distance and allocentric direction from the subject. [sent-73, score-0.996]
</p><p>35 Projections from this representation into the hippocampus drive the firing of place cells. [sent-74, score-0.27]
</p><p>36 This representation has been shown to account for the properties of place cells recorded across environments of varying shape and size [10, 12]. [sent-75, score-0.269]
</p><p>37 Recurrent connections between place cells allow subsequent pattern completion in the place cell layer. [sent-76, score-0.528]
</p><p>38 Return projections from the place cells to the parahippocampus allow reactivation of all landmark location information consistent with the current location. [sent-77, score-0.528]
</p><p>39 The perirhinal representation in our model consists of a layer of neurons, each tuned to a particular textural feature. [sent-78, score-0.331]
</p><p>40 Thus, in our model, object features can be used to cue the hippocampal system to activate a remembered location in an environment, and conversely, a remembered location can activate all associated object textures. [sent-80, score-0.935]
</p><p>41 Further, each allocentric spatial feature unit in the parahippocampus projects to the perirhinal object feature units so that attention to one location can activate a particular object's features. [sent-81, score-0.884]
</p><p>42 2  Parietal cortex  Neurons responding to specific egocentric stimulus locations (e. [sent-83, score-0.563]
</p><p>43 relative to the eye, head or hand) have been recorded in several parietal areas. [sent-85, score-0.511]
</p><p>44 Tasks involving imagery of the products of retrieval tend to activate medial parietal areas (precuneus, posterior cingulate, retrosplenial cortex) in neuroimaging studies [14]. [sent-86, score-0.741]
</p><p>45 We hypothesize that there is a medial parietal egocentric map of space, coding for the locations of objects organised by distance and angle from the body midline. [sent-87, score-1.155]
</p><p>46 In this representation cells are tuned to respond to the presence of an object at a specific distance in a specific egocentric direction. [sent-88, score-0.71]
</p><p>47 Cells have also been reported in posterior parietal areas with egocentrically tuned responses that are modulated by variables such as eye position [15] or body orientation (in area 7a [16]). [sent-89, score-0.496]
</p><p>48 We hypothesize that area 7a performs the translation between allocentric and egocentric representations so that, as well as being driven directly by perception, the medial parietal egocentric map can be driven by recalled allocentric parahippocampal representations. [sent-91, score-2.257]
</p><p>49 We consider simply translation between allocentric and view-dependent representations, requiring a modulatory input from the head direction system. [sent-92, score-0.445]
</p><p>50 A more detailed model would include translations between allocentric and body, head and eye centered representations, and possibly use of retrosplenial areas to buffer these intermediate representations [18]. [sent-93, score-0.447]
</p><p>51 The translation between parahippocampal and parietal representations occurs via a hardwired mapping of each to an expanded set of egocentric representations, each modulated  by head direction so that one is fully activated for each (coarse coded) head direction (see Figure 1). [sent-94, score-1.488]
</p><p>52 With activation from the appropriate head direction unit, activation from the parahippocampal or parietal representation can activate the appropriate cell in the other representation via this expanded representation. [sent-95, score-1.124]
</p><p>53 3  Simulation details  The hippocampal component of the model was trained on the spatial environment shown in the top-left panel of Figure 2, representing the buildings of the Milan square. [sent-97, score-0.431]
</p><p>54 The HF place cells were preassigned to cover a grid of locations in the environment, with each cell's activation falling off as a Gaussian of the distance to its preferred location. [sent-102, score-0.431]
</p><p>55 The weights to the perirhinal (PR) object feature units - on the HF-to-PR and PH-to-PR connections - were trained by simulating sequential attention to each visible object, from each training location. [sent-104, score-0.367]
</p><p>56 Thus, a single object's textural features in the PR layer were associated with the corresponding PH location features and HF place cell activations via Hebbian learning. [sent-105, score-0.639]
</p><p>57 The PR-to-HF weights were trained to associate each training location with the single predominant texture - either that of a nearby object or that of the background. [sent-106, score-0.329]
</p><p>58 The connections to and within the parietal component of the model were hard-wired to implement the bidirectional allocentric-egocentric mappings (these are functionally equivalent to a rotation by adding or subtracting the heading angle). [sent-107, score-0.605]
</p><p>59 The 2-layer parietal circuit in Figure 1 essentially encodes separate transformation matrices for each of a discrete set of head directions in the first layer. [sent-108, score-0.552]
</p><p>60 A right parietal lesion causing left neglect was simulated with graded, random knockout to units in the egocentric map of the left side of space. [sent-109, score-1.033]
</p><p>61 In simulation 1, the model was required to recall the allocentric representation of the Milan square after being cued with the texture and direction (()j) of each of the visible buildings in turn, at a short distance rj. [sent-114, score-0.941]
</p><p>62 The initial input to the HF, [HF (t = 0), was the sum of an externally provided texture cue from the PR cell layer, and a distance and direction cue from the PH cell layer obtained by initializing the PH states using equation 1, with rj = 2. [sent-115, score-0.577]
</p><p>63 A place was then recalled by repeatedly updating the HF cells' states until convergence according to:  IHF (t) AfF(t)  = =  . [sent-116, score-0.248]
</p><p>64 First, the PH cells and HF place cells were initialized to the states of the retrieved spatial location (obtained after settling in simulation 1). [sent-128, score-0.552]
</p><p>65 The model was then asked what it "saw" in various directions by simulating focused attention on the egocentric map, and requiring the model to retrieve the object texture at that location via activation of the PR region. [sent-129, score-0.904]
</p><p>66 The egocentric medial parietal (MP) activation was calculated from the PH-to-MP mapping, as described above. [sent-130, score-0.958]
</p><p>67 Attention to a queried egocentric direction was simulated by modulating the pattern of activation across the MP layer with a Gaussian filter centered on that location. [sent-131, score-0.627]
</p><p>68 4 Results and discussion In simulation 1, when cued with the textures of each of the 5 buildings around the training region, the model settled on an appropriate place cell activation. [sent-133, score-0.477]
</p><p>69 The model was cued with the texture of the cathedral front, and settled to a place representation near to its southwest corner. [sent-135, score-0.432]
</p><p>70 The resulting PH layer activations show correct recall of the locations of the other landmarks around the square. [sent-136, score-0.308]
</p><p>71 In simulation 2, shown in the lower panel, the model rotated the PH map according to the cued heading direction, and was able to retrieve correctly the texture of each building when queried with its egocentric direction. [sent-137, score-0.868]
</p><p>72 In the lesioned model, buildings to the egocentric left were usually not identified correctly. [sent-138, score-0.554]
</p><p>73 The building to the left has texture 5, and the building to the right has texture 7. [sent-141, score-0.332]
</p><p>74 After a simulated parietal lesion, the model neglects building 5. [sent-142, score-0.503]
</p><p>75 3 Predictions and future directions We have demonstrated how egocentric spatial representations may be formed from allocentric ones and vice versa. [sent-143, score-0.856]
</p><p>76 The entorhinal cortex (EC) is the major cortical input zone to the hippocampus, and both the parahippocampal and perirhinal regions project to it [13]. [sent-145, score-0.492]
</p><p>77 Single cell recordings in EC indicate tuning curves that are broadly similar to those of place cells, but are much more coarsely tuned and less specific to individual episodes [21, 9] . [sent-146, score-0.26]
</p><p>78 Additionally, EC cells can hold state information, such as a spatial location or object identity, over long time delays and even across intervening items [9]. [sent-147, score-0.398]
</p><p>79 An allocentric representation could emerge if the EC is under pressure to use a more compressed, temporally stable code to reconstruct the rapidly changing visuospatial input. [sent-148, score-0.316]
</p><p>80 An egocentric map is altered dramatically after changes in viewpoint, whereas an allocentric map is not. [sent-149, score-0.731]
</p><p>81 Thus, the PH and hippocampal representations could evolve via an unsupervised learning procedure that discovers a temporally stable, generative model of the parietal input. [sent-150, score-0.703]
</p><p>82 The inverse mapping from allocentric PH features to egocentric parietal features could be learned by training the back-projections similarly. [sent-151, score-1.142]
</p><p>83 But how could the egocentric map in the parietal region be learned in the first place? [sent-152, score-0.86]
</p><p>84 We note that our parietal imagery system might also support the short-term visuospatial working memory required in more perceptual tasks (e. [sent-154, score-0.631]
</p><p>85 In our model, sparse random connections from the object layer to the place layer ensure a high degree of initial place-tuning that should generalize across similar environments. [sent-163, score-0.474]
</p><p>86 Plasticity in the HF-PR connections will allow unique textures of walls, buildings etc to be associated with particular places; thus after extensive exposure, environment-specific place firing patterns should emerge. [sent-164, score-0.353]
</p><p>87 A selective lesion to the parahippocampus should abolish the ability to make allocentric object-place associations altogether, thereby severely disrupting both landmark-based and memory-based navigation. [sent-165, score-0.413]
</p><p>88 In contrast, a pure hippocampal lesion would spare the ability to represent a single object's distance and allocentric directions from a location, so navigation based on a single landmark should be spared. [sent-166, score-0.584]
</p><p>89 If an arrangement of objects is viewed in a 3-D environment, the recall or recognition of the arrangement from a new viewpoint will be facilitated by having formed an allocentric representation of their locations. [sent-167, score-0.425]
</p><p>90 PR activations - Control  f':L o  o  00  0  5 10 Texture neuron  ·  MP activns with neglect PR activations - Lesioned  0. [sent-175, score-0.24]
</p><p>91 Middle: HF place cell activations, after being cued that building #1 is nearby and to the north. [sent-179, score-0.361]
</p><p>92 Place cells are arranged in a polar coordinate grid according to the distance and direction of their preferred locations relative to the centre of the environment (bright white spot). [sent-180, score-0.357]
</p><p>93 Right: PH inputs to place cell layer are plotted in polar coordinates, representing the recalled distances and directions of visible edges associated with the maximally activated location. [sent-183, score-0.505]
</p><p>94 The externally cued heading direction is also shown here. [sent-184, score-0.301]
</p><p>95 Left: An imagined view in the egocentric map layer (MP), given that the heading direction is south; the visible edges shown above have been rotated by 180 degrees. [sent-187, score-0.836]
</p><p>96 Mid-left: the recalled texture features in the PR layer are plotted in two different conditions, simulating attention to the right (circles) and left (stars). [sent-188, score-0.377]
</p><p>97 Mid-right and right: Similarly, the MP and PR activations are shown after damage to the left side of the egocentric map. [sent-189, score-0.493]
</p><p>98 If the stimulus is evoking erroneous vestibular or somatosensory inputs to shift the perceived head direction system leftward, then all objects will now be mapped further rightward in egocentric space and into the 'good side' of the parietal map in a lesioned model. [sent-191, score-1.13]
</p><p>99 O' Keefe, editors, The hippocampal and parietal foundations of spatial cognition. [sent-263, score-0.684]
</p><p>100 O'Keefe, editors, The hippocampal and parietal foundations of spatial cognition. [sent-404, score-0.684]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parietal', 0.423), ('egocentric', 0.389), ('allocentric', 0.246), ('ph', 0.207), ('parahippocampal', 0.196), ('burgess', 0.166), ('hippocampal', 0.164), ('hf', 0.153), ('place', 0.143), ('imagery', 0.135), ('buildings', 0.13), ('parahippocampus', 0.12), ('perirhinal', 0.12), ('texture', 0.114), ('object', 0.113), ('milan', 0.105), ('recalled', 0.105), ('heading', 0.104), ('location', 0.102), ('neglect', 0.098), ('medial', 0.098), ('spatial', 0.097), ('cortex', 0.09), ('cued', 0.09), ('pr', 0.088), ('head', 0.088), ('hippocampus', 0.087), ('cells', 0.086), ('layer', 0.085), ('locations', 0.084), ('representations', 0.083), ('direction', 0.077), ('cell', 0.076), ('remembered', 0.075), ('activations', 0.071), ('recall', 0.068), ('mp', 0.065), ('patients', 0.065), ('hemispatial', 0.06), ('jeffery', 0.06), ('mental', 0.059), ('activate', 0.055), ('visible', 0.055), ('building', 0.052), ('entorhinal', 0.052), ('activation', 0.048), ('connections', 0.048), ('map', 0.048), ('lesion', 0.047), ('ahf', 0.045), ('cathedral', 0.045), ('imagined', 0.045), ('landmark', 0.045), ('maguire', 0.045), ('textural', 0.045), ('memory', 0.043), ('square', 0.042), ('features', 0.042), ('tuned', 0.041), ('directions', 0.041), ('distance', 0.041), ('environment', 0.04), ('objects', 0.04), ('representation', 0.04), ('cue', 0.039), ('simulation', 0.038), ('lesioned', 0.035), ('ec', 0.035), ('formation', 0.035), ('translation', 0.034), ('regions', 0.034), ('via', 0.033), ('rotated', 0.033), ('damage', 0.033), ('asked', 0.033), ('body', 0.032), ('allow', 0.032), ('attention', 0.031), ('viewpoint', 0.031), ('act', 0.03), ('ang', 0.03), ('becker', 0.03), ('bidirectional', 0.03), ('bisiach', 0.03), ('cacucd', 0.03), ('externally', 0.03), ('facing', 0.03), ('frackowiak', 0.03), ('frith', 0.03), ('guariglia', 0.03), ('ifr', 0.03), ('lever', 0.03), ('retrosplenial', 0.03), ('spiers', 0.03), ('suzuki', 0.03), ('udir', 0.03), ('vestibular', 0.03), ('visuospatial', 0.03), ('preferred', 0.029), ('simulated', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="87-tfidf-1" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>2 0.20998621 <a title="87-tfidf-2" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>3 0.18380463 <a title="87-tfidf-3" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>4 0.1118425 <a title="87-tfidf-4" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>5 0.086689278 <a title="87-tfidf-5" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>Author: Szabolcs KĂĄli, Peter Dayan</p><p>Abstract: In memory consolidation, declarative memories which initially require the hippocampus for their recall, ultimately become independent of it. Consolidation has been the focus of numerous experimental and qualitative modeling studies, but only little quantitative exploration. We present a consolidation model in which hierarchical connections in the cortex, that initially instantiate purely semantic information acquired through probabilistic unsupervised learning, come to instantiate episodic information as well. The hippocampus is responsible for helping complete partial input patterns before consolidation is complete, while also training the cortex to perform appropriate completion by itself.</p><p>6 0.079406798 <a title="87-tfidf-6" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>7 0.071448445 <a title="87-tfidf-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.070076481 <a title="87-tfidf-8" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>9 0.051919263 <a title="87-tfidf-9" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>10 0.05174144 <a title="87-tfidf-10" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>11 0.051495064 <a title="87-tfidf-11" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>12 0.048885219 <a title="87-tfidf-12" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>13 0.046078157 <a title="87-tfidf-13" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>14 0.045343708 <a title="87-tfidf-14" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>15 0.043858491 <a title="87-tfidf-15" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>16 0.041585799 <a title="87-tfidf-16" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>17 0.040757582 <a title="87-tfidf-17" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>18 0.040354367 <a title="87-tfidf-18" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>19 0.039298974 <a title="87-tfidf-19" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>20 0.038925767 <a title="87-tfidf-20" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.135), (1, -0.138), (2, -0.062), (3, 0.022), (4, -0.063), (5, 0.068), (6, 0.186), (7, -0.134), (8, 0.266), (9, -0.029), (10, 0.031), (11, 0.12), (12, -0.064), (13, 0.001), (14, 0.048), (15, 0.025), (16, 0.09), (17, -0.029), (18, 0.11), (19, 0.087), (20, -0.095), (21, 0.032), (22, 0.016), (23, 0.121), (24, -0.213), (25, -0.084), (26, -0.047), (27, -0.021), (28, -0.114), (29, 0.099), (30, -0.068), (31, -0.127), (32, 0.023), (33, 0.112), (34, 0.122), (35, 0.024), (36, 0.148), (37, 0.102), (38, 0.031), (39, -0.179), (40, -0.025), (41, -0.03), (42, 0.04), (43, 0.004), (44, -0.014), (45, -0.037), (46, -0.122), (47, 0.195), (48, -0.098), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97686529 <a title="87-lsi-1" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>2 0.75254267 <a title="87-lsi-2" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>3 0.71729058 <a title="87-lsi-3" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>4 0.43276086 <a title="87-lsi-4" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>5 0.40345648 <a title="87-lsi-5" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>Author: Szabolcs KĂĄli, Peter Dayan</p><p>Abstract: In memory consolidation, declarative memories which initially require the hippocampus for their recall, ultimately become independent of it. Consolidation has been the focus of numerous experimental and qualitative modeling studies, but only little quantitative exploration. We present a consolidation model in which hierarchical connections in the cortex, that initially instantiate purely semantic information acquired through probabilistic unsupervised learning, come to instantiate episodic information as well. The hippocampus is responsible for helping complete partial input patterns before consolidation is complete, while also training the cortex to perform appropriate completion by itself.</p><p>6 0.30240428 <a title="87-lsi-6" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>7 0.22867413 <a title="87-lsi-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.22345513 <a title="87-lsi-8" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>9 0.21412458 <a title="87-lsi-9" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>10 0.19490854 <a title="87-lsi-10" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>11 0.18048908 <a title="87-lsi-11" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>12 0.16232587 <a title="87-lsi-12" href="./nips-2000-The_Interplay_of_Symbolic_and_Subsymbolic_Processes_in_Anagram_Problem_Solving.html">132 nips-2000-The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving</a></p>
<p>13 0.16199858 <a title="87-lsi-13" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>14 0.15785493 <a title="87-lsi-14" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>15 0.15503946 <a title="87-lsi-15" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>16 0.14973482 <a title="87-lsi-16" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>17 0.14918429 <a title="87-lsi-17" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>18 0.14268152 <a title="87-lsi-18" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>19 0.13956524 <a title="87-lsi-19" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>20 0.13800129 <a title="87-lsi-20" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.016), (10, 0.022), (17, 0.108), (32, 0.015), (33, 0.02), (36, 0.018), (38, 0.055), (42, 0.018), (55, 0.045), (59, 0.307), (62, 0.025), (65, 0.022), (67, 0.045), (76, 0.019), (79, 0.014), (81, 0.045), (84, 0.037), (90, 0.015), (97, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87072176 <a title="87-lda-1" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>2 0.86205 <a title="87-lda-2" href="./nips-2000-A_Silicon_Primitive_for_Competitive_Learning.html">11 nips-2000-A Silicon Primitive for Competitive Learning</a></p>
<p>Author: David Hsu, Miguel Figueroa, Chris Diorio</p><p>Abstract: Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit performs a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task.</p><p>3 0.41959488 <a title="87-lda-3" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>4 0.38622597 <a title="87-lda-4" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>5 0.38575053 <a title="87-lda-5" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>Author: Carl Edward Rasmussen, Zoubin Ghahramani</p><p>Abstract: The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.</p><p>6 0.38244882 <a title="87-lda-6" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>7 0.38093019 <a title="87-lda-7" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>8 0.37989011 <a title="87-lda-8" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>9 0.37901407 <a title="87-lda-9" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>10 0.37760216 <a title="87-lda-10" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>11 0.3738192 <a title="87-lda-11" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>12 0.37250096 <a title="87-lda-12" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>13 0.3700299 <a title="87-lda-13" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>14 0.36902025 <a title="87-lda-14" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>15 0.36876836 <a title="87-lda-15" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>16 0.36563984 <a title="87-lda-16" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>17 0.36552265 <a title="87-lda-17" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>18 0.36449984 <a title="87-lda-18" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>19 0.36372843 <a title="87-lda-19" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>20 0.36221021 <a title="87-lda-20" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
