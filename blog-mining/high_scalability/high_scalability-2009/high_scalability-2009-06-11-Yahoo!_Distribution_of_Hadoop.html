<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-627" href="#">high_scalability-2009-627</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-627-html" href="http://highscalability.com//blog/2009/6/11/yahoo-distribution-of-hadoop.html">html</a></p><p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Many people in the Apache Hadoop community have asked Yahoo! [sent-1, score-0.295]
</p><p>2 to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. [sent-2, score-0.413]
</p><p>3 As a service to the Hadoop community, Yahoo is releasing the Yahoo! [sent-3, score-0.196]
</p><p>4 Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project. [sent-4, score-0.855]
</p><p>5 This source distribution includes code patches that they have added to improve the stability and performance of their clusters. [sent-5, score-1.206]
</p><p>6 In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop. [sent-6, score-0.818]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hadoop', 0.521), ('yahoo', 0.397), ('apache', 0.379), ('distribution', 0.325), ('patches', 0.321), ('releasing', 0.158), ('contributed', 0.158), ('community', 0.154), ('publish', 0.139), ('entirely', 0.114), ('code', 0.114), ('asked', 0.104), ('stability', 0.1), ('source', 0.092), ('includes', 0.09), ('release', 0.085), ('cases', 0.078), ('added', 0.071), ('version', 0.07), ('deploy', 0.069), ('improve', 0.067), ('yet', 0.064), ('test', 0.061), ('found', 0.059), ('already', 0.056), ('back', 0.05), ('available', 0.047), ('across', 0.04), ('service', 0.038), ('people', 0.037), ('based', 0.037), ('may', 0.037), ('large', 0.034), ('many', 0.028), ('get', 0.027), ('performance', 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="627-tfidf-1" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><p>2 0.25971073 <a title="627-tfidf-2" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>3 0.24616954 <a title="627-tfidf-3" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>4 0.24283022 <a title="627-tfidf-4" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>5 0.20452347 <a title="627-tfidf-5" href="../high_scalability-2009/high_scalability-2009-05-11-Facebook%2C_Hadoop%2C_and_Hive.html">596 high scalability-2009-05-11-Facebook, Hadoop, and Hive</a></p>
<p>Introduction: Facebook has the second largest installation of Hadoop (a software platform that lets one easily write and run applications that process vast amounts of data), Yahoo being the first.    Learn how they do it and what are the challenges on DBMS2 blog, which is a blog for people who care about database and analytic technologies.</p><p>6 0.19599724 <a title="627-tfidf-6" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>7 0.18907821 <a title="627-tfidf-7" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>8 0.18673001 <a title="627-tfidf-8" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>9 0.16812748 <a title="627-tfidf-9" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>10 0.15340465 <a title="627-tfidf-10" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>11 0.15019688 <a title="627-tfidf-11" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>12 0.14697352 <a title="627-tfidf-12" href="../high_scalability-2009/high_scalability-2009-06-04-New_Book%3A_Even_Faster_Web_Sites%3A_Performance_Best_Practices_for_Web_Developers.html">617 high scalability-2009-06-04-New Book: Even Faster Web Sites: Performance Best Practices for Web Developers</a></p>
<p>13 0.14659065 <a title="627-tfidf-13" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>14 0.13520555 <a title="627-tfidf-14" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>15 0.13115066 <a title="627-tfidf-15" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<p>16 0.13061926 <a title="627-tfidf-16" href="../high_scalability-2009/high_scalability-2009-07-29-Strategy%3A_Let_Google_and_Yahoo_Host_Your_Ajax_Library_-_For_Free.html">665 high scalability-2009-07-29-Strategy: Let Google and Yahoo Host Your Ajax Library - For Free</a></p>
<p>17 0.1298088 <a title="627-tfidf-17" href="../high_scalability-2009/high_scalability-2009-09-17-Hot_Links_for_2009-9-17_.html">707 high scalability-2009-09-17-Hot Links for 2009-9-17 </a></p>
<p>18 0.12692389 <a title="627-tfidf-18" href="../high_scalability-2011/high_scalability-2011-07-08-Stuff_The_Internet_Says_On_Scalability_For_July_8%2C_2011.html">1076 high scalability-2011-07-08-Stuff The Internet Says On Scalability For July 8, 2011</a></p>
<p>19 0.11918197 <a title="627-tfidf-19" href="../high_scalability-2011/high_scalability-2011-03-01-Sponsored_Post%3A__ScaleOut%2C_aiCache%2C_WAPT%2C_Karmasphere%2C_Kabam%2C_Opera_Solutions%2C_Newrelic%2C_Cloudkick%2C_Membase%2C_Joyent%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">997 high scalability-2011-03-01-Sponsored Post:  ScaleOut, aiCache, WAPT, Karmasphere, Kabam, Opera Solutions, Newrelic, Cloudkick, Membase, Joyent, CloudSigma, ManageEngine, Site24x7</a></p>
<p>20 0.11665212 <a title="627-tfidf-20" href="../high_scalability-2008/high_scalability-2008-02-11-Yahoo_Live%27s_Scaling_Problems_Prove%3A_Release_Early_and_Often_-_Just_Don%27t_Screw_Up.html">244 high scalability-2008-02-11-Yahoo Live's Scaling Problems Prove: Release Early and Often - Just Don't Screw Up</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.01), (2, 0.038), (3, -0.004), (4, 0.065), (5, 0.051), (6, 0.079), (7, 0.002), (8, 0.133), (9, 0.148), (10, 0.064), (11, -0.028), (12, 0.145), (13, -0.161), (14, 0.081), (15, -0.114), (16, 0.02), (17, -0.013), (18, -0.055), (19, 0.003), (20, -0.028), (21, 0.153), (22, 0.104), (23, 0.045), (24, -0.015), (25, 0.091), (26, 0.084), (27, -0.007), (28, 0.028), (29, 0.007), (30, 0.103), (31, 0.146), (32, -0.011), (33, 0.037), (34, 0.055), (35, 0.038), (36, -0.133), (37, 0.057), (38, -0.024), (39, -0.069), (40, -0.008), (41, 0.075), (42, -0.021), (43, -0.084), (44, 0.02), (45, 0.006), (46, 0.037), (47, 0.055), (48, -0.001), (49, 0.096)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99635118 <a title="627-lsi-1" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><p>2 0.91331804 <a title="627-lsi-2" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>Introduction: Yahoo has developed a new language called Pig Latin that fit in a sweet spot between high-level declarative querying in the spirit of SQL, and low-level, procedural programming `a la map-reduce and combines best of both worlds.  The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. Pig has just graduated from the Apache Incubator and joined Hadoop as a subproject.  The paper has a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly.  References:  Apache Pig Wiki</p><p>3 0.87816232 <a title="627-lsi-3" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>4 0.84058762 <a title="627-lsi-4" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>5 0.6969375 <a title="627-lsi-5" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>6 0.68210506 <a title="627-lsi-6" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>7 0.68017405 <a title="627-lsi-7" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>8 0.66783786 <a title="627-lsi-8" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>9 0.617024 <a title="627-lsi-9" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>10 0.60882854 <a title="627-lsi-10" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>11 0.60319239 <a title="627-lsi-11" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>12 0.59667331 <a title="627-lsi-12" href="../high_scalability-2009/high_scalability-2009-09-17-Hot_Links_for_2009-9-17_.html">707 high scalability-2009-09-17-Hot Links for 2009-9-17 </a></p>
<p>13 0.59271318 <a title="627-lsi-13" href="../high_scalability-2009/high_scalability-2009-07-02-Hypertable_is_a_New_BigTable_Clone_that_Runs_on_HDFS_or_KFS.html">647 high scalability-2009-07-02-Hypertable is a New BigTable Clone that Runs on HDFS or KFS</a></p>
<p>14 0.58675957 <a title="627-lsi-14" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>15 0.56186843 <a title="627-lsi-15" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>16 0.56079149 <a title="627-lsi-16" href="../high_scalability-2009/high_scalability-2009-05-11-Facebook%2C_Hadoop%2C_and_Hive.html">596 high scalability-2009-05-11-Facebook, Hadoop, and Hive</a></p>
<p>17 0.54935932 <a title="627-lsi-17" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>18 0.52967191 <a title="627-lsi-18" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>19 0.52112049 <a title="627-lsi-19" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>20 0.50206351 <a title="627-lsi-20" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.048), (2, 0.342), (10, 0.035), (61, 0.021), (79, 0.373)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99156249 <a title="627-lda-1" href="../high_scalability-2008/high_scalability-2008-01-06-Email_Architecture.html">202 high scalability-2008-01-06-Email Architecture</a></p>
<p>Introduction: I would like to know email architecture used by large ISPs.. or even used by google.      Can someone point me to some sites??       Thanks..</p><p>same-blog 2 0.98012418 <a title="627-lda-2" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><p>3 0.9658379 <a title="627-lda-3" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>Introduction: Submitted for your scaling pleasure: 
  
 Good idea:  Open The Index And Speed Up The Internet .  SmugMug estimates 50%  of their CPU is spent serving crawler robots. Having a common meta-data repository wouldn't prevent search engines from having their own special sauce. Then the problem becomes one of syncing data between repositories and processing change events. A generous soul could even offer a shared MapReduce service over the data. Now that would  speed up the internet . 
 Scaling Achievements:  YouTube Sees 3 Billion Views per Day ;  Twitter produces a sustained feed of 35 Mb per second ;  companies processing billions of APIs calls  (Twitter, Netflix, Amazon, NPR, Google, Facebook, eBay, Bing);  Astronomers Identify the Farthest Object Ever Observed, 13.14 Billion Light Years Away  
 Quotes that are Quotably Quotable:                        
 
  eekygeeky : When cloud computing news is slow? Switch to "big data"-100% of the vaguery, none of the used-up, mushy marketing feel!</p><p>4 0.95310819 <a title="627-lda-4" href="../high_scalability-2010/high_scalability-2010-07-27-YeSQL%3A_An_Overview_of_the_Various_Query_Semantics_in_the_Post_Only-SQL_World.html">867 high scalability-2010-07-27-YeSQL: An Overview of the Various Query Semantics in the Post Only-SQL World</a></p>
<p>Introduction: The NoSQL movement faults the SQL query language as the source of  many of the scalability issues that we face today with traditional  database approach.
 
I think that the main reason so many people  have come to see SQL as the source of all evil is the fact that,  traditionally, the query language was burned into the database  implementation. So by saying NoSQL you basically say "No" to the  traditional non-scalable RDBMS implementations.
 
This view has  brought on a flood of alternative query languages, each aiming to solve a  different aspect that is missing in the traditional SQL query approach,  such as a document model, or that provides a simpler approach, such as  Key/Value query.
 
Most of the people I speak with seem fairly confused on this subject,  and tend to use query semantics and architecture interchangeably. In  Part I  of this post i tried to provide quick overview of what  each query term stands for in the context of the NoSQL world .  Part II  illustrates those ide</p><p>5 0.95157516 <a title="627-lda-5" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>6 0.95027447 <a title="627-lda-6" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>7 0.94873822 <a title="627-lda-7" href="../high_scalability-2008/high_scalability-2008-05-19-Twitter_as_a_scalability_case_study.html">323 high scalability-2008-05-19-Twitter as a scalability case study</a></p>
<p>8 0.94693202 <a title="627-lda-8" href="../high_scalability-2013/high_scalability-2013-07-19-Stuff_The_Internet_Says_On_Scalability_For_July_19%2C_2013.html">1494 high scalability-2013-07-19-Stuff The Internet Says On Scalability For July 19, 2013</a></p>
<p>9 0.94685483 <a title="627-lda-9" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>10 0.94418752 <a title="627-lda-10" href="../high_scalability-2013/high_scalability-2013-02-08-Stuff_The_Internet_Says_On_Scalability_For_February_8%2C_2013.html">1403 high scalability-2013-02-08-Stuff The Internet Says On Scalability For February 8, 2013</a></p>
<p>11 0.9440583 <a title="627-lda-11" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>12 0.94278741 <a title="627-lda-12" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>13 0.93721664 <a title="627-lda-13" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>14 0.93599433 <a title="627-lda-14" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>15 0.93586695 <a title="627-lda-15" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>16 0.93501222 <a title="627-lda-16" href="../high_scalability-2009/high_scalability-2009-08-13-Reconnoiter_-_Large-Scale_Trending_and_Fault-Detection.html">680 high scalability-2009-08-13-Reconnoiter - Large-Scale Trending and Fault-Detection</a></p>
<p>17 0.93320107 <a title="627-lda-17" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>18 0.93160576 <a title="627-lda-18" href="../high_scalability-2010/high_scalability-2010-09-08-4_General_Core_Scalability_Patterns.html">897 high scalability-2010-09-08-4 General Core Scalability Patterns</a></p>
<p>19 0.93149543 <a title="627-lda-19" href="../high_scalability-2013/high_scalability-2013-07-01-PRISM%3A_The_Amazingly_Low_Cost_of_%C2%ADUsing_BigData_to_Know_More_About_You_in_Under_a_Minute.html">1485 high scalability-2013-07-01-PRISM: The Amazingly Low Cost of ­Using BigData to Know More About You in Under a Minute</a></p>
<p>20 0.93027145 <a title="627-lda-20" href="../high_scalability-2009/high_scalability-2009-03-05-Strategy%3A__In_Cloud_Computing_Systematically_Drive_Load_to_the_CPU.html">526 high scalability-2009-03-05-Strategy:  In Cloud Computing Systematically Drive Load to the CPU</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
