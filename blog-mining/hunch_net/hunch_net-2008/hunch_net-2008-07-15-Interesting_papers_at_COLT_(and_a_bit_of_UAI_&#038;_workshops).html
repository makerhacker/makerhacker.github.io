<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-310" href="#">hunch_net-2008-310</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-310-html" href="http://hunch.net/?p=341">html</a></p><p>Introduction: Here are a few papers fromCOLT 2008that I found interesting.Maria-Florina
Balcan,Steve Hanneke, andJenn Wortman,The True Sample Complexity of Active
Learning. This paper shows that in an asymptotic setting, active learning
isalwaysbetter than supervised learning (although the gap may be small). This
is evidence that the only thing in the way of universal active learning is us
knowing how to do it properly.Nir AilonandMehryar Mohri,An Efficient Reduction
of Ranking to Classification. This paper shows how to robustly ranknobjects
withn log(n)classifications using a quicksort based algorithm. The result is
applicable to many ranking loss functions and has implications for
others.Michael KearnsandJennifer Wortman.Learning from Collective Behavior.
This is about learning in a new model, where the goal is to predict how a
collection of interacting agents behave. One claim is that learning in this
setting can be reduced to IID learning.Due to the relation withMetric-E3, I
was particularly int</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('talk', 0.238), ('active', 0.235), ('ranking', 0.231), ('shows', 0.19), ('collective', 0.175), ('collobert', 0.175), ('wild', 0.175), ('workshopand', 0.175), ('classifications', 0.162), ('primal', 0.162), ('asymptotic', 0.162), ('hanneke', 0.162), ('robustly', 0.162), ('relation', 0.153), ('interacting', 0.146), ('thelarge', 0.146), ('agents', 0.146), ('won', 0.14), ('nlp', 0.135), ('setting', 0.134)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="310-tfidf-1" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>Introduction: Here are a few papers fromCOLT 2008that I found interesting.Maria-Florina
Balcan,Steve Hanneke, andJenn Wortman,The True Sample Complexity of Active
Learning. This paper shows that in an asymptotic setting, active learning
isalwaysbetter than supervised learning (although the gap may be small). This
is evidence that the only thing in the way of universal active learning is us
knowing how to do it properly.Nir AilonandMehryar Mohri,An Efficient Reduction
of Ranking to Classification. This paper shows how to robustly ranknobjects
withn log(n)classifications using a quicksort based algorithm. The result is
applicable to many ranking loss functions and has implications for
others.Michael KearnsandJennifer Wortman.Learning from Collective Behavior.
This is about learning in a new model, where the goal is to predict how a
collection of interacting agents behave. One claim is that learning in this
setting can be reduced to IID learning.Due to the relation withMetric-E3, I
was particularly int</p><p>2 0.16865635 <a title="310-tfidf-2" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>Introduction: This post is by Daniel Hsu and John Langford.In selective sampling style
active learning, a learning algorithm chooses which examples to label. We now
have an active learning algorithm that is:Efficientin label complexity,
unlabeled complexity, and computational complexity.Competitivewith supervised
learning anywhere that supervised learning works.Compatiblewith online
learning, with any optimization-based learning algorithm, with any loss
function, with offline testing, and even with changing learning
algorithms.Empiricallyeffective.The basic idea is to combinedisagreement
region-based samplingwithimportance weighting: an example is selected to be
labeled with probability proportional to how useful it is for distinguishing
among near-optimal classifiers, and labeled examples are importance-weighted
by the inverse of these probabilities. The combination of these simple ideas
removes thesampling biasproblem that has plagued many previous heuristics for
active learning, and yet leads to</p><p>3 0.16768393 <a title="310-tfidf-3" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,Sanjoymade a postsaying roughly "we should study
active learning theoretically, because not much is understood".At the time, we
did not understand basic things such as whether or not it was possible to PAC-
learn with an active algorithm without making strong assumptions about the
noise rate. In other words, the fundamental question was "can we do it?"The
nature of the question has fundamentally changed in my mind. The answer is to
the previous question is "yes", both information theoretically and
computationally, most places where supervised learning could be applied.In
many situation, the question has now changed to: "is it worth it?" Is the
programming and computational overhead low enough to make the label cost
savings of active learning worthwhile? Currently, there are situations where
this question could go either way. Much of the challenge for the future is in
figuring out how to make active learning easier or more worthwhile.At
theactive learning tutor</p><p>4 0.16340426 <a title="310-tfidf-4" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><p>5 0.15123755 <a title="310-tfidf-5" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>6 0.13833666 <a title="310-tfidf-6" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>7 0.12508534 <a title="310-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>8 0.12280025 <a title="310-tfidf-8" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>9 0.12011823 <a title="310-tfidf-9" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>10 0.11759498 <a title="310-tfidf-10" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>11 0.11569171 <a title="310-tfidf-11" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>12 0.11567967 <a title="310-tfidf-12" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>13 0.11097656 <a title="310-tfidf-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.10840996 <a title="310-tfidf-14" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>15 0.095032282 <a title="310-tfidf-15" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>16 0.095030926 <a title="310-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>17 0.088667333 <a title="310-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>18 0.085528918 <a title="310-tfidf-18" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>19 0.085457616 <a title="310-tfidf-19" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>20 0.083855167 <a title="310-tfidf-20" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, -0.073), (2, -0.053), (3, 0.104), (4, -0.097), (5, 0.053), (6, 0.102), (7, -0.076), (8, -0.119), (9, -0.142), (10, -0.011), (11, 0.13), (12, 0.059), (13, 0.13), (14, 0.096), (15, 0.009), (16, 0.077), (17, 0.009), (18, -0.001), (19, -0.05), (20, 0.021), (21, 0.11), (22, 0.003), (23, 0.014), (24, 0.046), (25, -0.027), (26, 0.039), (27, 0.067), (28, -0.009), (29, 0.035), (30, -0.042), (31, -0.005), (32, -0.077), (33, 0.097), (34, 0.041), (35, 0.007), (36, -0.032), (37, -0.01), (38, 0.024), (39, -0.046), (40, 0.01), (41, -0.098), (42, 0.016), (43, -0.022), (44, 0.019), (45, 0.046), (46, 0.052), (47, 0.003), (48, 0.061), (49, 0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94419527 <a title="310-lsi-1" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>Introduction: Here are a few papers fromCOLT 2008that I found interesting.Maria-Florina
Balcan,Steve Hanneke, andJenn Wortman,The True Sample Complexity of Active
Learning. This paper shows that in an asymptotic setting, active learning
isalwaysbetter than supervised learning (although the gap may be small). This
is evidence that the only thing in the way of universal active learning is us
knowing how to do it properly.Nir AilonandMehryar Mohri,An Efficient Reduction
of Ranking to Classification. This paper shows how to robustly ranknobjects
withn log(n)classifications using a quicksort based algorithm. The result is
applicable to many ranking loss functions and has implications for
others.Michael KearnsandJennifer Wortman.Learning from Collective Behavior.
This is about learning in a new model, where the goal is to predict how a
collection of interacting agents behave. One claim is that learning in this
setting can be reduced to IID learning.Due to the relation withMetric-E3, I
was particularly int</p><p>2 0.69521457 <a title="310-lsi-2" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>Introduction: This post is by Daniel Hsu and John Langford.In selective sampling style
active learning, a learning algorithm chooses which examples to label. We now
have an active learning algorithm that is:Efficientin label complexity,
unlabeled complexity, and computational complexity.Competitivewith supervised
learning anywhere that supervised learning works.Compatiblewith online
learning, with any optimization-based learning algorithm, with any loss
function, with offline testing, and even with changing learning
algorithms.Empiricallyeffective.The basic idea is to combinedisagreement
region-based samplingwithimportance weighting: an example is selected to be
labeled with probability proportional to how useful it is for distinguishing
among near-optimal classifiers, and labeled examples are importance-weighted
by the inverse of these probabilities. The combination of these simple ideas
removes thesampling biasproblem that has plagued many previous heuristics for
active learning, and yet leads to</p><p>3 0.68149906 <a title="310-lsi-3" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><p>4 0.66794693 <a title="310-lsi-4" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>5 0.66450053 <a title="310-lsi-5" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>6 0.64275604 <a title="310-lsi-6" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>7 0.60058355 <a title="310-lsi-7" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>8 0.58504349 <a title="310-lsi-8" href="../hunch_net-2009/hunch_net-2009-01-23-An_Active_Learning_Survey.html">338 hunch net-2009-01-23-An Active Learning Survey</a></p>
<p>9 0.57689559 <a title="310-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>10 0.55724227 <a title="310-lsi-10" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>11 0.53804314 <a title="310-lsi-11" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>12 0.52268517 <a title="310-lsi-12" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>13 0.50053817 <a title="310-lsi-13" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>14 0.48073524 <a title="310-lsi-14" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>15 0.46694058 <a title="310-lsi-15" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>16 0.46533462 <a title="310-lsi-16" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>17 0.45361918 <a title="310-lsi-17" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>18 0.44674361 <a title="310-lsi-18" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>19 0.44332147 <a title="310-lsi-19" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>20 0.44174364 <a title="310-lsi-20" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.015), (35, 0.032), (42, 0.319), (45, 0.018), (50, 0.026), (54, 0.379), (68, 0.077), (73, 0.017), (74, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97060168 <a title="310-lda-1" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>Introduction: Mark Reiddid a post onICML trendsthat I found interesting.</p><p>2 0.9401592 <a title="310-lda-2" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>Introduction: There are several summer schools related to machine learning.We are running a
two weekmachine learning summer schoolin Chicago, USA May 16-27.IPAM is
running a more focused three week summer school onIntelligent Extraction of
Information from Graphs and High Dimensional Datain Los Angeles, USA July
11-29.A broad one-week school onanalysis of patternswill be held in Erice,
Italy, Oct. 28-Nov 6.</p><p>3 0.89104426 <a title="310-lda-3" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>Introduction: Aaron Hertzmannpoints out thehealth of conferences wiki, which has a great
deal of information about how many different conferences function.</p><p>same-blog 4 0.88747501 <a title="310-lda-4" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>Introduction: Here are a few papers fromCOLT 2008that I found interesting.Maria-Florina
Balcan,Steve Hanneke, andJenn Wortman,The True Sample Complexity of Active
Learning. This paper shows that in an asymptotic setting, active learning
isalwaysbetter than supervised learning (although the gap may be small). This
is evidence that the only thing in the way of universal active learning is us
knowing how to do it properly.Nir AilonandMehryar Mohri,An Efficient Reduction
of Ranking to Classification. This paper shows how to robustly ranknobjects
withn log(n)classifications using a quicksort based algorithm. The result is
applicable to many ranking loss functions and has implications for
others.Michael KearnsandJennifer Wortman.Learning from Collective Behavior.
This is about learning in a new model, where the goal is to predict how a
collection of interacting agents behave. One claim is that learning in this
setting can be reduced to IID learning.Due to the relation withMetric-E3, I
was particularly int</p><p>5 0.67781782 <a title="310-lda-5" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>Introduction: I want to try to describe what doing research means, especially from the point
of view of an undergraduate. The shift from a class-taking mentality to a
research mentality is very significant and not easy.Problem PosingPosing the
right problem is often as important as solving them. Many people can get by in
research by solving problems others have posed, but that's not sufficient for
really inspiring research. For learning in particular, there is a strong
feeling that we just haven't figured out which questions are the right ones to
ask. You can see this, because the answers we have do not seem
convincing.Gambling your lifeWhen you do research, you think very hard about
new ways of solving problems, new problems, and new solutions. Many
conversations are of the form "I wonder what would happen ifâ€¦" These processes
can be short (days or weeks) or years-long endeavours. The worst part is that
you'll only know if you were succesful at the end of the process (and
sometimes not even then be</p><p>6 0.64709008 <a title="310-lda-6" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>7 0.6448034 <a title="310-lda-7" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>8 0.64073366 <a title="310-lda-8" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>9 0.63881636 <a title="310-lda-9" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>10 0.6382215 <a title="310-lda-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.63773787 <a title="310-lda-11" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>12 0.63706821 <a title="310-lda-12" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>13 0.63685864 <a title="310-lda-13" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>14 0.63677329 <a title="310-lda-14" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>15 0.63648444 <a title="310-lda-15" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>16 0.63648361 <a title="310-lda-16" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>17 0.63619703 <a title="310-lda-17" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>18 0.63538802 <a title="310-lda-18" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>19 0.63538039 <a title="310-lda-19" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>20 0.63494396 <a title="310-lda-20" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
